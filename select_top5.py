#!/usr/bin/env python3
"""
ì•™ìƒë¸” ì•ŒíŒŒë¡œ ìƒìœ„ 5ì¢…ëª© ì„ ì • (20ì˜ì—…ì¼ = 1ë‹¬ ë¦¬ë°¸ëŸ°ì‹±)
v11 Multi-Alpha Ensemble (60%) + v6 ì¬ë¬´ ì¶”ì„¸ (40%)
- Tech: best_alphas.jsonì—ì„œ CV-ê²€ì¦ëœ ìƒìœ„ Nê°œ ì•ŒíŒŒë¥¼ IC-weighted ì•™ìƒë¸”
- v6: ì˜ì—…ì´ìµ ì¶”ì„¸ + YoY + QoQ ê°œì„ 
- ì¥ì : ë‹¨ì¼ ì•ŒíŒŒ ëŒ€ë¹„ ë¶„ì‚° ê°ì†Œ, ì•ˆì •ì„± ì¦ê°€ (overfitting ë°©ì§€)
"""

import sys
import os
import json as _json
from pathlib import Path
import pandas as pd
import numpy as np
from dotenv import load_dotenv
import psycopg2

project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from alpha_gpt_kr.mining.operators import AlphaOperators as ops

load_dotenv()


def get_db_connection():
    return psycopg2.connect(
        host=os.getenv('DB_HOST', '192.168.0.248'),
        port=int(os.getenv('DB_PORT', 5432)),
        database=os.getenv('DB_NAME', 'marketsense'),
        user=os.getenv('DB_USER', 'yrbahn'),
        password=os.getenv('DB_PASSWORD', '1234')
    )


# ë°ì´í„° ë¡œë“œ
print("ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")
conn = get_db_connection()

stocks_df = pd.read_sql("""
    SELECT s.id, s.ticker, s.name, s.market_cap
    FROM stocks s
    WHERE s.is_active = true
    AND s.market_cap IS NOT NULL
    AND EXISTS (
        SELECT 1 FROM price_data p
        WHERE p.stock_id = s.id
        AND p.date >= CURRENT_DATE - INTERVAL '730 days'
        LIMIT 1
    )
    ORDER BY s.market_cap DESC
    LIMIT 2000
""", conn)

stock_ids = stocks_df['id'].tolist()
stock_id_list = ', '.join(map(str, stock_ids))

ticker_name = dict(zip(stocks_df['ticker'], stocks_df['name']))
ticker_mcap = dict(zip(stocks_df['ticker'], stocks_df['market_cap']))
id_ticker = dict(zip(stocks_df['id'], stocks_df['ticker']))

price_df = pd.read_sql(f"""
    SELECT s.ticker, p.date, p.open, p.high, p.low, p.close, p.volume
    FROM price_data p
    JOIN stocks s ON p.stock_id = s.id
    WHERE p.stock_id IN ({stock_id_list})
    AND p.date >= CURRENT_DATE - INTERVAL '730 days'
    ORDER BY s.ticker, p.date
""", conn)

open_price = price_df.pivot(index='date', columns='ticker', values='open')
high = price_df.pivot(index='date', columns='ticker', values='high')
low = price_df.pivot(index='date', columns='ticker', values='low')
close = price_df.pivot(index='date', columns='ticker', values='close')
volume = price_df.pivot(index='date', columns='ticker', values='volume')
returns = close.pct_change()

# ìˆ˜ê¸‰ ë°ì´í„° ë¡œë“œ
print("   ìˆ˜ê¸‰ ë°ì´í„° ë¡œë“œ ì¤‘...")
try:
    flow_df = pd.read_sql(f"""
        SELECT s.ticker, sd.date,
               sd.foreign_net_buy, sd.institution_net_buy,
               sd.individual_net_buy, sd.foreign_ownership
        FROM supply_demand_data sd
        JOIN stocks s ON sd.stock_id = s.id
        WHERE sd.stock_id IN ({stock_id_list})
        AND sd.date >= CURRENT_DATE - INTERVAL '730 days'
        ORDER BY s.ticker, sd.date
    """, conn)
    foreign_buy_raw = flow_df.pivot(index='date', columns='ticker', values='foreign_net_buy')
    inst_buy_raw = flow_df.pivot(index='date', columns='ticker', values='institution_net_buy')
    retail_buy_raw = flow_df.pivot(index='date', columns='ticker', values='individual_net_buy')
    foreign_own_raw = flow_df.pivot(index='date', columns='ticker', values='foreign_ownership')
    has_flow = True
except Exception as e:
    print(f"   âš ï¸ ìˆ˜ê¸‰ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    has_flow = False

# ì¬ë¬´ ì¶”ì„¸ ë°ì´í„°
print("   ì¬ë¬´ ì¶”ì„¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
fin_df = pd.read_sql(f"""
    SELECT stock_id, period_end, revenue, operating_income, net_income, raw_data
    FROM financial_statements
    WHERE stock_id IN ({stock_id_list})
    ORDER BY stock_id, period_end
""", conn)
conn.close()

# raw_dataì—ì„œ ë¶„ê¸° íƒ€ì… ì¶”ì¶œ
def _parse_raw(row):
    rd = row.get('raw_data')
    if rd is None:
        return {'quarter_type': None}
    if isinstance(rd, str):
        rd = _json.loads(rd)
    return {'quarter_type': rd.get('quarter', '')}

raw_parsed = fin_df.apply(_parse_raw, axis=1, result_type='expand')
fin_df = pd.concat([fin_df, raw_parsed], axis=1)
fin_df['ticker'] = fin_df['stock_id'].map(id_ticker)
fin_df = fin_df.dropna(subset=['ticker'])
fin_df = fin_df[fin_df['quarter_type'] != 'ì—°ê°„'].copy()
fin_df = fin_df.sort_values(['ticker', 'period_end'])

# â”€â”€ ë°¸ë¥˜ì—ì´ì…˜ í•„í„°ìš©: ìµœê·¼ 4ë¶„ê¸° ë§¤ì¶œ/ì˜ì—…ì´ìµ/ìˆœì´ìµ í•©ì‚° â”€â”€
print("   ë°¸ë¥˜ì—ì´ì…˜ í•„í„° ê³„ì‚° ì¤‘...")
valuation = {}
for ticker, grp in fin_df.groupby('ticker'):
    grp = grp.sort_values('period_end')
    recent = grp.tail(4)  # ìµœê·¼ 4ë¶„ê¸°
    if len(recent) < 2:
        continue
    trailing_rev = recent['revenue'].sum()
    trailing_oi = recent['operating_income'].sum()
    trailing_ni = recent['net_income'].sum()
    mcap = ticker_mcap.get(ticker, 0)
    if mcap and mcap > 0 and trailing_rev and trailing_rev > 0:
        psr = mcap / trailing_rev
    else:
        psr = np.nan
    if mcap and mcap > 0 and trailing_ni and trailing_ni > 0:
        per = mcap / trailing_ni
    else:
        per = np.nan  # ì ì ë˜ëŠ” ë°ì´í„° ì—†ìŒ
    valuation[ticker] = {
        'trailing_rev': trailing_rev,
        'trailing_oi': trailing_oi,
        'trailing_ni': trailing_ni,
        'per': per,
    }

# í•„í„° ê¸°ì¤€:
#   1) PER â‰¤ 50 (ì ì = ì œì™¸, PER 50ë°° ì´ˆê³¼ = ì œì™¸)
#   2) ìµœê·¼ 4ë¶„ê¸° ì˜ì—…ì´ìµ > 0 (í‘ì)
FILTER_PER_MAX = 50

filtered_out = set()
for ticker, v in valuation.items():
    reasons = []
    # ìˆœì´ìµ ì ì â†’ PER ì‚°ì¶œ ë¶ˆê°€ â†’ ì œì™¸
    if v['trailing_ni'] is None or v['trailing_ni'] <= 0:
        reasons.append("ìˆœì´ìµ ì ì")
    elif v['per'] is not None and v['per'] > FILTER_PER_MAX:
        reasons.append(f"PER {v['per']:.1f}x")
    # ì˜ì—…ì´ìµ ì ìë„ ì œì™¸
    if v['trailing_oi'] is not None and v['trailing_oi'] <= 0:
        reasons.append("ì˜ì—…ì ì")
    if reasons:
        filtered_out.add(ticker)

# ì¬ë¬´ë°ì´í„° ì—†ëŠ” ì¢…ëª©ë„ ì œì™¸
tickers_with_valuation = set(valuation.keys())
no_data_tickers = set(close.columns) - tickers_with_valuation

print(f"   í•„í„° ì¡°ê±´: PER â‰¤ {FILTER_PER_MAX}x, ì˜ì—…ì´ìµ í‘ì, ìˆœì´ìµ í‘ì")
print(f"   ë°¸ë¥˜ í•„í„° ì œì™¸: {len(filtered_out)}ê°œ, ì¬ë¬´ë°ì´í„° ì—†ìŒ: {len(no_data_tickers)}ê°œ")

# QoQ / YoY / 3ë¶„ê¸° ì¶”ì„¸
trend_records = []
for ticker, grp in fin_df.groupby('ticker'):
    grp = grp.sort_values('period_end').reset_index(drop=True)
    for i in range(len(grp)):
        row = grp.iloc[i]
        rec = {'ticker': ticker, 'period_end': row['period_end']}

        if i >= 1:
            prev = grp.iloc[i - 1]
            if prev['operating_income'] and prev['operating_income'] != 0:
                rec['oi_qoq'] = (row['operating_income'] - prev['operating_income']) / abs(prev['operating_income'])

        if i >= 3:
            yoy_prev = grp.iloc[i - 3]
            if row['period_end'].month == yoy_prev['period_end'].month:
                if yoy_prev['operating_income'] and yoy_prev['operating_income'] != 0:
                    rec['oi_yoy'] = (row['operating_income'] - yoy_prev['operating_income']) / abs(yoy_prev['operating_income'])

        if i >= 2:
            oi_vals = [grp.iloc[j]['operating_income'] for j in range(i - 2, i + 1)
                       if grp.iloc[j]['operating_income'] is not None and not np.isnan(grp.iloc[j]['operating_income'])]
            if len(oi_vals) == 3:
                rec['oi_trend'] = (oi_vals[2] - oi_vals[0]) / (abs(oi_vals[0]) + 1e-10)

        trend_records.append(rec)

trend_df = pd.DataFrame(trend_records)

# ì¼ë³„ forward-fill + cross-sectional rank
_empty = pd.DataFrame(np.nan, index=close.index, columns=close.columns)
trend_vars = {}
for field in ['oi_qoq', 'oi_yoy', 'oi_trend']:
    if field not in trend_df.columns:
        continue
    pivot = trend_df.pivot_table(index='period_end', columns='ticker', values=field, aggfunc='last')
    if pivot.empty or pivot.notna().sum().sum() < 50:
        continue
    daily = pivot.reindex(close.index).ffill().reindex(columns=close.columns)
    trend_vars[f'{field}_rank'] = daily.rank(axis=1, pct=True)

oi_trend_rank = trend_vars.get('oi_trend_rank', _empty)
oi_yoy_rank = trend_vars.get('oi_yoy_rank', _empty)
oi_qoq_rank = trend_vars.get('oi_qoq_rank', _empty)

# â”€â”€ íŒŒìƒ ê¸°ìˆ  ì§€í‘œ (21ê°œ ì „ì²´ â€” multi-alpha ensembleìš©) â”€â”€
vwap = (high + low + close) / 3
high_low_range = (high - low) / close
body = (close - open_price) / open_price
upper_shadow = (high - close.clip(lower=open_price)) / close
lower_shadow = (close.clip(upper=open_price) - low) / close

tr1 = high - low
tr2 = (high - close.shift(1)).abs()
tr3 = (low - close.shift(1)).abs()
true_range = pd.concat([tr1, tr2, tr3]).groupby(level=0).max()
true_range = true_range.reindex(close.index)
atr_ratio = true_range / close

amount = close * volume
amihud = returns.abs() / amount.replace(0, np.nan)
amihud = amihud.replace([np.inf, -np.inf], np.nan).fillna(0)

gap = open_price / close.shift(1) - 1
intraday_ret = close / open_price - 1

vol_ratio = volume / volume.rolling(20, min_periods=5).mean()
vol_ratio = vol_ratio.replace([np.inf, -np.inf], np.nan).fillna(1)

# ìˆ˜ê¸‰ ë¹„ìœ¨ ê³„ì‚°
if has_flow:
    foreign_buy_raw = foreign_buy_raw.reindex(index=close.index, columns=close.columns)
    inst_buy_raw = inst_buy_raw.reindex(index=close.index, columns=close.columns)
    retail_buy_raw = retail_buy_raw.reindex(index=close.index, columns=close.columns)
    foreign_own_raw = foreign_own_raw.reindex(index=close.index, columns=close.columns)
    safe_volume = volume.replace(0, np.nan)
    foreign_net_ratio = (foreign_buy_raw / safe_volume).clip(-1, 1).fillna(0)
    inst_net_ratio = (inst_buy_raw / safe_volume).clip(-1, 1).fillna(0)
    retail_net_ratio = (retail_buy_raw / safe_volume).clip(-1, 1).fillna(0)
    foreign_ownership_pct = (foreign_own_raw / 100).clip(0, 1).fillna(0)
    print(f"   ìˆ˜ê¸‰ ì§€í‘œ 4ê°œ ë¡œë“œ ì™„ë£Œ")
else:
    foreign_net_ratio = close * 0.0
    inst_net_ratio = close * 0.0
    retail_net_ratio = close * 0.0
    foreign_ownership_pct = close * 0.0

print(f"âœ… {len(close.columns)}ê°œ ì¢…ëª©, {len(close)}ì¼ ë°ì´í„°")
print(f"   ìµœì‹  ë‚ ì§œ: {close.index[-1]}")
print(f"   ì¬ë¬´ ì¶”ì„¸: {list(trend_vars.keys())}")

# â”€â”€ Multi-Alpha Ensemble (best_alphas.jsonì—ì„œ ë¡œë“œ) â”€â”€
alphas_path = project_root / 'best_alphas.json'
if alphas_path.exists():
    with open(alphas_path) as f:
        alpha_configs = _json.load(f)

    print(f"\nğŸ§¬ Multi-Alpha Ensemble: {len(alpha_configs)}ê°œ ì•ŒíŒŒ ë¡œë“œ")
    alpha_values_list = []
    alpha_weights = []
    alpha_infos = []
    for i, cfg in enumerate(alpha_configs, 1):
        expr = cfg['expression']
        ic = cfg['mean_test_ic']
        try:
            val = eval(expr)
            if isinstance(val, pd.DataFrame):
                # ICê°€ ì–‘ìˆ˜ì¸ ì•ŒíŒŒë§Œ í¬í•¨ (ìŒìˆ˜ IC = ì—­íš¨ê³¼)
                weight = max(ic, 0.001)
                alpha_values_list.append(val)
                alpha_weights.append(weight)
                alpha_infos.append(cfg)
                print(f"   #{i} IC={ic:.4f} IR={cfg['mean_test_ir']:.2f} [{cfg['factors']}] âœ…")
            else:
                print(f"   #{i} ë¹„ì •ìƒ ì¶œë ¥ (DataFrameì´ ì•„ë‹˜) âš ï¸")
        except Exception as e:
            print(f"   #{i} ê³„ì‚° ì‹¤íŒ¨: {e} âš ï¸")

    if alpha_values_list:
        # IC-weighted average â†’ normed_rank
        total_w = sum(alpha_weights)
        tech_alpha = sum(v * (w / total_w) for v, w in zip(alpha_values_list, alpha_weights))
        tech_alpha = ops.normed_rank(tech_alpha)
        n_alphas = len(alpha_values_list)
        print(f"   â†’ {n_alphas}ê°œ ì•ŒíŒŒ IC-weighted ì•™ìƒë¸” ì™„ë£Œ (ì´ ê°€ì¤‘ì¹˜: {total_w:.4f})")
    else:
        print("   âš ï¸ ìœ íš¨í•œ ì•ŒíŒŒ ì—†ìŒ â†’ ë‹¨ì¼ í´ë°± ì•ŒíŒŒ ì‚¬ìš©")
        tech_alpha = ops.normed_rank(
            ops.add(
                ops.div(ops.ts_decayed_linear(foreign_ownership_pct, 8), ops.ts_mean(vol_ratio, 57)),
                ops.zscore_scale(ops.div(amihud, ops.ts_mean(close, 110)))
            )
        )
        n_alphas = 1
else:
    print("\nâš ï¸ best_alphas.json ì—†ìŒ â†’ ë‹¨ì¼ v10 ì•ŒíŒŒ ì‚¬ìš©")
    tech_alpha = ops.normed_rank(
        ops.add(
            ops.div(ops.ts_decayed_linear(foreign_ownership_pct, 8), ops.ts_mean(vol_ratio, 57)),
            ops.zscore_scale(ops.div(amihud, ops.ts_mean(close, 110)))
        )
    )
    n_alphas = 1

# â”€â”€ v6 ì¬ë¬´ ì¶”ì„¸ ì•ŒíŒŒ â”€â”€
v6_alpha = ops.normed_rank(
    ops.add(
        ops.add(oi_trend_rank, oi_yoy_rank),
        oi_qoq_rank
    )
)

# â”€â”€ ì•™ìƒë¸” (60% multi-alpha tech + 40% ì¬ë¬´ì¶”ì„¸) â”€â”€
W_TECH = 0.60
W_FUND = 0.40
v6_filled = v6_alpha.fillna(0.5)
ensemble = ops.normed_rank(tech_alpha * W_TECH + v6_filled * W_FUND)

# ìµœì‹  ë‚ ì§œ ê¸°ì¤€
latest_date = ensemble.index[-1]
all_scores = ensemble.loc[latest_date].dropna().sort_values(ascending=False)

# ë°¸ë¥˜ì—ì´ì…˜ í•„í„° ì ìš©
exclude_tickers = filtered_out | no_data_tickers
filtered_scores = all_scores[~all_scores.index.isin(exclude_tickers)]

print(f"\n{'='*100}")
print(f"ğŸ† ì•™ìƒë¸” ìƒìœ„ ì¢…ëª© (ê¸°ì¤€ì¼: {latest_date})")
print(f"   Multi-Alpha Tech ({W_TECH:.0%}, {n_alphas}ê°œ IC-weighted) + v6 ì¬ë¬´ì¶”ì„¸ ({W_FUND:.0%})")
print(f"   í•„í„°: PER â‰¤ {FILTER_PER_MAX}x, ì˜ì—…í‘ì, ìˆœì´ìµí‘ì")
print(f"   ìœ ë‹ˆë²„ìŠ¤: {len(all_scores)}ì¢…ëª© â†’ í•„í„° í†µê³¼ {len(filtered_scores)}ì¢…ëª©")
print(f"{'='*100}")
print(f"{'ìˆœìœ„':>4} {'ì¢…ëª©ì½”ë“œ':<10} {'ì¢…ëª©ëª…':<16} {'ì•™ìƒë¸”':>8} {'Tech':>6} {'v6':>6} {'í˜„ì¬ê°€':>12} {'PER':>6} {'NI(ì–µ)':>8}")
print(f"{'-'*100}")

for i, (ticker, score) in enumerate(filtered_scores.head(10).items(), 1):
    name = ticker_name.get(ticker, '?')
    v10_s = tech_alpha.loc[latest_date, ticker] if ticker in tech_alpha.columns else np.nan
    v6_s = v6_alpha.loc[latest_date, ticker] if ticker in v6_alpha.columns else np.nan
    price = close.loc[latest_date, ticker]
    v = valuation.get(ticker, {})
    per = v.get('per', np.nan)
    ni = v.get('trailing_ni', 0)
    ni_ì–µ = ni / 1e8 if ni else 0
    per_s = f"{per:.1f}x" if per and not np.isnan(per) else "  -"
    print(f"  {i:>2}. {ticker:<10} {name:<16} {score:.4f} {v10_s:.3f}  {v6_s:.3f} {price:>12,.0f}ì› {per_s:>6} {ni_ì–µ:>7,.0f}ì–µ")

# í•„í„°ë¡œ ì œì™¸ëœ ìƒìœ„ ì¢…ëª© (ì°¸ê³ ìš©)
excluded_top = all_scores[all_scores.index.isin(filtered_out)].head(5)
if not excluded_top.empty:
    print(f"\nâš ï¸  í•„í„°ë¡œ ì œì™¸ëœ ê³ ì ìˆ˜ ì¢…ëª©:")
    for ticker, score in excluded_top.items():
        name = ticker_name.get(ticker, '?')
        v = valuation.get(ticker, {})
        per = v.get('per', np.nan)
        ni = v.get('trailing_ni', 0)
        ni_ì–µ = ni / 1e8 if ni else 0
        per_s = f"PER {per:.1f}x" if per and not np.isnan(per) else "PER N/A"
        ni_s = f"NI {ni_ì–µ:,.0f}ì–µ" if ni and ni > 0 else "ìˆœì´ìµì ì"
        oi = v.get('trailing_oi', 0)
        oi_s = "" if oi and oi > 0 else " ì˜ì—…ì ì"
        print(f"     {ticker:<10} {name:<16} ì ìˆ˜ {score:.4f}  {per_s}  {ni_s}{oi_s}")

# í•˜ìœ„ 5ì¢…ëª©
print(f"\n{'='*100}")
print(f"ğŸ“‰ í•˜ìœ„ 5ì¢…ëª© (ìˆ í›„ë³´, í•„í„° í†µê³¼)")
print(f"{'='*100}")
print(f"{'ìˆœìœ„':>4} {'ì¢…ëª©ì½”ë“œ':<10} {'ì¢…ëª©ëª…':<16} {'ì•™ìƒë¸”':>8} {'v10':>6} {'v6':>6} {'í˜„ì¬ê°€':>12} {'PER':>6} {'NI(ì–µ)':>8}")
print(f"{'-'*100}")

for i, (ticker, score) in enumerate(filtered_scores.tail(5).items(), 1):
    name = ticker_name.get(ticker, '?')
    v10_s = tech_alpha.loc[latest_date, ticker] if ticker in tech_alpha.columns else np.nan
    v6_s = v6_alpha.loc[latest_date, ticker] if ticker in v6_alpha.columns else np.nan
    price = close.loc[latest_date, ticker]
    v = valuation.get(ticker, {})
    per = v.get('per', np.nan)
    ni = v.get('trailing_ni', 0)
    ni_ì–µ = ni / 1e8 if ni else 0
    per_s = f"{per:.1f}x" if per and not np.isnan(per) else "  -"
    print(f"  {i:>2}. {ticker:<10} {name:<16} {score:.4f} {v10_s:.3f}  {v6_s:.3f} {price:>12,.0f}ì› {per_s:>6} {ni_ì–µ:>7,.0f}ì–µ")

print(f"\nğŸ’¡ í•´ì„:")
print(f"   Tech: {n_alphas}ê°œ CV-ê²€ì¦ ì•ŒíŒŒ IC-weighted ì•™ìƒë¸” (ë‹¨ì¼ ì•ŒíŒŒ ëŒ€ë¹„ ë¶„ì‚°â†“ ì•ˆì •ì„±â†‘)")
print(f"   v6 ì¬ë¬´: ì˜ì—…ì´ìµ 3Q ì¶”ì„¸ + YoY + QoQ ê°œì„ ë„ ë­í‚¹")
print(f"   ì•™ìƒë¸”: Multi-Alpha Tech ({W_TECH:.0%}) + ì¬ë¬´ì¶”ì„¸ ({W_FUND:.0%})")
print(f"   í•„í„°: PER â‰¤ {FILTER_PER_MAX}x + í‘ì â†’ ê³ ë°¸ë¥˜ ëª¨ë©˜í…€ í•¨ì • ì œê±°")
print(f"   â†’ 20ì˜ì—…ì¼(ì•½ 1ë‹¬) ë³´ìœ  ì „ëµì— ìµœì í™”")
