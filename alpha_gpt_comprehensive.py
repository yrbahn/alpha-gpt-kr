#!/usr/bin/env python3
"""
ì¢…í•© ì•ŒíŒŒ ìƒì„±: ë…¼ë¬¸ ë°©ì‹ + ëª¨ë“  ì§€í‘œ ëª…ì‹œ
LLMì—ê²Œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ë°ì´í„°ì™€ ì—°ì‚°ìë¥¼ ìƒì„¸íˆ ì œê³µ
"""

import sys
import os
from pathlib import Path
from datetime import date
import pandas as pd
import numpy as np
from dotenv import load_dotenv
import psycopg2
import openai
import random
from multiprocessing import Pool, cpu_count

project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from alpha_gpt_kr.mining.operators import AlphaOperators

load_dotenv()

def get_db_connection():
    return psycopg2.connect(
        host=os.getenv('DB_HOST', '192.168.0.248'),
        port=int(os.getenv('DB_PORT', 5432)),
        database=os.getenv('DB_NAME', 'marketsense'),
        user=os.getenv('DB_USER', 'yrbahn'),
        password=os.getenv('DB_PASSWORD', '1234')
    )

def load_comprehensive_data():
    """ëª¨ë“  ì§€í‘œ í†µí•© ë¡œë“œ"""
    print("ğŸ“Š ì¢…í•© ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    conn = get_db_connection()
    
    # ì‹œì´ ìƒìœ„ 100
    query_stocks = """
        SELECT DISTINCT ON (s.ticker)
            s.id, s.ticker, s.name
        FROM stocks s
        JOIN price_data p ON s.id = p.stock_id
        WHERE s.is_active = true
        AND p.date = (SELECT MAX(date) FROM price_data)
        ORDER BY s.ticker, (p.close * p.volume) DESC
        LIMIT 100
    """
    stocks_df = pd.read_sql(query_stocks, conn)
    stock_ids = stocks_df['id'].tolist()
    stock_id_list = ', '.join(map(str, stock_ids))
    
    # 1. ê°€ê²© ë°ì´í„°
    query_price = f"""
        SELECT s.ticker, p.date, p.close, p.open, p.high, p.low, p.volume
        FROM price_data p
        JOIN stocks s ON p.stock_id = s.id
        WHERE p.stock_id IN ({stock_id_list})
        AND p.date >= CURRENT_DATE - INTERVAL '730 days'
        ORDER BY s.ticker, p.date
    """
    price_df = pd.read_sql(query_price, conn)
    close = price_df.pivot(index='date', columns='ticker', values='close')
    open_px = price_df.pivot(index='date', columns='ticker', values='open')
    high = price_df.pivot(index='date', columns='ticker', values='high')
    low = price_df.pivot(index='date', columns='ticker', values='low')
    volume = price_df.pivot(index='date', columns='ticker', values='volume')
    
    # 2. ê¸°ìˆ ì  ì§€í‘œ
    query_tech = f"""
        SELECT s.ticker, t.date, t.rsi_14, t.macd, t.macd_signal, t.macd_hist,
               t.bb_upper, t.bb_middle, t.bb_lower, t.atr_14,
               t.sma_20, t.sma_50, t.sma_200, t.volatility_20d
        FROM technical_indicators t
        JOIN stocks s ON t.stock_id = s.id
        WHERE t.stock_id IN ({stock_id_list})
        AND t.date >= CURRENT_DATE - INTERVAL '730 days'
        ORDER BY s.ticker, t.date
    """
    tech_df = pd.read_sql(query_tech, conn)
    rsi = tech_df.pivot(index='date', columns='ticker', values='rsi_14')
    macd = tech_df.pivot(index='date', columns='ticker', values='macd')
    macd_signal = tech_df.pivot(index='date', columns='ticker', values='macd_signal')
    macd_hist = tech_df.pivot(index='date', columns='ticker', values='macd_hist')
    bb_upper = tech_df.pivot(index='date', columns='ticker', values='bb_upper')
    bb_middle = tech_df.pivot(index='date', columns='ticker', values='bb_middle')
    bb_lower = tech_df.pivot(index='date', columns='ticker', values='bb_lower')
    atr = tech_df.pivot(index='date', columns='ticker', values='atr_14')
    sma_20 = tech_df.pivot(index='date', columns='ticker', values='sma_20')
    sma_50 = tech_df.pivot(index='date', columns='ticker', values='sma_50')
    sma_200 = tech_df.pivot(index='date', columns='ticker', values='sma_200')
    volatility = tech_df.pivot(index='date', columns='ticker', values='volatility_20d')
    
    # 3. ìˆ˜ê¸‰ ë°ì´í„°
    query_supply = f"""
        SELECT s.ticker, sd.date, sd.foreign_net_buy, sd.institution_net_buy,
               sd.individual_net_buy, sd.foreign_ownership,
               sd.short_ratio, sd.margin_ratio
        FROM supply_demand_data sd
        JOIN stocks s ON sd.stock_id = s.id
        WHERE sd.stock_id IN ({stock_id_list})
        AND sd.date >= CURRENT_DATE - INTERVAL '730 days'
        ORDER BY s.ticker, sd.date
    """
    supply_df = pd.read_sql(query_supply, conn)
    foreign_net = supply_df.pivot(index='date', columns='ticker', values='foreign_net_buy')
    institution_net = supply_df.pivot(index='date', columns='ticker', values='institution_net_buy')
    individual_net = supply_df.pivot(index='date', columns='ticker', values='individual_net_buy')
    foreign_own = supply_df.pivot(index='date', columns='ticker', values='foreign_ownership')
    short_ratio = supply_df.pivot(index='date', columns='ticker', values='short_ratio')
    margin_ratio = supply_df.pivot(index='date', columns='ticker', values='margin_ratio')
    
    conn.close()
    
    print(f"âœ… {len(close.columns)}ê°œ ì¢…ëª©, {len(close)}ì¼ ë°ì´í„°")
    
    return {
        # ê°€ê²©
        'close': close, 'open': open_px, 'high': high, 'low': low, 'volume': volume,
        'returns': close.pct_change(),
        # ê¸°ìˆ ì 
        'rsi': rsi, 'macd': macd, 'macd_signal': macd_signal, 'macd_hist': macd_hist,
        'bb_upper': bb_upper, 'bb_middle': bb_middle, 'bb_lower': bb_lower,
        'atr': atr, 'sma_20': sma_20, 'sma_50': sma_50, 'sma_200': sma_200,
        'volatility': volatility,
        # ìˆ˜ê¸‰
        'foreign_net': foreign_net, 'institution_net': institution_net,
        'individual_net': individual_net, 'foreign_own': foreign_own,
        'short_ratio': short_ratio, 'margin_ratio': margin_ratio
    }

def generate_comprehensive_alphas():
    """ë…¼ë¬¸ ë°©ì‹ í”„ë¡¬í”„íŠ¸: ëª¨ë“  ì§€í‘œì™€ ì—°ì‚°ì ëª…ì‹œ"""
    
    print("\nğŸ¤– LLMì—ê²Œ ëª¨ë“  ì§€í‘œ ì œê³µí•˜ì—¬ ì¢…í•© ì•ŒíŒŒ ìƒì„± ì¤‘...")
    
    client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
    
    # ë…¼ë¬¸ ë°©ì‹: ìƒì„¸í•œ ë°ì´í„° í•„ë“œ ë° ì—°ì‚°ì ì„¤ëª…
    prompt = """ë‹¹ì‹ ì€ WorldQuant ìˆ˜ì¤€ì˜ í€€íŠ¸ ê°œë°œìì…ë‹ˆë‹¤.
í•œêµ­ ì¦ì‹œì—ì„œ 10ì¼ ë³´ìœ  ì‹œ ìˆ˜ìµì´ ë†’ì„ ì¢…ëª©ì„ ì°¾ëŠ” ì•ŒíŒŒë¥¼ ìƒì„±í•˜ì„¸ìš”.

## ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° í•„ë“œ (ì „ì²´ ëª©ë¡)

### 1. ê°€ê²© ë°ì´í„°
- close: ì¢…ê°€
- open: ì‹œê°€
- high: ê³ ê°€
- low: ì €ê°€
- volume: ê±°ë˜ëŸ‰
- returns: ìˆ˜ìµë¥  (close.pct_change())

### 2. ê¸°ìˆ ì  ì§€í‘œ
- rsi: RSI(14) - ìƒëŒ€ê°•ë„ì§€ìˆ˜ (0~100, 30ì´í•˜ ê³¼ë§¤ë„, 70ì´ìƒ ê³¼ë§¤ìˆ˜)
- macd: MACD - ëª¨ë©˜í…€ ì§€í‘œ
- macd_signal: MACD ì‹œê·¸ë„ì„ 
- macd_hist: MACD íˆìŠ¤í† ê·¸ë¨ (macd - signal)
- bb_upper: ë³¼ë¦°ì € ë°´ë“œ ìƒë‹¨
- bb_middle: ë³¼ë¦°ì € ë°´ë“œ ì¤‘ê°„ (20ì¼ ì´ë™í‰ê· )
- bb_lower: ë³¼ë¦°ì € ë°´ë“œ í•˜ë‹¨
- atr: ATR(14) - Average True Range (ë³€ë™ì„±)
- sma_20: 20ì¼ ë‹¨ìˆœì´ë™í‰ê· 
- sma_50: 50ì¼ ë‹¨ìˆœì´ë™í‰ê· 
- sma_200: 200ì¼ ë‹¨ìˆœì´ë™í‰ê· 
- volatility: 20ì¼ ë³€ë™ì„± (í‘œì¤€í¸ì°¨)

### 3. ìˆ˜ê¸‰ ë°ì´í„°
- foreign_net: ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜ëŸ‰ (ì–‘ìˆ˜=ë§¤ìˆ˜, ìŒìˆ˜=ë§¤ë„)
- institution_net: ê¸°ê´€ ìˆœë§¤ìˆ˜ëŸ‰
- individual_net: ê°œì¸ ìˆœë§¤ìˆ˜ëŸ‰
- foreign_own: ì™¸êµ­ì¸ ë³´ìœ  ì§€ë¶„ìœ¨ (%)
- short_ratio: ê³µë§¤ë„ ë¹„ìœ¨ (%)
- margin_ratio: ì‹ ìš©ê±°ë˜ ë¹„ìœ¨ (%)

## ğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ ì—°ì‚°ì

### Time-Series ì—°ì‚°ì (ì‹œê³„ì—´)
- ts_delta(x, period): í˜„ì¬ê°’ - Nì¼ ì „ ê°’ (ì˜ˆ: ts_delta(close, 20) = 20ì¼ ê°€ê²© ë³€í™”)
- ts_mean(x, window): Nì¼ ì´ë™í‰ê·  (ì˜ˆ: ts_mean(volume, 10) = 10ì¼ í‰ê·  ê±°ë˜ëŸ‰)
- ts_std(x, window): Nì¼ ì´ë™ í‘œì¤€í¸ì°¨ (ë³€ë™ì„±)
- ts_rank(x, window): Nì¼ ê¸°ì¤€ ìˆœìœ„ 0~1 (ì˜ˆ: ts_rank(close, 20) = 20ì¼ ì¤‘ í˜„ì¬ ê°€ê²© ìˆœìœ„)
- ts_corr(x, y, window): Nì¼ ìƒê´€ê³„ìˆ˜
- ts_min(x, window), ts_max(x, window): Nì¼ ìµœì†Œ/ìµœëŒ€ê°’

### Cross-Sectional ì—°ì‚°ì (íš¡ë‹¨ë©´)
- zscore_scale(x): Z-score ì •ê·œí™” (í‰ê· =0, í‘œì¤€í¸ì°¨=1)
- normed_rank(x): ìˆœìœ„ ì •ê·œí™” 0~1

## ğŸ’¡ ì „ëµ ì•„ì´ë””ì–´ (ê°ê° ë‹¤ë¥¸ ì ‘ê·¼)

1. **RSI ì—­ë°œìƒ**: RSI < 30 ê³¼ë§¤ë„ + ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜ ì¦ê°€ â†’ ë°˜ë“± ê¸°ëŒ€
2. **MACD ê³¨ë“ í¬ë¡œìŠ¤**: macd > macd_signal ì „í™˜ + ê±°ë˜ëŸ‰ ì¦ê°€ â†’ ìƒìŠ¹ ëª¨ë©˜í…€
3. **ë³¼ë¦°ì € í•˜ë‹¨ í„°ì¹˜**: close < bb_lower + ê¸°ê´€ ìˆœë§¤ìˆ˜ â†’ ì €ì  ë§¤ìˆ˜ ê¸°íšŒ
4. **ì´ë™í‰ê·  ì •ë°°ì—´**: sma_20 > sma_50 > sma_200 + ë‚®ì€ ë³€ë™ì„± â†’ ì•ˆì •ì  ìƒìŠ¹
5. **ì™¸êµ­ì¸ ë§¤ì§‘**: ts_delta(foreign_own, 60) > 0 (60ì¼ ì§€ë¶„ìœ¨ ì¦ê°€) â†’ ì¥ê¸° ê°•ì„¸
6. **ê³µë§¤ë„ ì»¤ë²„ë§**: short_ratio ê°ì†Œ + ê°€ê²© ìƒìŠ¹ â†’ ê³µë§¤ë„ ì²­ì‚° ì••ë ¥
7. **ìƒ¤í”„ ë¹„ìœ¨ ìš°ìˆ˜**: ts_mean(returns, 10) / volatility â†’ ìœ„í—˜ ëŒ€ë¹„ ìˆ˜ìµë¥ 
8. **ê±°ë˜ëŸ‰ ëŒíŒŒ**: ts_rank(volume, 20) > 0.9 + ts_delta(close, 5) > 0 â†’ ê±°ë˜ëŸ‰ ë™ë°˜ ìƒìŠ¹
9. **ìƒëŒ€ê°•ë„**: (close / sma_20) * ts_rank(returns, 20) â†’ ì¶”ì„¸ ê°•ë„
10. **ìˆ˜ê¸‰ ì¼ì¹˜**: foreign_net + institution_net (ì™¸êµ­ì¸+ê¸°ê´€ ë™ì‹œ ë§¤ìˆ˜)

## ğŸ“ ì¶œë ¥ í˜•ì‹

50ê°œì˜ ë§¤ìš° ë‹¤ì–‘í•œ ì•ŒíŒŒë¥¼ ìƒì„±í•˜ì„¸ìš”. ë‹¨ìˆœí•œ ê²ƒë¶€í„° ë³µì¡í•œ ì¡°í•©ê¹Œì§€:

ALPHA_1: AlphaOperators.ts_rank((rsi < 30) * foreign_net, 20)
ALPHA_2: AlphaOperators.ts_rank(macd - macd_signal, 20) * AlphaOperators.ts_rank(AlphaOperators.ts_delta(volume, 5), 20)
ALPHA_3: AlphaOperators.normed_rank((close < bb_lower) * institution_net)
ALPHA_4: AlphaOperators.ts_rank(sma_20 > sma_50, 20) * AlphaOperators.normed_rank(-volatility)
ALPHA_5: AlphaOperators.normed_rank(AlphaOperators.ts_delta(foreign_own, 60))
...
ALPHA_50: [ë§¤ìš° ë³µì¡í•œ ì¡°í•©]

ê·œì¹™:
- AlphaOperators. ì ‘ë‘ì‚¬ í•„ìˆ˜
- ëª¨ë“  ë³€ìˆ˜ëª… ì •í™•íˆ ì‚¬ìš© (ìœ„ ëª©ë¡ ì°¸ê³ )
- ë³µì¡í•œ ìˆ˜ì‹ í™˜ì˜ (2~3ê°œ ì—°ì‚°ì ì¡°í•©)
- í•œêµ­ ì¦ì‹œ íŠ¹ì„± ê³ ë ¤ (ì™¸êµ­ì¸/ê¸°ê´€ ì˜í–¥ í¼)
"""

    response = client.chat.completions.create(
        model="gpt-4-turbo-preview",
        messages=[
            {"role": "system", "content": "You are a world-class quantitative researcher."},
            {"role": "user", "content": prompt}
        ],
        temperature=1.0,  # ìµœëŒ€ ë‹¤ì–‘ì„±
        max_tokens=5000  # 50ê°œ ì•ŒíŒŒ
    )
    
    content = response.choices[0].message.content
    
    alphas = []
    for line in content.split('\n'):
        line = line.strip()
        if 'AlphaOperators' in line:
            if ':' in line:
                line = line.split(':', 1)[1].strip()
            if '#' in line:
                line = line.split('#')[0].strip()
            if line:
                alphas.append(line)
    
    print(f"âœ… {len(alphas)}ê°œ ì¢…í•© ì•ŒíŒŒ ìƒì„±")
    return alphas

_global_data = None

def set_global_data(data):
    global _global_data
    _global_data = data

def evaluate_alpha_ic_worker(alpha_expr):
    """10ì¼ í›„ ìˆ˜ìµë¥  ì˜ˆì¸¡"""
    global _global_data
    data = _global_data
    
    try:
        # ëª¨ë“  ë³€ìˆ˜ ë°”ì¸ë”©
        close = data['close']
        open_px = data['open']
        high = data['high']
        low = data['low']
        volume = data['volume']
        returns = data['returns']
        rsi = data['rsi']
        macd = data['macd']
        macd_signal = data['macd_signal']
        macd_hist = data['macd_hist']
        bb_upper = data['bb_upper']
        bb_middle = data['bb_middle']
        bb_lower = data['bb_lower']
        atr = data['atr']
        sma_20 = data['sma_20']
        sma_50 = data['sma_50']
        sma_200 = data['sma_200']
        volatility = data['volatility']
        foreign_net = data['foreign_net']
        institution_net = data['institution_net']
        individual_net = data['individual_net']
        foreign_own = data['foreign_own']
        short_ratio = data['short_ratio']
        margin_ratio = data['margin_ratio']
        
        # 10ì¼ í›„ ìˆ˜ìµë¥ 
        returns_forward_10 = close.pct_change(10).shift(-10)
        
        alpha_values = eval(alpha_expr)
        
        ic_list = []
        for date in alpha_values.index[:-10]:
            alpha_cs = alpha_values.loc[date]
            returns_cs = returns_forward_10.loc[date]
            valid = alpha_cs.notna() & returns_cs.notna()
            
            if valid.sum() > 10:
                ic = alpha_cs[valid].corr(returns_cs[valid])
                if not np.isnan(ic):
                    ic_list.append(ic)
        
        if len(ic_list) < 10:
            return (alpha_expr, -999.0)
        
        return (alpha_expr, np.mean(ic_list))
        
    except Exception as e:
        return (alpha_expr, -999.0)

def genetic_programming_parallel(seed_alphas, data, generations=15, population_size=200):
    """ë³‘ë ¬ GP"""
    num_workers = min(cpu_count(), 8)
    
    print(f"\nğŸ§¬ ë³‘ë ¬ GP ì§„í™”")
    print(f"   ì„¸ëŒ€: {generations}, ê°œì²´ìˆ˜: {population_size}")
    
    population = seed_alphas[:population_size]
    while len(population) < population_size:
        population.append(random.choice(seed_alphas))
    
    set_global_data(data)
    
    for gen in range(generations):
        print(f"\n  ì„¸ëŒ€ {gen+1}/{generations}")
        
        with Pool(num_workers, initializer=set_global_data, initargs=(data,)) as pool:
            results = pool.map(evaluate_alpha_ic_worker, population)
        
        fitness_scores = sorted(results, key=lambda x: x[1], reverse=True)
        
        best_ic = fitness_scores[0][1]
        print(f"    ìµœê³  IC: {best_ic:.4f}")
        
        next_population = []
        
        elite_count = population_size // 5
        for alpha, _ in fitness_scores[:elite_count]:
            next_population.append(alpha)
        
        while len(next_population) < population_size:
            parent = random.choice([a for a, _ in fitness_scores[:population_size//2]])
            next_population.append(parent)
        
        population = next_population
    
    with Pool(num_workers, initializer=set_global_data, initargs=(data,)) as pool:
        final_results = pool.map(evaluate_alpha_ic_worker, population)
    
    final_fitness = sorted(final_results, key=lambda x: x[1], reverse=True)
    
    print(f"\nâœ… GP ì™„ë£Œ! ìµœê³  IC: {final_fitness[0][1]:.4f}")
    
    return final_fitness

def main():
    print("=" * 70)
    print("ì¢…í•© ì•ŒíŒŒ ìƒì„±: ëª¨ë“  ì§€í‘œ + ë…¼ë¬¸ ë°©ì‹ í”„ë¡¬í”„íŠ¸")
    print("=" * 70)
    print()
    
    # ë°ì´í„° ë¡œë“œ
    data = load_comprehensive_data()
    
    # LLM ì•ŒíŒŒ ìƒì„±
    seed_alphas = generate_comprehensive_alphas()
    
    print("\nğŸ“Š ìƒì„±ëœ ì´ˆê¸° ì•ŒíŒŒ:")
    for i, alpha in enumerate(seed_alphas, 1):
        print(f"   {i}. {alpha[:80]}...")
    
    # GP ì§„í™” (Large-scale: 50 seeds â†’ 200 population)
    evolved_alphas = genetic_programming_parallel(
        seed_alphas=seed_alphas,
        data=data,
        generations=15,
        population_size=200
    )
    
    # ê²°ê³¼
    print("\n" + "=" * 70)
    print("ğŸ† ì§„í™”ëœ ìƒìœ„ 5ê°œ ì¢…í•© ì•ŒíŒŒ")
    print("=" * 70)
    
    for i, (alpha, ic) in enumerate(evolved_alphas[:5], 1):
        print(f"\n{i}. IC: {ic:.4f}")
        print(f"   {alpha}")
    
    # DB ì €ì¥
    best_alpha, best_ic = evolved_alphas[0]
    
    conn = get_db_connection()
    cur = conn.cursor()
    
    try:
        cur.execute("""
            INSERT INTO alpha_performance
            (alpha_formula, start_date, is_active, sharpe_ratio, notes)
            VALUES (%s, %s, %s, %s, %s)
            ON CONFLICT (alpha_formula, start_date) DO UPDATE
            SET sharpe_ratio = EXCLUDED.sharpe_ratio, notes = EXCLUDED.notes
        """, (
            best_alpha,
            date.today(),
            True,
            float(best_ic * 10),
            f"IC: {best_ic:.4f}, Comprehensive (All indicators), 10-day forward"
        ))
        conn.commit()
        print("\nâœ… DB ì €ì¥ ì™„ë£Œ")
    finally:
        cur.close()
        conn.close()
    
    print("\nğŸ‰ ì¢…í•© ì•ŒíŒŒ ìƒì„± ì™„ë£Œ!")

if __name__ == "__main__":
    main()
