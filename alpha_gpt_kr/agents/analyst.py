"""
Analyst Agent
ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ìì—°ì–´ë¡œ í•´ì„í•˜ëŠ” ì—ì´ì „íŠ¸
"""

from typing import List, Dict, Optional
from dataclasses import dataclass
import json
from loguru import logger


@dataclass
class AnalysisReport:
    """ë¶„ì„ ë¦¬í¬íŠ¸"""
    summary: str
    key_findings: List[str]
    strengths: List[str]
    weaknesses: List[str]
    suggestions: List[str]
    risk_assessment: str


class Analyst:
    """
    Analyst Agent
    
    ì—­í• :
    1. ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ í•´ì„
    2. ì•ŒíŒŒì˜ ê°•ì ê³¼ ì•½ì  ë¶„ì„
    3. ê°œì„  ë°©í–¥ ì œì•ˆ
    4. ë¦¬ìŠ¤í¬ í‰ê°€
    """
    
    SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì „ë¬¸ í€€íŠ¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  íˆ¬ììê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ í•´ì„í•©ë‹ˆë‹¤.

ë¶„ì„ ì‹œ ë‹¤ìŒì„ ê³ ë ¤í•˜ì„¸ìš”:
1. IC (Information Coefficient): ì˜ˆì¸¡ë ¥ì˜ ì²™ë„
   - IC > 0.03: ìš°ìˆ˜
   - 0.01 < IC < 0.03: ì–‘í˜¸
   - IC < 0.01: ë¯¸í¡

2. Sharpe Ratio: ìœ„í—˜ ëŒ€ë¹„ ìˆ˜ìµ
   - Sharpe > 2.0: ìš°ìˆ˜
   - 1.0 < Sharpe < 2.0: ì–‘í˜¸
   - Sharpe < 1.0: ë¯¸í¡

3. Turnover: ê±°ë˜ ë¹„ìš©
   - Turnover < 0.3: ì €íšŒì „
   - 0.3 < Turnover < 1.0: ì¤‘íšŒì „
   - Turnover > 1.0: ê³ íšŒì „ (ë¹„ìš© ì£¼ì˜)

4. Maximum Drawdown: ë¦¬ìŠ¤í¬
   - MDD > -10%: ë‚®ì€ ë¦¬ìŠ¤í¬
   - -20% < MDD < -10%: ì¤‘ê°„ ë¦¬ìŠ¤í¬
   - MDD < -20%: ë†’ì€ ë¦¬ìŠ¤í¬

í•œêµ­ ì¦ì‹œ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”."""
    
    def __init__(self, llm_client):
        """
        Args:
            llm_client: LLM í´ë¼ì´ì–¸íŠ¸
        """
        self.llm = llm_client
        logger.info("Analyst initialized")
    
    def analyze_backtest(self, 
                        backtest_result,
                        alpha_expr: str) -> AnalysisReport:
        """
        ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„
        
        Args:
            backtest_result: BacktestResult ê°ì²´
            alpha_expr: ì•ŒíŒŒ í‘œí˜„ì‹
            
        Returns:
            AnalysisReport
        """
        logger.info(f"Analyzing backtest result for: {alpha_expr[:100]}")
        
        # ë©”íŠ¸ë¦­ ì¶”ì¶œ
        metrics = self._extract_metrics(backtest_result)
        
        # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._build_analysis_prompt(alpha_expr, metrics)
        
        # LLM í˜¸ì¶œ
        response = self._call_llm(prompt)
        
        # ì‘ë‹µ íŒŒì‹±
        report = self._parse_report(response)
        
        logger.info("Analysis complete")
        return report
    
    def compare_alphas(self,
                      results: List[tuple],  # [(alpha_expr, backtest_result), ...]
                      top_n: int = 5) -> str:
        """
        ì—¬ëŸ¬ ì•ŒíŒŒ ë¹„êµ ë¶„ì„
        
        Args:
            results: (ì•ŒíŒŒ í‘œí˜„ì‹, ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            top_n: ìƒìœ„ ëª‡ ê°œ ì„ íƒ
            
        Returns:
            ë¹„êµ ë¶„ì„ í…ìŠ¤íŠ¸
        """
        logger.info(f"Comparing {len(results)} alphas")
        
        # IC ê¸°ì¤€ ì •ë ¬
        sorted_results = sorted(results, key=lambda x: x[1].ic, reverse=True)
        
        comparison = "## ì•ŒíŒŒ ë¹„êµ ë¶„ì„\n\n"
        
        comparison += "### Top {} Alphas (IC ê¸°ì¤€)\n\n".format(min(top_n, len(sorted_results)))
        
        for i, (expr, result) in enumerate(sorted_results[:top_n], 1):
            comparison += f"""
**{i}. {expr[:80]}{'...' if len(expr) > 80 else ''}**
- IC: {result.ic:.4f} (Â±{result.ic_std:.4f})
- Sharpe: {result.sharpe_ratio:.2f}
- Annual Return: {result.annual_return:.2%}
- MDD: {result.max_drawdown:.2%}
- Turnover: {result.turnover:.2%}

"""
        
        # ì¢…í•© ë¶„ì„
        prompt = f"""ë‹¤ìŒ ìƒìœ„ ì•ŒíŒŒë“¤ì˜ íŠ¹ì§•ì„ ì¢…í•© ë¶„ì„í•˜ì„¸ìš”:

{comparison}

ê³µí†µì , ì°¨ì´ì , ê·¸ë¦¬ê³  ìµœì ì˜ ì¡°í•© ë°©ë²•ì„ ì œì•ˆí•˜ì„¸ìš”."""
        
        summary = self._call_llm(prompt)
        comparison += "\n### ì¢…í•© ë¶„ì„\n" + summary
        
        return comparison
    
    def suggest_improvements(self,
                           alpha_expr: str,
                           backtest_result) -> List[str]:
        """
        ê°œì„  ë°©í–¥ ì œì•ˆ
        
        Args:
            alpha_expr: ì•ŒíŒŒ í‘œí˜„ì‹
            backtest_result: ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼
            
        Returns:
            ê°œì„  ì œì•ˆ ë¦¬ìŠ¤íŠ¸
        """
        metrics = self._extract_metrics(backtest_result)
        
        prompt = f"""ë‹¤ìŒ ì•ŒíŒŒì˜ ê°œì„  ë°©í–¥ì„ êµ¬ì²´ì ìœ¼ë¡œ ì œì•ˆí•˜ì„¸ìš”:

ì•ŒíŒŒ: {alpha_expr}

ì„±ëŠ¥:
- IC: {metrics['ic']:.4f}
- Sharpe: {metrics['sharpe']:.2f}
- Turnover: {metrics['turnover']:.2%}
- MDD: {metrics['mdd']:.2%}

ì•½ì ì„ ë³´ì™„í•˜ê³  ê°•ì ì„ ì‚´ë¦´ ìˆ˜ ìˆëŠ” 3-5ê°€ì§€ êµ¬ì²´ì ì¸ ê°œì„ ì•ˆì„ ì œì‹œí•˜ì„¸ìš”.
JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
  "suggestions": [
    "ì œì•ˆ 1",
    "ì œì•ˆ 2",
    ...
  ]
}}
"""
        
        response = self._call_llm(prompt)
        
        try:
            data = json.loads(response)
            return data.get('suggestions', [])
        except:
            return ["ê°œì„  ì œì•ˆì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."]
    
    def _extract_metrics(self, result) -> Dict:
        """ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ"""
        return {
            'ic': result.ic,
            'ic_std': result.ic_std,
            'ir': result.ir,
            'sharpe': result.sharpe_ratio,
            'annual_return': result.annual_return,
            'total_return': result.total_return,
            'mdd': result.max_drawdown,
            'turnover': result.turnover,
            'win_rate': result.win_rate,
            'num_trades': result.num_trades
        }
    
    def _build_analysis_prompt(self, alpha_expr: str, metrics: Dict) -> str:
        """ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompt = f"""{self.SYSTEM_PROMPT}

## ì•ŒíŒŒ í‘œí˜„ì‹
{alpha_expr}

## ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼
- **IC (Information Coefficient)**: {metrics['ic']:.4f} (Â±{metrics['ic_std']:.4f})
- **IR (Information Ratio)**: {metrics['ir']:.2f}
- **Sharpe Ratio**: {metrics['sharpe']:.2f}
- **Annual Return**: {metrics['annual_return']:.2%}
- **Total Return**: {metrics['total_return']:.2%}
- **Maximum Drawdown**: {metrics['mdd']:.2%}
- **Turnover**: {metrics['turnover']:.2%}
- **Win Rate**: {metrics['win_rate']:.2%}
- **Number of Trades**: {metrics['num_trades']:,}

## ë¶„ì„ ìš”ì²­
ìœ„ ê²°ê³¼ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:

{{
  "summary": "ì „ì²´ ìš”ì•½ (2-3ë¬¸ì¥)",
  "key_findings": ["ì£¼ìš” ë°œê²¬ 1", "ì£¼ìš” ë°œê²¬ 2", ...],
  "strengths": ["ê°•ì  1", "ê°•ì  2", ...],
  "weaknesses": ["ì•½ì  1", "ì•½ì  2", ...],
  "suggestions": ["ê°œì„  ì œì•ˆ 1", "ê°œì„  ì œì•ˆ 2", ...],
  "risk_assessment": "ë¦¬ìŠ¤í¬ í‰ê°€"
}}
"""
        return prompt
    
    def _call_llm(self, prompt: str) -> str:
        """LLM í˜¸ì¶œ"""
        try:
            if hasattr(self.llm, 'chat'):
                response = self.llm.chat.completions.create(
                    model="gpt-4-turbo-preview",
                    messages=[
                        {"role": "system", "content": self.SYSTEM_PROMPT},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    response_format={"type": "json_object"}
                )
                return response.choices[0].message.content
            
            elif hasattr(self.llm, 'messages'):
                response = self.llm.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=2000,
                    temperature=0.3,
                    messages=[
                        {"role": "user", "content": prompt}
                    ]
                )
                return response.content[0].text
                
        except Exception as e:
            logger.error(f"LLM call failed: {e}")
            return self._generate_fallback_analysis()
    
    def _generate_fallback_analysis(self) -> str:
        """í´ë°± ë¶„ì„"""
        return json.dumps({
            "summary": "ë¶„ì„ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            "key_findings": [],
            "strengths": [],
            "weaknesses": [],
            "suggestions": [],
            "risk_assessment": "ì•Œ ìˆ˜ ì—†ìŒ"
        })
    
    def _parse_report(self, response: str) -> AnalysisReport:
        """LLM ì‘ë‹µ íŒŒì‹±"""
        try:
            data = json.loads(response)
            
            return AnalysisReport(
                summary=data.get('summary', ''),
                key_findings=data.get('key_findings', []),
                strengths=data.get('strengths', []),
                weaknesses=data.get('weaknesses', []),
                suggestions=data.get('suggestions', []),
                risk_assessment=data.get('risk_assessment', '')
            )
        except Exception as e:
            logger.error(f"Failed to parse report: {e}")
            return AnalysisReport(
                summary="ë¶„ì„ íŒŒì‹± ì‹¤íŒ¨",
                key_findings=[],
                strengths=[],
                weaknesses=[],
                suggestions=[],
                risk_assessment="ì•Œ ìˆ˜ ì—†ìŒ"
            )
    
    def format_report(self, report: AnalysisReport) -> str:
        """ë¦¬í¬íŠ¸ë¥¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…"""
        formatted = f"""
# ì•ŒíŒŒ ë¶„ì„ ë¦¬í¬íŠ¸

## ğŸ“Š ìš”ì•½
{report.summary}

## ğŸ” ì£¼ìš” ë°œê²¬
"""
        for finding in report.key_findings:
            formatted += f"- {finding}\n"
        
        formatted += "\n## âœ… ê°•ì \n"
        for strength in report.strengths:
            formatted += f"- {strength}\n"
        
        formatted += "\n## âš ï¸ ì•½ì \n"
        for weakness in report.weaknesses:
            formatted += f"- {weakness}\n"
        
        formatted += "\n## ğŸ’¡ ê°œì„  ì œì•ˆ\n"
        for suggestion in report.suggestions:
            formatted += f"- {suggestion}\n"
        
        formatted += f"\n## ğŸ›¡ï¸ ë¦¬ìŠ¤í¬ í‰ê°€\n{report.risk_assessment}\n"
        
        return formatted
