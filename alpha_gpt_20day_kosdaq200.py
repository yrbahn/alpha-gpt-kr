#!/usr/bin/env python3
"""
Alpha-GPT: 15-day Forward with GPT-4o (v2 â€” Improved Prompt)
ê°œì„ ëœ QuantDeveloper í”„ë¡¬í”„íŠ¸ + ops.xxx() ë¬¸ë²• + ë³‘ë ¬ GP
"""

import sys
import os
import re
import json
from pathlib import Path
import pandas as pd
import numpy as np
from dotenv import load_dotenv
import psycopg2
import openai
import random
import gc
from multiprocessing import Pool
from scipy.stats import spearmanr

project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from alpha_gpt_kr.mining.operators import AlphaOperators as ops
from alpha_gpt_kr.agents.quant_developer import QuantDeveloper

load_dotenv()

def get_db_connection():
    return psycopg2.connect(
        host=os.getenv('DB_HOST', '192.168.0.248'),
        port=int(os.getenv('DB_PORT', 5432)),
        database=os.getenv('DB_NAME', 'marketsense'),
        user=os.getenv('DB_USER', 'yrbahn'),
        password=os.getenv('DB_PASSWORD', '1234')
    )

def load_market_data():
    """200ê°œ ì¢…ëª© ë°ì´í„° ë¡œë“œ (ì‹œê°€ì´ì•¡ ìƒìœ„)"""
    print("ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘... (KOSDAQ ì‹œì´ ìƒìœ„ 200ì¢…ëª©, 2ë…„)")
    
    conn = get_db_connection()
    
    # ì‹œê°€ì´ì•¡ ìƒìœ„ 200ê°œ
    query_stocks = """
        SELECT 
            s.id,
            s.ticker,
            s.name,
            s.market_cap
        FROM stocks s
        WHERE s.is_active = true
        AND s.market_cap IS NOT NULL
        AND EXISTS (
            SELECT 1 FROM price_data p 
            WHERE p.stock_id = s.id 
            AND p.date >= CURRENT_DATE - INTERVAL '730 days'
            LIMIT 1
        )
        AND s.ticker >= '400000' ORDER BY s.market_cap DESC
        LIMIT 200
    """
    
    stocks_df = pd.read_sql(query_stocks, conn)
    stock_ids = stocks_df['id'].tolist()
    stock_id_list = ', '.join(map(str, stock_ids))
    
    query_prices = f"""
        SELECT
            s.ticker,
            p.date,
            p.open,
            p.high,
            p.low,
            p.close,
            p.volume
        FROM price_data p
        JOIN stocks s ON p.stock_id = s.id
        WHERE p.stock_id IN ({stock_id_list})
        AND p.date >= CURRENT_DATE - INTERVAL '730 days'
        ORDER BY s.ticker, p.date
    """

    price_df = pd.read_sql(query_prices, conn)

    open_price = price_df.pivot(index='date', columns='ticker', values='open')
    high = price_df.pivot(index='date', columns='ticker', values='high')
    low = price_df.pivot(index='date', columns='ticker', values='low')
    close = price_df.pivot(index='date', columns='ticker', values='close')
    volume = price_df.pivot(index='date', columns='ticker', values='volume')
    returns = close.pct_change()

    # â”€â”€ ìˆ˜ê¸‰ (Investor Flow) ì§€í‘œ â”€â”€
    try:
        flow_query = f"""
            SELECT s.ticker, sd.date,
                   sd.foreign_net_buy, sd.institution_net_buy,
                   sd.individual_net_buy, sd.foreign_ownership
            FROM supply_demand_data sd
            JOIN stocks s ON sd.stock_id = s.id
            WHERE sd.stock_id IN ({stock_id_list})
            AND sd.date >= CURRENT_DATE - INTERVAL '730 days'
            ORDER BY s.ticker, sd.date
        """
        flow_df = pd.read_sql(flow_query, conn)
        foreign_buy_raw = flow_df.pivot(index='date', columns='ticker', values='foreign_net_buy')
        inst_buy_raw = flow_df.pivot(index='date', columns='ticker', values='institution_net_buy')
        retail_buy_raw = flow_df.pivot(index='date', columns='ticker', values='individual_net_buy')
        foreign_own_raw = flow_df.pivot(index='date', columns='ticker', values='foreign_ownership')
        has_flow = True
    except Exception as e:
        print(f"   âš ï¸ ìˆ˜ê¸‰ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        has_flow = False

    conn.close()

    # â”€â”€ íŒŒìƒ ê¸°ìˆ ì  ì§€í‘œ â”€â”€
    vwap = (high + low + close) / 3
    high_low_range = (high - low) / close
    body = (close - open_price) / open_price
    upper_shadow = (high - close.clip(lower=open_price)) / close
    lower_shadow = (close.clip(upper=open_price) - low) / close

    # ATR (Average True Range) â€” ë³€ë™ì„± ì§€í‘œ
    tr1 = high - low
    tr2 = (high - close.shift(1)).abs()
    tr3 = (low - close.shift(1)).abs()
    true_range = pd.concat([tr1, tr2, tr3]).groupby(level=0).max()
    true_range = true_range.reindex(close.index)  # ì¸ë±ìŠ¤ ì •ë ¬
    atr_ratio = true_range / close  # ê°€ê²© ëŒ€ë¹„ ATR

    # ê±°ë˜ëŒ€ê¸ˆ (amount = close Ã— volume) â€” ìœ ë™ì„± ì§€í‘œ
    amount = close * volume

    # Amihud ë¹„ìœ ë™ì„± (|returns| / amount) â€” ìœ ë™ì„± ì—­ìˆ˜
    amihud = returns.abs() / amount.replace(0, np.nan)
    amihud = amihud.replace([np.inf, -np.inf], np.nan).fillna(0)

    # ê°­ (gap = open / prev_close - 1) â€” ì•¼ê°„ ì •ë³´
    gap = open_price / close.shift(1) - 1

    # ì¥ì¤‘ ìˆ˜ìµë¥  (intraday = close / open - 1) â€” ì¥ì¤‘ ì›€ì§ì„
    intraday_ret = close / open_price - 1

    # ê±°ë˜ëŸ‰ íšŒì „ìœ¨ ìƒëŒ€ë¹„ (volume / 20ì¼ í‰ê· )
    vol_ratio = volume / volume.rolling(20, min_periods=5).mean()
    vol_ratio = vol_ratio.replace([np.inf, -np.inf], np.nan).fillna(1)

    # â”€â”€ ìˆ˜ê¸‰ ë¹„ìœ¨ ê³„ì‚° â”€â”€
    if has_flow:
        # price_dataì™€ ì¸ë±ìŠ¤/ì»¬ëŸ¼ ì •ë ¬
        foreign_buy_raw = foreign_buy_raw.reindex(index=close.index, columns=close.columns)
        inst_buy_raw = inst_buy_raw.reindex(index=close.index, columns=close.columns)
        retail_buy_raw = retail_buy_raw.reindex(index=close.index, columns=close.columns)
        foreign_own_raw = foreign_own_raw.reindex(index=close.index, columns=close.columns)

        # ìˆœë§¤ìˆ˜ ë¹„ìœ¨ = ìˆœë§¤ìˆ˜ì£¼ìˆ˜ / ê±°ë˜ëŸ‰ (clip to [-1, 1])
        safe_volume = volume.replace(0, np.nan)
        foreign_net_ratio = (foreign_buy_raw / safe_volume).clip(-1, 1).fillna(0)
        inst_net_ratio = (inst_buy_raw / safe_volume).clip(-1, 1).fillna(0)
        retail_net_ratio = (retail_buy_raw / safe_volume).clip(-1, 1).fillna(0)
        foreign_ownership_pct = (foreign_own_raw / 100).clip(0, 1).fillna(0)
        print(f"   ìˆ˜ê¸‰ ì§€í‘œ 4ê°œ ë¡œë“œ (foreign/inst/retail net ratio + ownership)")
    else:
        foreign_net_ratio = close * 0.0
        inst_net_ratio = close * 0.0
        retail_net_ratio = close * 0.0
        foreign_ownership_pct = close * 0.0

    tech_vars = [
        'close', 'open_price', 'high', 'low', 'volume', 'returns',
        'vwap', 'high_low_range', 'body', 'upper_shadow', 'lower_shadow',
        'atr_ratio', 'amount', 'amihud', 'gap', 'intraday_ret', 'vol_ratio',
        'foreign_net_ratio', 'inst_net_ratio', 'retail_net_ratio', 'foreign_ownership_pct',
    ]
    print(f"âœ… {len(close.columns)}ê°œ ì¢…ëª©, {len(close)}ì¼ ë°ì´í„°")
    print(f"   ì§€í‘œ ({len(tech_vars)}ê°œ): {', '.join(tech_vars)}")

    return {
        'close': close,
        'open_price': open_price,
        'high': high,
        'low': low,
        'volume': volume,
        'returns': returns,
        'vwap': vwap,
        'high_low_range': high_low_range,
        'body': body,
        'upper_shadow': upper_shadow,
        'lower_shadow': lower_shadow,
        'atr_ratio': atr_ratio,
        'amount': amount,
        'amihud': amihud,
        'gap': gap,
        'intraday_ret': intraday_ret,
        'vol_ratio': vol_ratio,
        'foreign_net_ratio': foreign_net_ratio,
        'inst_net_ratio': inst_net_ratio,
        'retail_net_ratio': retail_net_ratio,
        'foreign_ownership_pct': foreign_ownership_pct,
    }

# â”€â”€ CogAlpha-inspired: LLM-guided Mutation + Adaptive Feedback â”€â”€

def _build_adaptive_feedback(raw_scores, prev_feedback=None):
    """ë§¤ ì„¸ëŒ€ top-3 ì„±ê³µ + bottom-3 ì‹¤íŒ¨ë¥¼ CoT ë¶„ì„í•˜ì—¬ ëˆ„ì  í”¼ë“œë°± ìƒì„±.

    CogAlpha ë…¼ë¬¸ í™•ì¥: ë” ë§ì€ ìƒ˜í”Œë¡œ í’ë¶€í•œ íŒ¨í„´ í•™ìŠµ.
    """
    valid = [(a, ic) for a, ic in raw_scores if ic > -999.0]
    if len(valid) < 6:
        return prev_feedback or ""

    top2 = valid[:3]
    bottom2 = valid[-3:]

    # íŒ©í„° ë¶„ì„ í•¨ìˆ˜
    def _analyze_factors(expr):
        factors = []
        if any(v in expr for v in ['close', 'open_price', 'high', 'low', 'vwap']):
            factors.append('price')
        if any(v in expr for v in ['volume', 'amount', 'vol_ratio']):
            factors.append('volume')
        if 'returns' in expr:
            factors.append('returns')
        if any(v in expr for v in ['high_low_range', 'body', 'upper_shadow', 'lower_shadow', 'atr_ratio']):
            factors.append('volatility')
        if any(v in expr for v in ['amihud', 'gap', 'intraday_ret']):
            factors.append('micro')
        if any(v in expr for v in ['foreign_net_ratio', 'inst_net_ratio', 'retail_net_ratio', 'foreign_ownership_pct']):
            factors.append('flow')
        return '+'.join(factors) if factors else 'unknown'

    def _extract_windows(expr):
        return [int(w) for w in re.findall(r',\s*(\d+)\)', expr)]

    feedback = "### Generation Feedback (Top-3 vs Bottom-3 analysis)\n"
    feedback += "**Top performers this generation:**\n"
    for expr, ic in top2:
        factors = _analyze_factors(expr)
        windows = _extract_windows(expr)
        win_str = f"windows={windows}" if windows else ""
        feedback += f"  - IC={ic:.4f} [{factors}] {win_str}: `{expr[:100]}`\n"

    feedback += "**Bottom performers (patterns to avoid):**\n"
    for expr, ic in bottom2:
        factors = _analyze_factors(expr)
        feedback += f"  - IC={ic:.4f} [{factors}]: `{expr[:100]}`\n"

    # ì„±ê³µ íŒ¨í„´ vs ì‹¤íŒ¨ íŒ¨í„´ ëŒ€ë¹„
    top_factors = set()
    for expr, _ in top2:
        for v in ['amihud', 'vol_ratio', 'foreign_ownership_pct', 'foreign_net_ratio',
                   'inst_net_ratio', 'vwap', 'lower_shadow', 'close', 'volume']:
            if v in expr:
                top_factors.add(v)

    bottom_factors = set()
    for expr, _ in bottom2:
        for v in ['amihud', 'vol_ratio', 'foreign_ownership_pct', 'foreign_net_ratio',
                   'inst_net_ratio', 'vwap', 'lower_shadow', 'close', 'volume']:
            if v in expr:
                bottom_factors.add(v)

    winning_vars = top_factors - bottom_factors
    if winning_vars:
        feedback += f"**Winning variables**: {', '.join(winning_vars)}\n"

    # ì´ì „ í”¼ë“œë°±ê³¼ ë³‘í•© (ìµœê·¼ 3ì„¸ëŒ€ê¹Œì§€ë§Œ ìœ ì§€)
    if prev_feedback:
        prev_lines = prev_feedback.strip().split('\n')
        # ì´ì „ í”¼ë“œë°±ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì¸ì‚¬ì´íŠ¸ë§Œ ìœ ì§€
        kept = [l for l in prev_lines if l.startswith('**') or l.startswith('  - IC=')]
        if len(kept) > 12:
            kept = kept[:12]  # ìµœê·¼ ê²ƒë§Œ (ë” ê¸´ ê¸°ì–µ)
        feedback += "\n### Previous generation insights:\n" + '\n'.join(kept) + "\n"

    return feedback


def _llm_guided_mutation(top_alphas, adaptive_feedback, num_mutations=15):
    """CogAlpha-inspired LLM-guided mutation (v11 â€” 7 Diversification Modes).

    ëœë¤ ë³€ì´ ëŒ€ì‹  GPT-4oê°€ ê¸ˆìœµ ë¡œì§ì„ ì´í•´í•˜ë©´ì„œ ë³€ì´ ìˆ˜í–‰.
    7ê°€ì§€ Diversification Guidance Mode + ëª¨ë“œë³„ ì°¨ë“± temperature ì ìš©.
    """
    try:
        client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
    except Exception:
        return []

    # Top ì•ŒíŒŒë“¤ì„ ë¶„ì„ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë” ë„“ì€ ë¶€ëª¨ í’€)
    parent_block = ""
    for i, (expr, ic) in enumerate(top_alphas[:8], 1):
        parent_block += f"  Parent #{i} (IC={ic:.4f}): `{expr}`\n"

    prompt = f"""### Task: Intelligent Alpha Mutation (7-Mode Diversity Expansion)
You are an expert quant researcher performing **guided mutation** on high-performing alpha expressions.
Unlike random mutation, you understand the financial logic behind each expression and make targeted improvements.
Your goal is to MAXIMIZE DIVERSITY â€” each mutation should explore a meaningfully different region of alpha space.

### Parent Alphas (these already work well â€” improve them):
{parent_block}
{adaptive_feedback}
### Mutation Guidance Modes (apply ALL 7 modes, {num_mutations} total mutations):

**Mode 1 - Light** (2 mutations): Fine-tune lookback windows only. If a parent uses ts_mean(x, 5), try 8 or 10.
  Consider: Monthly rebalancing (20d) favors medium-to-long windows (10-150d).

**Mode 2 - Moderate** (2 mutations): Replace operators with similar ones (e.g., ts_meanâ†’ts_ema, ts_medianâ†’ts_decayed_linear).
  Keep the same financial logic but change computation method.

**Mode 3 - Creative** (2 mutations): Add a COMPLETELY NEW variable to an existing parent.
  Example: If parent uses amihud/close_MA, add inst_net_ratio or gap or atr_ratio.
  IMPORTANT: Use variables NOT already present in the parent.

**Mode 4 - Divergent** (2 mutations): Combine building blocks from 2+ DISTANT parents into a new alpha.
  Example: Take the numerator structure from Parent #1 and the normalization from Parent #6.
  Choose parents that are structurally MOST different from each other.

**Mode 5 - Concrete** (2 mutations): Create a precise refinement based on the feedback analysis.
  If feedback says "flow variables win", create a new flow-centric combination.

**Mode 6 - Orthogonal** (2 mutations): Use `ops.ts_regression_residual(y, x, window)` to create signals
  that are ORTHOGONAL to existing parents. Extract what existing alphas CANNOT explain.
  Example: `ops.normed_rank(ops.ts_regression_residual(returns, vol_ratio, 20))` = returns unexplained by volume
  Example: `ops.zscore_scale(ops.ts_regression_residual(close, foreign_net_ratio, 30))` = price moves unexplained by foreign flow

**Mode 7 - Conditional** (1 mutation): Use `ops.sign()`, `ops.greater()`, or `ops.relu()` to create
  regime-conditional alphas that behave differently in different market states.
  Example: `ops.normed_rank(ops.cwise_mul(ops.sign(ops.ts_delta(close, 20)), ops.ts_ir(returns, 10)))` = momentum direction Ã— IR
  Example: `ops.normed_rank(ops.cwise_mul(ops.relu(ops.ts_delta_ratio(close, 15)), ops.div(amihud, ops.ts_mean(amihud, 60))))` = only positive momentum Ã— illiquidity

**+ 2 Bonus mutations**: Your most creative ideas combining ANY of the above modes.

### Available Data (21 variables)
close, open_price, high, low, volume, returns, vwap, high_low_range, body, upper_shadow, lower_shadow,
atr_ratio, amount, amihud, gap, intraday_ret, vol_ratio,
foreign_net_ratio, inst_net_ratio, retail_net_ratio, foreign_ownership_pct

### Operator DSL
Time-series (1-var): ts_mean, ts_std, ts_median, ts_ema, ts_linear_reg, ts_delta, ts_delta_ratio,
  ts_zscore_scale, ts_maxmin_scale, ts_rank, ts_ir, ts_skew, ts_min, ts_max, ts_decayed_linear
Time-series (2-var): ts_corr(x, y, window), ts_regression_residual(y, x, window)
Cross-sectional: normed_rank, zscore_scale
Arithmetic: add, minus, cwise_mul, div, neg, abs, log
Conditional: sign(x), relu(x), greater(x, y)

### Rules
- Wrap every alpha with `ops.normed_rank()` or `ops.zscore_scale()`
- Use `ops.` prefix for ALL operators
- Use 2+ lookback windows (multi-timeframe)
- Complexity: 2~5 nesting levels
- Window range: 3~150 (explore extreme short/long windows)
- Try using ts_regression_residual for orthogonal signals
- MAXIMIZE DIVERSITY: each mutation should use DIFFERENT variable combinations

### Output Format
{{"mutations": [
  {{"mode": "Light|Moderate|Creative|Divergent|Concrete|Orthogonal|Conditional",
    "parent_id": 1,
    "reasoning": "Why this mutation improves the parent",
    "expression": "ops.normed_rank(...)"}}
]}}

**CRITICAL**: Return exactly {num_mutations} mutations with valid ops.xxx() expressions.
Modes 3,4,6,7 should be BOLD and explore novel territory â€” don't play it safe."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are an expert quantitative alpha researcher performing intelligent guided mutation on financial alpha expressions. Prioritize DIVERSITY over incremental improvement. Return your response as a valid JSON object."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=8000,
            response_format={"type": "json_object"}
        )

        content = response.choices[0].message.content
        data = json.loads(content)

        mutations = []
        items = []
        if isinstance(data, dict):
            for key in data:
                val = data[key]
                if isinstance(val, list):
                    items = val
                    break

        for item in items:
            expr = None
            if isinstance(item, dict):
                expr = item.get('expression') or item.get('expr')
            elif isinstance(item, str):
                expr = item

            if expr and 'ops.' in expr and expr.count('(') == expr.count(')'):
                mutations.append(expr)

        # ë§ˆí¬ë‹¤ìš´ í´ë°±
        if not mutations:
            for line in content.split('\n'):
                line = line.strip()
                if line.startswith('ops.') and '(' in line:
                    expr = line.strip('",` ')
                    if expr.count('(') == expr.count(')'):
                        mutations.append(expr)

        return mutations[:num_mutations]

    except Exception as e:
        print(f"      âš ï¸ LLM mutation ì‹¤íŒ¨: {e}")
        return []


def _llm_guided_crossover(top_alphas, adaptive_feedback, num_children=8):
    """CogAlpha-inspired LLM-guided crossover (v11 â€” ë” ë„“ì€ ë¶€ëª¨ í’€ + ì›ê±°ë¦¬ êµì°¨).

    ë‘ ë¶€ëª¨ ì•ŒíŒŒì˜ ê¸ˆìœµ ë¡œì§ì„ ì´í•´í•˜ê³  ì˜ë¯¸ìˆëŠ” êµì°¨ë¥¼ ìˆ˜í–‰.
    ë” ë¨¼ ë¶€ëª¨ ê°„ êµì°¨ë¡œ ë‹¤ì–‘ì„± ê·¹ëŒ€í™”.
    """
    try:
        client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
    except Exception:
        return []

    parent_block = ""
    for i, (expr, ic) in enumerate(top_alphas[:10], 1):
        parent_block += f"  Parent #{i} (IC={ic:.4f}): `{expr}`\n"

    prompt = f"""### Task: Intelligent Alpha Crossover
Combine building blocks from multiple parent alphas to create novel offspring.

### Parent Alphas:
{parent_block}
{adaptive_feedback}
### Crossover Strategy
For each offspring:
1. Select 2 parents â€” PREFER DISTANT parents (e.g., #1 Ã— #8, not #1 Ã— #2) for maximum novelty
2. Identify the "winning component" from each (e.g., the numerator logic, the normalization method, the variable selection)
3. Combine them into a new alpha that inherits strengths from both parents
4. Explain WHY this combination should work
5. At least 2 crossovers should use ts_regression_residual or conditional operators (sign/greater/relu)

### Rules
- Wrap with `ops.normed_rank()` or `ops.zscore_scale()`
- Use `ops.` prefix for ALL operators
- 2+ lookback windows, 2~5 nesting levels
- Window range: 3~150
- MAXIMIZE the number of UNIQUE variable combinations across offspring

### Output Format
{{"crossovers": [
  {{"parent1_id": 1, "parent2_id": 8,
    "reasoning": "Combines X's liquidity signal with Y's flow signal",
    "expression": "ops.normed_rank(...)"}}
]}}

Generate exactly {num_children} crossover offspring. Each MUST use a different variable combination."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are an expert quantitative alpha researcher performing intelligent crossover on financial alpha expressions. Prioritize DISTANT parent combinations for maximum novelty. Return your response as a valid JSON object."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.6,
            max_tokens=8000,
            response_format={"type": "json_object"}
        )

        content = response.choices[0].message.content
        data = json.loads(content)

        children = []
        items = []
        if isinstance(data, dict):
            for key in data:
                val = data[key]
                if isinstance(val, list):
                    items = val
                    break

        for item in items:
            expr = None
            if isinstance(item, dict):
                expr = item.get('expression') or item.get('expr')
            elif isinstance(item, str):
                expr = item

            if expr and 'ops.' in expr and expr.count('(') == expr.count(')'):
                children.append(expr)

        return children[:num_children]

    except Exception as e:
        print(f"      âš ï¸ LLM crossover ì‹¤íŒ¨: {e}")
        return []


def _load_previous_results():
    """DBì—ì„œ ì´ì „ GP ê²°ê³¼ (best/worst) ë¡œë“œ"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            SELECT formula, ic_score, description
            FROM alpha_formulas
            WHERE description LIKE '%15d fwd%'
            ORDER BY ic_score DESC
            LIMIT 20
        """)
        rows = cursor.fetchall()
        cursor.close()
        conn.close()
        if rows:
            best = [(r[0], r[1], r[2]) for r in rows[:5]]
            worst = [(r[0], r[1], r[2]) for r in rows[-3:] if r[1] is not None]
            return best, worst
    except Exception:
        pass
    return [], []


def generate_seed_alphas_gpt4o(num_seeds=30):
    """2ë‹¨ê³„ ì‹œë“œ ìƒì„±: (1) ê°€ì„¤ 10ê°œ ìƒì„± â†’ (2) ê°€ì„¤ ê¸°ë°˜ ì•ŒíŒŒ ìƒì„± + ì´ì „ ê²°ê³¼ í”¼ë“œë°±"""
    print(f"\nğŸ¤– GPT-4o 2ë‹¨ê³„ ì‹œë“œ ìƒì„± (ê°€ì„¤â†’ì•ŒíŒŒ, {num_seeds}ê°œ)...")

    client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
    system_prompt = QuantDeveloper.SYSTEM_PROMPT

    # â”€â”€ ì´ì „ GP ê²°ê³¼ ë¡œë“œ â”€â”€
    prev_best, prev_worst = _load_previous_results()
    feedback_block = ""
    if prev_best:
        feedback_block += "\n### Previous GP Results (LEVERAGE these insights)\n"
        feedback_block += "**Top performers (replicate and extend these patterns):**\n"
        for expr, ic, desc in prev_best[:5]:
            test_ic_str = ""
            if desc:
                import re as _re
                m = _re.search(r'test IC=([0-9.-]+)', desc)
                if m:
                    test_ic_str = f", test IC={m.group(1)}"
            feedback_block += f"  - IC={ic:.4f}{test_ic_str}: `{expr[:120]}`\n"
        feedback_block += "\n**Patterns to AVOID (these failed OOS validation):**\n"
        for expr, ic, desc in prev_worst:
            feedback_block += f"  - IC={ic:.4f}: `{expr[:100]}`\n"
        feedback_block += "\n**Key lessons**: Focus on patterns similar to top performers. "
        feedback_block += "Explore NEW combinations of successful building blocks (MA slope, amihud, lower_shadow, vwap).\n"
        print(f"   ì´ì „ ê²°ê³¼ í”¼ë“œë°±: best {len(prev_best)}ê°œ, worst {len(prev_worst)}ê°œ")

    # â”€â”€ 1ë‹¨ê³„: ê°€ì„¤ ìƒì„± (temperature=0.5, ë‹¤ì–‘í•œ ê°€ì„¤) â”€â”€
    print("   [1/2] ê°€ì„¤ ìƒì„± ì¤‘...")
    hypothesis_prompt = f"""You are a quantitative finance researcher. Generate 10 structured trading hypotheses
for a **20-day (1-month) holding period** strategy in the Korean stock market (KRX).

Each hypothesis must follow this EXACT JSON format:
{{"hypotheses": [
  {{
    "hypothesis": "Complete hypothesis statement",
    "reason": "Why this captures alpha â€” economic/behavioral explanation",
    "observation": "Key market pattern or anomaly being exploited",
    "knowledge": "If [condition], then [expected outcome] over 20 trading days (~1 month)"
  }}
]}}

Generate EXACTLY 10 hypotheses, one per theme:
1. **Momentum + Volume confirmation**: Price trend confirmed by trading activity pattern
2. **Volatility regime + Mean-reversion**: Candle body/shadow patterns predicting reversals
3. **Liquidity premium**: Amihud illiquidity ratio combined with price structure
4. **Multi-timeframe divergence**: Short-term vs long-term momentum disagreement
5. **Cross-variable decorrelation**: Using ts_regression_residual to extract orthogonal signals
   (e.g., returns not explained by volume, price moves not explained by flow)
6. **Microstructure signals**: Gap, intraday returns, candle shape as information signals
7. **Institutional flow momentum**: ê¸°ê´€/ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜ ì¶”ì„¸ê°€ ê°€ê²©ì— ì„ í–‰í•˜ëŠ” íŒ¨í„´
   (foreign_net_ratio, inst_net_ratioì˜ ëˆ„ì  íë¦„ì´ í–¥í›„ ìˆ˜ìµë¥ ì„ ì˜ˆì¸¡)
8. **Volatility compression breakout**: ATR ìˆ˜ì¶• í›„ í™•ì¥ â†’ ì¶”ì„¸ ì‹œì‘ ì‹ í˜¸
   (atr_ratioê°€ ë‚®ì•„ì¡Œë‹¤ê°€ ë†’ì•„ì§€ëŠ” ì¢…ëª©ì´ 20ì¼ í›„ ìˆ˜ìµë¥  ë†’ìŒ)
9. **Turnover anomaly**: ê±°ë˜ëŒ€ê¸ˆ(amount) ê¸°ë°˜ ìœ ë™ì„± í”„ë¦¬ë¯¸ì—„
   (ê±°ë˜ëŒ€ê¸ˆ ë³€í™”ìœ¨ê³¼ ê°€ê²© ëª¨ë©˜í…€ì˜ ë¹„ì„ í˜• ê´€ê³„)
10. **Regime-conditional alpha**: ë³€ë™ì„± ë ˆì§ì— ë”°ë¼ ë‹¤ë¥¸ ì‹ í˜¸ ì ìš©
    (sign/greater ì¡°ê±´ ì—°ì‚°ìë¡œ ê³ ë³€ë™ì„± vs ì €ë³€ë™ì„± êµ¬ê°„ ë¶„ê¸°)
{feedback_block}
Available data: close, open_price, high, low, volume, returns, vwap, high_low_range, body,
upper_shadow, lower_shadow, atr_ratio, amount, amihud, gap, intraday_ret, vol_ratio,
foreign_net_ratio, inst_net_ratio, retail_net_ratio, foreign_ownership_pct"""

    try:
        hyp_response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are an expert quantitative researcher specializing in KRX alpha factor hypothesis generation."},
                {"role": "user", "content": hypothesis_prompt}
            ],
            temperature=0.5,
            max_tokens=6000,
            response_format={"type": "json_object"}
        )
        hyp_content = hyp_response.choices[0].message.content
        hyp_data = json.loads(hyp_content)
        hypotheses = hyp_data.get('hypotheses', [hyp_data]) if isinstance(hyp_data, dict) else [hyp_data]
        if not isinstance(hypotheses, list):
            hypotheses = [hypotheses]
        print(f"   âœ… {len(hypotheses)}ê°œ ê°€ì„¤ ìƒì„±")
        for i, h in enumerate(hypotheses[:4], 1):
            hyp_text = h.get('hypothesis', h.get('reason', '?'))
            print(f"      {i}. {hyp_text[:80]}...")
    except Exception as e:
        print(f"   âš ï¸  ê°€ì„¤ ìƒì„± ì‹¤íŒ¨: {e}, ê¸°ë³¸ ê°€ì„¤ ì‚¬ìš©")
        hypotheses = [
            {"hypothesis": "Momentum confirmed by volume surge predicts 20-day continuation",
             "knowledge": "If price rises with above-average volume, then trend continues for 20 days"},
            {"hypothesis": "Low volatility stocks with buying pressure show mean-reversion alpha",
             "knowledge": "If ATR is low and lower_shadow is large, then price rebounds within 20 trading days"},
            {"hypothesis": "Illiquid stocks with trend signals offer liquidity premium",
             "knowledge": "If amihud is high and MA slope is positive, then excess returns over 20 trading days"},
            {"hypothesis": "Short-term vs long-term momentum divergence signals regime change",
             "knowledge": "If 5-day momentum diverges from 60-day trend, then reversal within 20 trading days"},
            {"hypothesis": "Returns orthogonal to volume activity predict future returns",
             "knowledge": "If stock returns are high but not explained by volume, then alpha persists for 20 days"},
            {"hypothesis": "Overnight gap + intraday reversal patterns predict next month",
             "knowledge": "If gap and intraday return diverge, then price corrects within 20 trading days"},
            {"hypothesis": "Institutional flow momentum leads price by 1-4 weeks",
             "knowledge": "If foreign_net_ratio or inst_net_ratio accumulates over 10-20 days, then price follows within 20 trading days"},
            {"hypothesis": "Volatility compression followed by expansion signals breakout",
             "knowledge": "If atr_ratio contracts then expands, then directional move occurs within 20 trading days"},
            {"hypothesis": "Trading amount anomalies predict liquidity-driven returns",
             "knowledge": "If amount surges relative to history while price is flat, then price catches up within 20 days"},
            {"hypothesis": "Regime-conditional signals: different alphas work in different volatility regimes",
             "knowledge": "If market volatility is low, momentum works; if high, mean-reversion works over 20 days"},
        ]

    # â”€â”€ 2ë‹¨ê³„: ê°€ì„¤ ê¸°ë°˜ ì•ŒíŒŒ ìƒì„± (temperature=0.7, ë‹¤ì–‘ì„± ê·¹ëŒ€í™”) â”€â”€
    print("   [2/2] ê°€ì„¤ ê¸°ë°˜ ì•ŒíŒŒ ìƒì„± ì¤‘...")
    hypotheses_text = ""
    for i, h in enumerate(hypotheses[:10], 1):
        hyp = h.get('hypothesis', '') or h.get('reason', '')
        knowledge = h.get('knowledge', '') or h.get('concise_knowledge', '')
        hypotheses_text += f"\n**Hypothesis {i}**: {hyp}\n"
        if knowledge:
            hypotheses_text += f"  Knowledge: {knowledge}\n"

    alphas_per_hyp = num_seeds // 10
    remaining = num_seeds - alphas_per_hyp * 10

    prompt = f"""### Task
Generate {num_seeds} diverse alpha expressions for **20-day (1-month) forward returns** in KRX.
Each alpha MUST be grounded in one of the hypotheses below.
{hypotheses_text}
### Alpha Generation Rules
- Generate {alphas_per_hyp} alphas per hypothesis ({alphas_per_hyp}Ã—10 = {alphas_per_hyp*10}), plus {remaining} bonus composite alphas.
- Each alpha MUST reference which hypothesis (1-6) it implements.
{feedback_block}

### âš ï¸ BANNED PATTERNS (these are overfit â€” DO NOT generate anything similar)
- `ops.div(ops.ts_mean(foreign_ownership_pct, N), ops.ts_decayed_linear(vol_ratio, N))` â€” overfit on quarterly data
- Any alpha where `foreign_ownership_pct` is the PRIMARY driver (it's forward-filled quarterly, not daily)
- `amihud / ts_mean(close, N)` as the ONLY signal â€” too simplistic, already discovered
- Any simple combination of ONLY foreign_ownership_pct + vol_ratio + amihud + close

### âœ… DIVERSITY REQUIREMENTS (MUST follow)
- At least 3 alphas MUST use `ts_corr(x, y, window)` for cross-variable correlation
- At least 2 alphas MUST use `ts_regression_residual(y, x, window)` for orthogonal signals
- At least 2 alphas MUST use conditional operators: `sign()`, `relu()`, or `greater()`
- At least 4 alphas MUST use NONE of the flow variables (pure technical: close/volume/vwap/body/shadow/gap/amihud etc.)
- Each alpha MUST use a DIFFERENT main variable combination from the others

### Available Data (21 variables: 17 technical + 4 supply/demand)
**Price**: close, open_price, high, low, vwap
**Volume**: volume, amount, vol_ratio
**Returns**: returns
**Volatility/Shape**: high_low_range, body, upper_shadow, lower_shadow, atr_ratio
**Microstructure**: amihud (illiquidity), gap (overnight), intraday_ret (intraday)
**Investor Flow**: foreign_net_ratio, inst_net_ratio, retail_net_ratio, foreign_ownership_pct

### Operator DSL
Time-series (1-var): ts_mean, ts_std, ts_median, ts_ema, ts_linear_reg, ts_delta, ts_delta_ratio,
  ts_zscore_scale, ts_maxmin_scale, ts_rank, ts_ir, ts_skew, ts_min, ts_max, ts_decayed_linear
Time-series (2-var):
  ts_corr(x, y, window) â€” rolling correlation between two variables
  ts_regression_residual(y, x, window) â€” rolling OLS residual (y unexplained by x)
Cross-sectional: normed_rank, zscore_scale
Arithmetic: add, minus, cwise_mul, div, neg, abs, log
Conditional: sign(x) â€” returns -1/0/+1, relu(x) â€” max(0, x), greater(x, y) â€” 1 if x>y else 0

### HIGH-VALUE STRUCTURAL TEMPLATES (use these as building blocks)
1. **Cross-correlation**: `ops.ts_corr(returns, vol_ratio, 20)` â€” price-volume divergence
2. **Orthogonal signal**: `ops.ts_regression_residual(returns, vol_ratio, 30)` â€” returns NOT explained by volume
3. **Directional filter**: `ops.cwise_mul(ops.sign(ops.ts_delta(close, 20)), other_signal)` â€” only long in uptrend
4. **Asymmetric capture**: `ops.relu(ops.ts_delta_ratio(vwap, 15))` â€” upside momentum only
5. **Conditional regime**: `ops.cwise_mul(ops.greater(ops.ts_mean(volume, 5), ops.ts_mean(volume, 20)), signal)` â€” volume breakout filter
6. **Divergence signal**: `ops.minus(ops.ts_rank(close, 20), ops.ts_rank(volume, 20))` â€” price-volume rank divergence
7. **Residual momentum**: `ops.ts_linear_reg(ops.ts_regression_residual(returns, vol_ratio, 30), 10)` â€” trend in residual returns

### Quality Rules
- Wrap every alpha with `ops.normed_rank()` or `ops.zscore_scale()`
- Use 2+ lookback windows per alpha (multi-timeframe)
- Use `ops.div()` for division (safe)
- Prefer `ops.cwise_mul()` for multiplicative signals
- Complexity: 2~4 nesting levels
- MAXIMIZE diversity: each alpha should explore a DIFFERENT combination of variables and operators

### Output Format
{{"alphas": [
  {{"alpha_name": "...", "hypothesis_id": 1, "category": "...",
    "rationale": "...", "expression": "ops.normed_rank(...)"}}
]}}

**CRITICAL**: Return a valid JSON object with {num_seeds} alphas total. Each MUST have "expression" with valid ops.xxx() code."""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=16000,
        response_format={"type": "json_object"}
    )

    content = response.choices[0].message.content
    print(f"   GPT-4o ì‘ë‹µ ê¸¸ì´: {len(content)}ì")
    print(f"   ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {content[:200]}...")

    # JSON íŒŒì‹±
    alphas = []
    try:
        data = json.loads(content)
        print(f"   íŒŒì‹±ëœ íƒ€ì…: {type(data).__name__}")

        # dict â†’ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        if isinstance(data, dict):
            print(f"   í‚¤ ëª©ë¡: {list(data.keys())}")

            # 1ìˆœìœ„: dict ìì²´ê°€ ë‹¨ì¼ ì•ŒíŒŒì¸ ê²½ìš° (expression í‚¤ ì¡´ì¬)
            if 'expression' in data or 'expr' in data:
                data = [data]
                print(f"   ë‹¨ì¼ ì•ŒíŒŒ dict â†’ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜")

            else:
                # 2ìˆœìœ„: {"alphas": [{...}, ...]} í˜•íƒœ â€” dict ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì§„ í‚¤ ì°¾ê¸°
                found_list = False
                for key in data:
                    if isinstance(data[key], list) and data[key] and isinstance(data[key][0], dict):
                        data = data[key]
                        print(f"   '{key}' í‚¤ì—ì„œ {len(data)}ê°œ í•­ëª© ì¶”ì¶œ")
                        found_list = True
                        break

                if not found_list:
                    # 3ìˆœìœ„: ì¤‘ì²© dict: {"alpha_1": {...}, "alpha_2": {...}}
                    items = []
                    for key, val in data.items():
                        if isinstance(val, dict) and ('expression' in val or 'expr' in val):
                            items.append(val)
                    if items:
                        data = items
                        print(f"   ì¤‘ì²© dictì—ì„œ {len(items)}ê°œ í•­ëª© ì¶”ì¶œ")
                    else:
                        print(f"   âš ï¸  ì•Œ ìˆ˜ ì—†ëŠ” dict êµ¬ì¡°: {list(data.keys())[:5]}")
                        data = []

        for item in data:
            if isinstance(item, str):
                if 'ops.' in item:
                    alphas.append(item)
                continue
            if not isinstance(item, dict):
                continue
            expr = item.get('expression', item.get('expr', ''))
            if expr and 'ops.' in expr:
                alphas.append(expr)
            elif expr:
                print(f"   âš ï¸  ops. ì—†ëŠ” í‘œí˜„ì‹ ìŠ¤í‚µ: {expr[:80]}")

    except (json.JSONDecodeError, Exception) as e:
        print(f"âš ï¸  JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì•ˆì˜ JSON ì¶”ì¶œ ì‹œë„
        json_match = re.search(r'```(?:json)?\s*(\[.*?\])\s*```', content, re.DOTALL)
        if json_match:
            try:
                data = json.loads(json_match.group(1))
                for item in data:
                    if isinstance(item, dict):
                        expr = item.get('expression', item.get('expr', ''))
                        if expr and 'ops.' in expr:
                            alphas.append(expr)
                print(f"   ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì—ì„œ {len(alphas)}ê°œ ë³µêµ¬")
            except Exception:
                pass

    # â”€â”€ ê²€ì¦ëœ OOS-validated ì‹œë“œ (í•µì‹¬ë§Œ, ë‹¤ì–‘ì„± ìš°ì„ ) â”€â”€
    proven_seeds = [
        # [price+micro] vwap MA slope + amihud (IC 0.060)
        "ops.normed_rank(ops.add(ops.normed_rank(ops.ts_delta_ratio(ops.ts_mean(vwap, 120), 10)), ops.normed_rank(ops.div(amihud, ops.ts_median(close, 20)))))",
        # [price] lower_shadow + close MA (IC 0.127)
        "ops.normed_rank(ops.add(ops.ts_mean(lower_shadow, 15), ops.div(close, ops.ts_mean(close, 120))))",
        # [price] 120ì¼ ì´ê²©ë„
        "ops.normed_rank(ops.div(close, ops.ts_mean(close, 120)))",
        # [price+volume] ëª¨ë©˜í…€ Ã— ê±°ë˜ëŸ‰ ì•ˆì •ì„±
        "ops.normed_rank(ops.cwise_mul(ops.cwise_mul(ops.ts_delta_ratio(close, 25), ops.div(ops.ts_median(volume, 10), ops.ts_std(volume, 15))), ops.ts_maxmin_scale(close, 28)))",
        # [micro] amihud / close MA
        "ops.normed_rank(ops.div(amihud, ops.ts_median(close, 20)))",
        # [price+volume] ë‹¤ì¤‘ MA Ã— ê±°ë˜ëŸ‰ + MAê¸°ìš¸ê¸°
        "ops.normed_rank(ops.add(ops.cwise_mul(ops.div(ops.ts_mean(close, 5), ops.ts_mean(close, 20)), ops.div(ops.ts_mean(volume, 5), ops.ts_mean(volume, 20))), ops.ts_delta_ratio(ops.ts_mean(close, 20), 10)))",
        # â”€â”€ ìƒˆ íŒ¨í„´: ts_regression_residual â”€â”€
        # [orthogonal] returnsì—ì„œ vol_ratio ì˜í–¥ ì œê±° â†’ ìˆœìˆ˜ ê°€ê²© ì‹œê·¸ë„
        "ops.normed_rank(ops.ts_regression_residual(returns, vol_ratio, 30))",
        # [orthogonal] close ì›€ì§ì„ì—ì„œ amihud ì˜í–¥ ì œê±° â†’ ìœ ë™ì„± í”„ë¦¬ë¯¸ì—„ ì œì™¸ ëª¨ë©˜í…€
        "ops.normed_rank(ops.ts_linear_reg(ops.ts_regression_residual(close, amihud, 20), 10))",
        # [orthogonal + micro] ìˆ˜ìµë¥  ì”ì°¨ Ã— gap ë°˜ì „
        "ops.normed_rank(ops.cwise_mul(ops.ts_regression_residual(returns, vol_ratio, 20), ops.neg(ops.ts_mean(gap, 10))))",
        # â”€â”€ ìƒˆ íŒ¨í„´: rank divergence â”€â”€
        # [divergence] close rank vs volume rank ê´´ë¦¬ (ê°€ê²© ê³¼ì—´/ê³¼ëƒ‰ íƒì§€)
        "ops.normed_rank(ops.minus(ops.ts_rank(close, 20), ops.ts_rank(volume, 20)))",
        # [divergence] vwap rank vs amihud rank ê´´ë¦¬
        "ops.normed_rank(ops.minus(ops.ts_rank(vwap, 30), ops.ts_rank(amihud, 30)))",
        # â”€â”€ ê¸°ì¡´ ê²€ì¦ íŒ¨í„´: ts_corr â”€â”€
        # [corr] ê°€ê²©-ê±°ë˜ëŸ‰ ìƒê´€ ì—­ì „
        "ops.normed_rank(ops.neg(ops.ts_corr(returns, vol_ratio, 20)))",
        # [conditional] ìƒìŠ¹ ì¶”ì„¸ì—ì„œë§Œ ë¹„ìœ ë™ì„± í”„ë¦¬ë¯¸ì—„
        "ops.normed_rank(ops.cwise_mul(ops.sign(ops.ts_delta(close, 20)), ops.div(amihud, ops.ts_mean(close, 60))))",
        # [conditional+volume] ê±°ë˜ëŸ‰ ê¸‰ì¦ êµ¬ê°„ì—ì„œ body ì‹œê·¸ë„
        "ops.normed_rank(ops.cwise_mul(ops.greater(ops.ts_mean(volume, 5), ops.ts_mean(volume, 20)), ops.ts_mean(body, 10)))",
    ]

    # í•­ìƒ ê²€ì¦ëœ ì‹œë“œë¥¼ ë¨¼ì € í¬í•¨ + GPT-4o ì‹œë“œ ì¶”ê°€
    proven_not_in = [s for s in proven_seeds if s not in alphas]
    gpt_alphas = [a for a in alphas if a not in proven_seeds]
    alphas = proven_not_in + gpt_alphas  # ê²€ì¦ëœ ì‹œë“œ ìš°ì„ 
    print(f"   ê²€ì¦ëœ ì‹œë“œ: {len(proven_not_in)}ê°œ + GPT-4o ì‹œë“œ: {len(gpt_alphas)}ê°œ")

    # ë¶€ì¡±í•˜ë©´ ì¶”ê°€ í´ë°±
    if len(alphas) < 10:
        print(f"âš ï¸  {len(alphas)}ê°œë§Œ íŒŒì‹±ë¨, ì¶”ê°€ í´ë°±")
        extra_fallback = [
            "ops.normed_rank(ops.cwise_mul(ops.div(close, ops.ts_mean(close, 20)), ops.div(ops.ts_mean(volume, 5), ops.ts_mean(volume, 20))))",
            "ops.normed_rank(ops.cwise_mul(ops.ts_delta_ratio(close, 15), ops.div(ops.ts_mean(volume, 5), ops.ts_mean(volume, 20))))",
            "ops.normed_rank(ops.neg(ops.ts_corr(ops.ts_delta(close, 5), ops.ts_delta(volume, 5), 20)))",
            "ops.normed_rank(ops.cwise_mul(ops.ts_delta_ratio(close, 20), ops.neg(ops.ts_mean(high_low_range, 15))))",
            "ops.normed_rank(ops.ts_linear_reg(close, 20))",
            "ops.normed_rank(ops.ts_maxmin_scale(close, 60))",
            "ops.normed_rank(ops.div(close, ops.ts_mean(vwap, 20)))",
            "ops.normed_rank(ops.ts_mean(body, 10))",
            "ops.normed_rank(ops.div(ops.ts_mean(volume, 5), ops.ts_mean(volume, 60)))",
            "ops.normed_rank(ops.ts_zscore_scale(close, 20))",
            "ops.normed_rank(ops.ts_delta_ratio(ops.ts_mean(close, 120), 20))",
        ]
        alphas = alphas + [f for f in extra_fallback if f not in alphas]

    print(f"âœ… {len(alphas)}ê°œ ì´ˆê¸° ì•ŒíŒŒ ìƒì„± (ê²€ì¦ëœ ì‹œë“œ í¬í•¨)")
    for i, a in enumerate(alphas[:5], 1):
        print(f"   {i}. {a[:80]}...")

    return alphas[:num_seeds]

# ì „ì—­ ë°ì´í„°
_global_data = None
_global_train_start_date = None
_global_train_end_date = None

def set_global_data(data, train_start_date=None, train_end_date=None):
    global _global_data, _global_train_start_date, _global_train_end_date
    _global_data = data
    _global_train_start_date = train_start_date
    _global_train_end_date = train_end_date

def _compute_ic_series(alpha_expr, data, date_start=None, date_end=None):
    """ì•ŒíŒŒì˜ ì¼ë³„ IC ë¦¬ìŠ¤íŠ¸ + í„´ì˜¤ë²„ ê³„ì‚°.

    Returns:
        (ic_list, turnover) â€” turnover = 1 - rank_autocorrelation (ë‚®ì„ìˆ˜ë¡ ì•ˆì •)
    """
    close = data['close']
    open_price = data['open_price']
    high = data['high']
    low = data['low']
    volume = data['volume']
    returns = data['returns']
    vwap = data['vwap']
    high_low_range = data['high_low_range']
    body = data['body']
    upper_shadow = data.get('upper_shadow', (high - close.clip(lower=open_price)) / close)
    lower_shadow = data.get('lower_shadow', (close.clip(upper=open_price) - low) / close)
    atr_ratio = data.get('atr_ratio', high_low_range)
    amount = data.get('amount', close * volume)
    amihud = data.get('amihud', returns.abs() / amount.replace(0, np.nan))
    gap = data.get('gap', open_price / close.shift(1) - 1)
    intraday_ret = data.get('intraday_ret', close / open_price - 1)
    vol_ratio = data.get('vol_ratio', volume / volume.rolling(20, min_periods=5).mean())
    foreign_net_ratio = data.get('foreign_net_ratio', close * 0.0)
    inst_net_ratio = data.get('inst_net_ratio', close * 0.0)
    retail_net_ratio = data.get('retail_net_ratio', close * 0.0)
    foreign_ownership_pct = data.get('foreign_ownership_pct', close * 0.0)

    forward_return = close.shift(-20) / close - 1  # 20ì˜ì—…ì¼ (~1ë‹¬) ì„ í–‰ìˆ˜ìµë¥ 
    alpha_values = eval(alpha_expr)

    if not isinstance(alpha_values, pd.DataFrame):
        return [], 1.0

    alpha_values = alpha_values.replace([np.inf, -np.inf], np.nan)

    n_stocks = len(close.columns)
    coverage_threshold = n_stocks * 0.5
    ic_list = []
    low_coverage_days = 0
    total_days = 0

    # í„´ì˜¤ë²„: 20ì¼ ê°„ê²© rank autocorrelation
    rank_autocorrs = []
    prev_ranks = None
    day_counter = 0

    for date in alpha_values.index[:-20]:
        if date_start is not None and date < date_start:
            continue
        if date_end is not None and date > date_end:
            continue

        alpha_cs = alpha_values.loc[date]
        returns_cs = forward_return.loc[date]
        valid = alpha_cs.notna() & returns_cs.notna()
        total_days += 1

        if valid.sum() < coverage_threshold:
            low_coverage_days += 1

        if valid.sum() > 30:
            # ê·¹ë‹¨ ìˆ˜ìµë¥  í•„í„° (3-sigma, ê¸°ì—… ì´ë²¤íŠ¸/ì°©ì˜¤ ì œê±°)
            ret_mean = returns_cs[valid].mean()
            ret_std = returns_cs[valid].std()
            if ret_std > 0:
                extreme = (returns_cs - ret_mean).abs() > 3 * ret_std
                valid = valid & ~extreme
            if valid.sum() > 30:
                rho, _ = spearmanr(alpha_cs[valid].values, returns_cs[valid].values)
                if not np.isnan(rho):
                    ic_list.append(rho)

        # 20ì¼ ê°„ê²© rank autocorrelation (í„´ì˜¤ë²„ proxy)
        current_ranks = alpha_cs.rank()
        day_counter += 1
        if day_counter % 20 == 0 and prev_ranks is not None:
            joint_valid = current_ranks.notna() & prev_ranks.notna()
            if joint_valid.sum() > 30:
                rank_corr, _ = spearmanr(
                    current_ranks[joint_valid].values,
                    prev_ranks[joint_valid].values
                )
                if not np.isnan(rank_corr):
                    rank_autocorrs.append(rank_corr)
            prev_ranks = current_ranks
        elif day_counter % 20 == 0:
            prev_ranks = current_ranks

    # ì»¤ë²„ë¦¬ì§€ í˜ë„í‹°
    if total_days > 0:
        valid_day_ratio = 1.0 - (low_coverage_days / total_days)
        if valid_day_ratio < 0.8:
            ic_list = [ic * valid_day_ratio for ic in ic_list]

    # í„´ì˜¤ë²„ = 1 - rank_autocorrelation (0=ì•ˆì •, 1=ì™„ì „ êµì²´)
    avg_rank_autocorr = float(np.mean(rank_autocorrs)) if rank_autocorrs else 0.5
    turnover = 1.0 - avg_rank_autocorr

    return ic_list, turnover


def _compute_raw_ic(alpha_expr, data, date_start=None, date_end=None):
    """í•˜ìœ„ í˜¸í™˜: mean ICë§Œ ë°˜í™˜."""
    try:
        ic_list, _ = _compute_ic_series(alpha_expr, data, date_start, date_end)
        if len(ic_list) < 10:
            return -999.0
        return float(np.mean(ic_list))
    except Exception:
        return -999.0

def _multi_factor_bonus(alpha_expr):
    """ë‹¤ì¤‘ íŒ©í„° êµ¬ì¡° ë³´ë„ˆìŠ¤ â€” 10x ê°•í™”, ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ í˜ë„í‹°"""
    bonus = 0.0

    # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ â€” ë³´ë„ˆìŠ¤ 1/3 ì¶•ì†Œ (ìˆœìˆ˜ IC ìµœì í™” ì§‘ì¤‘)
    categories_used = 0
    if any(v in alpha_expr for v in ['close', 'open_price', 'high', 'low', 'vwap']):
        categories_used += 1
    if any(v in alpha_expr for v in ['volume', 'amount', 'vol_ratio']):
        categories_used += 1
        bonus += 0.003
    if 'returns' in alpha_expr:
        categories_used += 1
    if any(v in alpha_expr for v in ['high_low_range', 'atr_ratio']):
        categories_used += 1
    if any(v in alpha_expr for v in ['body', 'upper_shadow', 'lower_shadow']):
        categories_used += 1
    if any(v in alpha_expr for v in ['amihud', 'gap', 'intraday_ret']):
        categories_used += 1
        bonus += 0.003
    if any(v in alpha_expr for v in ['foreign_net_ratio', 'inst_net_ratio', 'retail_net_ratio', 'foreign_ownership_pct']):
        categories_used += 1
        bonus += 0.003

    # ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ í˜ë„í‹°
    if categories_used <= 1:
        bonus -= 0.010
    elif categories_used >= 3:
        bonus += 0.005
    elif categories_used >= 2:
        bonus += 0.002

    # MA êµ¬ì¡° ë³´ë„ˆìŠ¤
    has_ma = bool(re.search(r'ts_mean\([^)]*close[^)]*,\s*\d+\)', alpha_expr))
    if has_ma:
        bonus += 0.002

    # ìƒˆ ì—°ì‚°ì ì‚¬ìš© ë³´ë„ˆìŠ¤ (íƒìƒ‰ ì¥ë ¤)
    if 'ts_regression_residual' in alpha_expr:
        bonus += 0.003
    if 'ts_corr' in alpha_expr:
        bonus += 0.002

    # ë‹¤ì¤‘ íƒ€ì„í”„ë ˆì„ ë³´ë„ˆìŠ¤
    windows = [int(w) for w in re.findall(r',\s*(\d+)\)', alpha_expr)]
    if len(windows) >= 2 and max(windows) >= min(windows) * 2:
        bonus += 0.002
    if windows and max(windows) >= 60:
        bonus += 0.001

    # ë³µì¡ë„ í˜ë„í‹°
    depth = alpha_expr.count('(')
    if depth < 3:
        bonus -= 0.003
    if depth > 8:
        bonus -= 0.003 * (depth - 8)

    return bonus

def evaluate_alpha_worker(alpha_expr):
    """Fitness = 0.85 Ã— mean_IC + 0.15 Ã— IC_IR Ã— 0.05 - turnover_penalty + bonus.

    ìˆœìˆ˜ IC ìµœì í™” ì§‘ì¤‘ (ë³´ë„ˆìŠ¤ ì¶•ì†Œ, IC ê°€ì¤‘ ê°•í™”).
    """
    global _global_data, _global_train_start_date, _global_train_end_date
    data = _global_data

    try:
        ic_list, turnover = _compute_ic_series(
            alpha_expr, data,
            date_start=_global_train_start_date,
            date_end=_global_train_end_date
        )
        if len(ic_list) < 10:
            return (alpha_expr, -999.0)

        mean_ic = float(np.mean(ic_list))
        std_ic = float(np.std(ic_list))
        ic_ir = mean_ic / std_ic if std_ic > 0.001 else 0.0

        # Fitness: 85% mean IC + 15% IC_IR (IC ìµœì í™” ì§‘ì¤‘)
        fitness = 0.85 * mean_ic + 0.15 * ic_ir * 0.05

        # í„´ì˜¤ë²„ í˜ë„í‹°: 30% ì´ˆê³¼ ì‹œ ë¹„ë¡€ ê°ì 
        turnover_penalty = max(0, turnover - 0.3) * 0.02

        bonus = _multi_factor_bonus(alpha_expr)
        return (alpha_expr, fitness - turnover_penalty + bonus)
    except Exception:
        return (alpha_expr, -999.0)

def evaluate_alpha_oos(alpha_expr, data, date_start=None):
    """Out-of-sample IC ê³„ì‚° (ë³´ë„ˆìŠ¤ ì—†ì´ ìˆœìˆ˜ IC, ë‚ ì§œ ë²”ìœ„ ì§€ì›)"""
    try:
        return _compute_raw_ic(alpha_expr, data, date_start=date_start)
    except Exception:
        return -999.0

# ì—°ì‚°ì êµí™˜ ê·¸ë£¹ (ê°™ì€ ì‹œê·¸ë‹ˆì²˜ë¼ë¦¬ë§Œ êµì²´)
OPERATOR_SWAP_GROUPS = [
    ['ts_mean', 'ts_std', 'ts_median', 'ts_ema', 'ts_linear_reg', 'ts_decayed_linear'],
    ['ts_zscore_scale', 'ts_maxmin_scale', 'ts_rank'],
    ['ts_delta', 'ts_delta_ratio'],
    ['ts_skew', 'ts_kurt', 'ts_ir'],
    ['ts_min', 'ts_max'],
    ['ts_argmin', 'ts_argmax'],
    ['ts_max_diff', 'ts_min_diff'],
    ['normed_rank', 'zscore_scale'],
    ['cwise_mul', 'add', 'minus'],
    ['ts_corr', 'ts_regression_residual'],  # 2-var ì—°ì‚°ì êµí™˜
]

OPERAND_POOL = ['close', 'open_price', 'high', 'low', 'volume', 'returns', 'vwap', 'high_low_range', 'body',
                'upper_shadow', 'lower_shadow', 'atr_ratio', 'amount', 'amihud', 'gap', 'intraday_ret', 'vol_ratio',
                'foreign_net_ratio', 'inst_net_ratio', 'retail_net_ratio', 'foreign_ownership_pct']

def mutate_alpha(alpha_expr):
    """ì•ŒíŒŒ ë³€ì´ â€” 4ê°€ì§€ íƒ€ì…: ìœˆë„ìš°(20%), ì—°ì‚°ì(20%), í”¼ì—°ì‚°ì(30%), êµ¬ì¡°(30%)
    í”¼ì—°ì‚°ìì™€ êµ¬ì¡° ë³€ì´ ë¹„ì¤‘ì„ ë†’ì—¬ ë” ë‹¤ì–‘í•œ ë³€ìˆ˜/êµ¬ì¡° ì¡°í•© íƒìƒ‰."""
    try:
        mutation_type = random.choices(
            ['window', 'operator', 'operand', 'structural'],
            weights=[0.20, 0.20, 0.30, 0.30]
        )[0]

        if mutation_type == 'window':
            return _mutate_window(alpha_expr)
        elif mutation_type == 'operator':
            return _mutate_operator(alpha_expr)
        elif mutation_type == 'operand':
            return _mutate_operand(alpha_expr)
        else:
            return _mutate_structural(alpha_expr)
    except Exception:
        return None

def _mutate_window(alpha_expr):
    """ìœˆë„ìš° íŒŒë¼ë¯¸í„° ë³€ê²½ (ë²”ìœ„ 3~150, ê·¹ë‹¨ì  ë‹¨ê¸°/ì¥ê¸° íƒìƒ‰ í¬í•¨)"""
    matches = list(re.finditer(r'(ts_\w+|shift)\([^,]+,\s*(\d+)\)', alpha_expr))
    if not matches:
        return None
    match = random.choice(matches)
    old_window = int(match.group(2))
    # í˜„ì¬ ìœˆë„ìš° í¬ê¸°ì— ë”°ë¼ ë³€ì´ í­ ì¡°ì ˆ (ë¹„ë¡€ì  ë³€ì´)
    if old_window <= 20:
        deltas = [-7, -5, -3, -2, 2, 3, 5, 7, 10, 15, 20]
    elif old_window <= 60:
        deltas = [-20, -15, -10, -7, -5, 5, 7, 10, 15, 20, 30, 40]
    else:
        deltas = [-40, -30, -20, -10, 10, 20, 30, 40]
    new_window = max(3, min(150, old_window + random.choice(deltas)))
    if new_window == old_window:
        new_window = max(3, min(150, old_window + random.choice([-25, 25])))
    start, end = match.span(2)
    return alpha_expr[:start] + str(new_window) + alpha_expr[end:]

def _mutate_operator(alpha_expr):
    """ì—°ì‚°ì êµì²´ â€” ê°™ì€ ì‹œê·¸ë‹ˆì²˜ ê·¸ë£¹ ë‚´ì—ì„œë§Œ"""
    # í˜„ì¬ í‘œí˜„ì‹ì—ì„œ ì—°ì‚°ì ì¶”ì¶œ
    op_matches = list(re.finditer(r'ops\.(\w+)\(', alpha_expr))
    if not op_matches:
        return None

    # êµí™˜ ê°€ëŠ¥í•œ ì—°ì‚°ìë§Œ í•„í„°
    swappable = []
    for m in op_matches:
        op_name = m.group(1)
        for group in OPERATOR_SWAP_GROUPS:
            if op_name in group:
                swappable.append((m, op_name, group))
                break

    if not swappable:
        return _mutate_window(alpha_expr)  # êµí™˜ ë¶ˆê°€ë©´ ìœˆë„ìš° ë³€ì´

    match, old_op, group = random.choice(swappable)
    candidates = [op for op in group if op != old_op]
    if not candidates:
        return _mutate_window(alpha_expr)

    new_op = random.choice(candidates)
    start, end = match.span(1)
    return alpha_expr[:start] + new_op + alpha_expr[end:]

def _mutate_operand(alpha_expr):
    """í”¼ì—°ì‚°ì êµì²´ â€” close/volume/returns ê°„ êµí™˜"""
    present = [op for op in OPERAND_POOL if op in alpha_expr]
    if not present:
        return _mutate_window(alpha_expr)

    old_operand = random.choice(present)
    candidates = [op for op in OPERAND_POOL if op != old_operand]
    new_operand = random.choice(candidates)

    # ì²« ë²ˆì§¸ ë“±ì¥ë§Œ êµì²´ (ì „ì²´ êµì²´ ë°©ì§€)
    return alpha_expr.replace(old_operand, new_operand, 1)


def _mutate_structural(alpha_expr):
    """êµ¬ì¡°ì  ë³€ì´ â€” 6ê°€ì§€ íƒ€ì…ìœ¼ë¡œ íƒìƒ‰ ê³µê°„ ëŒ€í­ í™•ì¥.

    ts_corr, ts_regression_residual, sign, relu, greater, rank_divergence ë“±
    ìƒˆë¡œìš´ ì—°ì‚°ìë¥¼ ì£¼ì…í•˜ì—¬ ì™„ì „íˆ ë‹¤ë¥¸ ì•ŒíŒŒ êµ¬ì¡° ìƒì„±.
    """
    structural_type = random.choices(
        ['ts_corr_new', 'ts_corr_add', 'sign_filter', 'relu_clip',
         'regression_residual', 'rank_divergence'],
        weights=[0.15, 0.15, 0.15, 0.15, 0.25, 0.15]
    )[0]

    def _unwrap(expr):
        """normed_rank/zscore_scale ë˜í¼ ì œê±°"""
        for wrapper in ['ops.normed_rank(', 'ops.zscore_scale(']:
            if expr.startswith(wrapper) and expr.endswith(')'):
                return expr[len(wrapper):-1]
        return expr

    try:
        if structural_type == 'ts_corr_new':
            var1 = random.choice(OPERAND_POOL)
            var2 = random.choice([v for v in OPERAND_POOL if v != var1])
            window = random.choice([10, 15, 20, 30, 60])
            return f"ops.normed_rank(ops.ts_corr({var1}, {var2}, {window}))"

        elif structural_type == 'ts_corr_add':
            inner = _unwrap(alpha_expr)
            var1 = random.choice(OPERAND_POOL)
            var2 = random.choice([v for v in OPERAND_POOL if v != var1])
            window = random.choice([10, 15, 20, 30, 60])
            return f"ops.normed_rank(ops.add({inner}, ops.ts_corr({var1}, {var2}, {window})))"

        elif structural_type == 'sign_filter':
            inner = _unwrap(alpha_expr)
            var = random.choice(OPERAND_POOL)
            window = random.choice([5, 10, 20, 40])
            return f"ops.normed_rank(ops.cwise_mul(ops.sign(ops.ts_delta({var}, {window})), {inner}))"

        elif structural_type == 'relu_clip':
            inner = _unwrap(alpha_expr)
            return f"ops.normed_rank(ops.relu({inner}))"

        elif structural_type == 'regression_residual':
            # ts_regression_residual: í•œ ë³€ìˆ˜ì—ì„œ ë‹¤ë¥¸ ë³€ìˆ˜ì˜ ì˜í–¥ ì œê±°
            y_var = random.choice(OPERAND_POOL)
            x_var = random.choice([v for v in OPERAND_POOL if v != y_var])
            window = random.choice([15, 20, 30, 60])
            # 50% í™•ë¥ ë¡œ ì”ì°¨ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© vs ì”ì°¨ì˜ ì¶”ì„¸ ì‚¬ìš©
            if random.random() < 0.5:
                return f"ops.normed_rank(ops.ts_regression_residual({y_var}, {x_var}, {window}))"
            else:
                trend_window = random.choice([5, 10, 15, 20])
                return f"ops.normed_rank(ops.ts_linear_reg(ops.ts_regression_residual({y_var}, {x_var}, {window}), {trend_window}))"

        elif structural_type == 'rank_divergence':
            # ë‘ ë³€ìˆ˜ì˜ ì‹œê³„ì—´ ìˆœìœ„ ì°¨ì´ â†’ ê´´ë¦¬ í¬ì°©
            var1 = random.choice(OPERAND_POOL)
            var2 = random.choice([v for v in OPERAND_POOL if v != var1])
            window = random.choice([10, 20, 30, 60])
            return f"ops.normed_rank(ops.minus(ops.ts_rank({var1}, {window}), ops.ts_rank({var2}, {window})))"

    except Exception:
        pass
    return None


def _subtree_crossover(alpha1, alpha2):
    """ì„œë¸ŒíŠ¸ë¦¬ êµì°¨ â€” í•œ ì•ŒíŒŒì˜ ì„œë¸ŒíŠ¸ë¦¬ë¥¼ ë‹¤ë¥¸ ì•ŒíŒŒì˜ ì„œë¸ŒíŠ¸ë¦¬ë¡œ êµì²´"""
    try:
        # ops.xxx(...) íŒ¨í„´ì˜ ì„œë¸ŒíŠ¸ë¦¬ ì¶”ì¶œ
        def find_subtrees(expr):
            """ê´„í˜¸ ë§¤ì¹­ìœ¼ë¡œ ops.xxx(...) ì„œë¸ŒíŠ¸ë¦¬ ìœ„ì¹˜ ì°¾ê¸°"""
            subtrees = []
            for m in re.finditer(r'ops\.\w+\(', expr):
                start = m.start()
                depth = 0
                for i in range(m.end() - 1, len(expr)):
                    if expr[i] == '(':
                        depth += 1
                    elif expr[i] == ')':
                        depth -= 1
                    if depth == 0:
                        subtrees.append((start, i + 1, expr[start:i+1]))
                        break
            return subtrees

        trees1 = find_subtrees(alpha1)
        trees2 = find_subtrees(alpha2)

        if len(trees1) < 2 or not trees2:
            return None

        # alpha1ì—ì„œ êµì²´í•  ì„œë¸ŒíŠ¸ë¦¬ ì„ íƒ (ìµœìƒìœ„ ì œì™¸)
        replaceable = [t for t in trees1 if t[2] != alpha1]
        if not replaceable:
            return None

        target = random.choice(replaceable)
        donor = random.choice(trees2)

        result = alpha1[:target[0]] + donor[2] + alpha1[target[1]:]
        # ìœ íš¨ì„± ê²€ì‚¬: ops.ê°€ ìˆê³  ê´„í˜¸ê°€ ë§ëŠ”ì§€
        if result.count('(') != result.count(')') or 'ops.' not in result:
            return None
        return result
    except Exception:
        return None


def crossover_alphas(alpha1, alpha2):
    """ì•ŒíŒŒ êµì°¨ â€” ìœˆë„ìš° êµí™˜(60%) + ì„œë¸ŒíŠ¸ë¦¬ êµì°¨(40%)"""
    try:
        # 40% í™•ë¥ ë¡œ ì„œë¸ŒíŠ¸ë¦¬ êµì°¨ ì‹œë„
        if random.random() < 0.4:
            result = _subtree_crossover(alpha1, alpha2)
            if result:
                return result

        matches1 = list(re.finditer(r'(ts_\w+|shift)\(([^,]+),\s*(\d+)\)', alpha1))
        matches2 = list(re.finditer(r'(ts_\w+|shift)\(([^,]+),\s*(\d+)\)', alpha2))

        if not matches1 or not matches2:
            return None

        # ê°™ì€ ì—°ì‚°ìê°€ ìˆìœ¼ë©´ ìš°ì„  êµì°¨
        ops1 = {m.group(1): m for m in matches1}
        ops2 = {m.group(1): m for m in matches2}
        common_ops = set(ops1.keys()) & set(ops2.keys())

        if common_ops:
            op = random.choice(list(common_ops))
            m1, m2 = ops1[op], ops2[op]
        else:
            m1 = random.choice(matches1)
            m2 = random.choice(matches2)

        # alpha1ì˜ ìœˆë„ìš°ë¥¼ alpha2ì˜ ê°’ìœ¼ë¡œ êµì²´
        win2 = m2.group(3)
        start, end = m1.span(3)
        return alpha1[:start] + win2 + alpha1[end:]
    except Exception:
        return None

def _get_alpha_structure(alpha_expr):
    """ì•ŒíŒŒì˜ êµ¬ì¡° ì‹œê·¸ë‹ˆì²˜ (ìœˆë„ìš° ì œê±°) â€” ë‹¤ì–‘ì„± ë¹„êµìš©"""
    return re.sub(r',\s*\d+\)', ', N)', alpha_expr)

def _get_variable_signature(alpha_expr):
    """ì•ŒíŒŒì— ì‚¬ìš©ëœ ë³€ìˆ˜ ì¡°í•© (ìˆœì„œ ë¬´ê´€) â€” ë³€ìˆ˜ ìˆ˜ì¤€ ë‹¤ì–‘ì„± ë¹„êµìš©.

    ê°™ì€ ë³€ìˆ˜ ì¡°í•© {foreign_ownership_pct, vol_ratio, amihud, close}ì˜
    window/operator ë³€í˜•ì€ ë™ì¼ ì‹œê·¸ë‹ˆì²˜ë¡œ ì·¨ê¸‰.
    """
    vars_used = set()
    for var in OPERAND_POOL:
        if var in alpha_expr:
            vars_used.add(var)
    return frozenset(vars_used)

def _select_diverse_top_n(results, n=5):
    """IC ìƒìœ„ì—ì„œ êµ¬ì¡°ê°€ ë‹¤ë¥¸ Top-N ì„ íƒ"""
    sorted_results = sorted(results, key=lambda x: x[1], reverse=True)
    selected = []
    seen_structures = set()

    for alpha, ic in sorted_results:
        if ic <= -999.0:
            continue
        structure = _get_alpha_structure(alpha)
        if structure not in seen_structures:
            selected.append((alpha, ic))
            seen_structures.add(structure)
            if len(selected) >= n:
                break

    return selected

def _tournament_select(fitness_scores, tournament_size=5):
    """í† ë„ˆë¨¼íŠ¸ ì„ íƒ â€” ë‹¤ì–‘ì„± ìœ ì§€í•˜ë©´ì„œ ìš°ìˆ˜ ê°œì²´ ì„ í˜¸"""
    candidates = random.sample(fitness_scores, min(tournament_size, len(fitness_scores)))
    return max(candidates, key=lambda x: x[1])[0]


def _fitness_sharing(fitness_scores, sharing_radius=1.0):
    """ì í•©ë„ ê³µìœ  â€” êµ¬ì¡° + ë³€ìˆ˜ ì¡°í•© 2ë‹¨ê³„ ë‹ˆì‰¬ í˜ë„í‹°ë¡œ ë‹¤ì–‘ì„± ë³´ì¡´.

    Level 1: ê°™ì€ êµ¬ì¡°(operator+variable, window ì œì™¸) â†’ ê°•í•œ í˜ë„í‹°
    Level 2: ê°™ì€ ë³€ìˆ˜ ì¡°í•©(operator ë¬´ê´€) â†’ ì¶”ê°€ í˜ë„í‹°
    â†’ foreign_ownership/vol_ratio/amihud/close ì¡°í•©ì´ ë…ì í•˜ëŠ” ê²ƒì„ ë°©ì§€
    """
    # Level 1: êµ¬ì¡° ë‹ˆì‰¬ (ê¸°ì¡´)
    structures = {}
    for alpha, ic in fitness_scores:
        struct = _get_alpha_structure(alpha)
        if struct not in structures:
            structures[struct] = []
        structures[struct].append((alpha, ic))

    # Level 2: ë³€ìˆ˜ ì¡°í•© ë‹ˆì‰¬
    var_sigs = {}
    for alpha, ic in fitness_scores:
        sig = _get_variable_signature(alpha)
        if sig not in var_sigs:
            var_sigs[sig] = 0
        var_sigs[sig] += 1

    shared = []
    for struct, members in structures.items():
        struct_niche = len(members)
        for alpha, ic in members:
            # êµ¬ì¡° ë‹ˆì‰¬ í˜ë„í‹°
            penalty = 1.0 + sharing_radius * (struct_niche - 1)
            # ë³€ìˆ˜ ì¡°í•© ë‹ˆì‰¬ í˜ë„í‹° (ê°™ì€ ë³€ìˆ˜ ì¡°í•©ì´ 8ê°œ ì´ìƒì´ë©´ ì¶”ê°€ ê°ì )
            var_sig = _get_variable_signature(alpha)
            var_count = var_sigs.get(var_sig, 1)
            if var_count > 8:
                penalty += 0.3 * (var_count - 8) / 8  # ì ì§„ì  ì¶”ê°€ í˜ë„í‹° (ì¡°ê¸° ë°œë™)
            shared_ic = ic / penalty
            shared.append((alpha, shared_ic))

    return sorted(shared, key=lambda x: x[1], reverse=True)


def genetic_programming(seed_alphas, data, train_start_date=None, train_end_date=None,
                        generations=70, population_size=300):
    """ë³‘ë ¬ GP v11 â€” CogAlpha-inspired: LLM-guided mutation + adaptive feedback + ë‹¤ì–‘ì„± ê·¹ëŒ€í™”"""

    close_idx = data['close'].index
    print(f"\nğŸ§¬ ë³‘ë ¬ GP ì‹œì‘ (v11 â€” CogAlpha: LLM-guided + diversity expansion)")
    if train_end_date is not None:
        print(f"   Train IC range: {train_start_date or close_idx[0]} ~ {train_end_date}")
    else:
        print(f"   Train IC range: full data")
    print(f"   Seed: {len(seed_alphas)}ê°œ, ì„¸ëŒ€: {generations}, ê°œì²´ìˆ˜: {population_size}, ì›Œì»¤: 8")
    print(f"   LLM mutation: ë§¤ 3ì„¸ëŒ€, LLM crossover: ë§¤ 5ì„¸ëŒ€")

    population = seed_alphas[:population_size]
    while len(population) < population_size:
        parent = random.choice(seed_alphas)
        mutated = mutate_alpha(parent)
        if mutated:
            population.append(mutated)

    set_global_data(data, train_start_date=train_start_date, train_end_date=train_end_date)
    best_ever = (None, -999.0)
    stagnation_count = 0
    immigration_count = 0
    all_results_history = []

    # CogAlpha: ëˆ„ì  adaptive feedback
    adaptive_feedback = ""
    llm_injection_count = 0

    elite_count = max(5, population_size // 20)  # 5% ì—˜ë¦¬íŠ¸ (ë‹¤ì–‘ì„± ìš°ì„ )
    base_mutation_rate = 0.50  # ê¸°ë³¸ ë³€ì´ìœ¨ (íƒìƒ‰ ë¹„ì¤‘ ê°•í™”)

    # LLM mutation/crossover ì£¼ê¸° (ë” ë¹ˆë²ˆí•œ LLM ê°œì…ìœ¼ë¡œ ë‹¤ì–‘ì„± ê·¹ëŒ€í™”)
    LLM_MUTATION_INTERVAL = 3
    LLM_CROSSOVER_INTERVAL = 5

    for gen in range(1, generations + 1):
        # ì ì‘ì  ë³€ì´ìœ¨: ì •ì²´ ì‹œ ë³€ì´ ë¹„ì¤‘ ì¦ê°€
        mutation_rate = min(0.80, base_mutation_rate + stagnation_count * 0.05)
        crossover_rate = 1.0 - mutation_rate

        print(f"\n  ì„¸ëŒ€ {gen}/{generations} (ë³€ì´ìœ¨: {mutation_rate:.0%}, ì •ì²´: {stagnation_count})")

        with Pool(8, initializer=set_global_data, initargs=(data, train_start_date, train_end_date)) as pool:
            results = pool.map(evaluate_alpha_worker, population)

        # ì í•©ë„ ê³µìœ  ì ìš© (ê°™ì€ êµ¬ì¡°ë¼ë¦¬ fitness ë¶„ì‚°)
        raw_scores = sorted(results, key=lambda x: x[1], reverse=True)
        all_results_history.extend([(a, ic) for a, ic in raw_scores if ic > -999.0])

        # ìˆœìˆ˜ train fitness + fitness sharing (validation ì—†ìŒ)
        fitness_scores = _fitness_sharing(raw_scores)

        best_ic = raw_scores[0][1]  # ê³µìœ  ì „ ì‹¤ì œ IC
        median_ic = raw_scores[len(raw_scores)//2][1] if raw_scores else -999.0
        unique_structures = len(set(_get_alpha_structure(a) for a, _ in raw_scores if _ > -999.0))
        print(f"    ìµœê³  IC: {best_ic:.4f}  ì¤‘ì•™ê°’: {median_ic:.4f}  ê³ ìœ êµ¬ì¡°: {unique_structures}ê°œ")

        # â”€â”€ CogAlpha: Per-Generation Adaptive Feedback â”€â”€
        adaptive_feedback = _build_adaptive_feedback(raw_scores, prev_feedback=adaptive_feedback)

        if best_ic > best_ever[1]:
            best_ever = raw_scores[0]
            stagnation_count = 0
            print(f"    ğŸ† ì‹ ê¸°ë¡!")
        else:
            stagnation_count += 1

        # ì´ë¯¼(immigration): ì •ì²´ ì‹œ LLM-guided ê°œì²´ ì£¼ì… (CogAlpha ìŠ¤íƒ€ì¼)
        if stagnation_count >= 4 and immigration_count < 5:
            immigration_count += 1
            stagnation_count = 0
            n_immigrants = int(population_size * 0.30)  # 30% êµì²´
            print(f"    ğŸŒ ì´ë¯¼ #{immigration_count}: {n_immigrants}ê°œ ìƒˆ ê°œì²´ ì£¼ì… (LLM-guided)")

            # CogAlpha ê°œì„ : ì´ë¯¼ ì‹œ LLM mutation + ëœë¤ mutation í˜¼í•©
            top_for_llm = [(a, ic) for a, ic in raw_scores[:10] if ic > -999.0]
            llm_immigrants = _llm_guided_mutation(top_for_llm, adaptive_feedback, num_mutations=min(15, n_immigrants // 3))
            if llm_immigrants:
                print(f"      ğŸ¤– LLM mutation: {len(llm_immigrants)}ê°œ ìƒì„±")
                llm_injection_count += len(llm_immigrants)

            # ë‚˜ë¨¸ì§€ëŠ” ëœë¤ ë³€ì´ë¡œ ì±„ì›€
            random_immigrants = []
            needed = n_immigrants - len(llm_immigrants)
            for _ in range(needed):
                parent = random.choice(seed_alphas)
                for _ in range(random.randint(2, 3)):
                    m = mutate_alpha(parent)
                    if m:
                        parent = m
                random_immigrants.append(parent)

            immigrants = llm_immigrants + random_immigrants
            # í•˜ìœ„ 25% êµì²´
            population = [a for a, _ in fitness_scores[:population_size - n_immigrants]] + immigrants[:n_immigrants]
            continue

        # ìµœì¢… ì¢…ë£Œ: ì´ë¯¼ 3íšŒ í›„ì—ë„ 5ì„¸ëŒ€ ë¬´ê°œì„ 
        if stagnation_count >= 4:
            print(f"    â¹ï¸  ì´ë¯¼ {immigration_count}íšŒ í›„ 4ì„¸ëŒ€ ë¬´ê°œì„  â†’ ì¢…ë£Œ")
            break

        # â”€â”€ CogAlpha: ì£¼ê¸°ì  LLM-guided Mutation ì£¼ì… â”€â”€
        llm_offspring = []
        if gen % LLM_MUTATION_INTERVAL == 0:
            top_for_llm = [(a, ic) for a, ic in raw_scores[:10] if ic > -999.0]
            print(f"    ğŸ¤– LLM-guided mutation (ì„¸ëŒ€ {gen})...")
            llm_mutated = _llm_guided_mutation(top_for_llm, adaptive_feedback, num_mutations=15)
            if llm_mutated:
                llm_offspring.extend(llm_mutated)
                llm_injection_count += len(llm_mutated)
                print(f"      âœ… LLM mutation: {len(llm_mutated)}ê°œ ìƒì„±")

        # â”€â”€ CogAlpha: ì£¼ê¸°ì  LLM-guided Crossover ì£¼ì… â”€â”€
        if gen % LLM_CROSSOVER_INTERVAL == 0:
            top_for_llm = [(a, ic) for a, ic in raw_scores[:10] if ic > -999.0]
            print(f"    ğŸ¤– LLM-guided crossover (ì„¸ëŒ€ {gen})...")
            llm_crossed = _llm_guided_crossover(top_for_llm, adaptive_feedback, num_children=8)
            if llm_crossed:
                llm_offspring.extend(llm_crossed)
                llm_injection_count += len(llm_crossed)
                print(f"      âœ… LLM crossover: {len(llm_crossed)}ê°œ ìƒì„±")

        # ë‹¤ìŒ ì„¸ëŒ€ êµ¬ì„±
        next_population = []

        # ì—˜ë¦¬íŠ¸ ë³´ì¡´ (7%)
        for alpha, _ in fitness_scores[:elite_count]:
            next_population.append(alpha)

        # LLM offspring ì£¼ì… (í•˜ìœ„ ê°œì²´ êµì²´)
        if llm_offspring:
            next_population.extend(llm_offspring)

        # í† ë„ˆë¨¼íŠ¸ ì„ íƒ + êµì°¨/ë³€ì´ (ë‚˜ë¨¸ì§€)
        while len(next_population) < population_size:
            if random.random() < crossover_rate:
                # í† ë„ˆë¨¼íŠ¸ ì„ íƒìœ¼ë¡œ ë¶€ëª¨ 2ê°œ ì„ íƒ â†’ êµì°¨
                parent1 = _tournament_select(fitness_scores, tournament_size=5)
                parent2 = _tournament_select(fitness_scores, tournament_size=5)
                child = crossover_alphas(parent1, parent2)
                if child:
                    next_population.append(child)
                else:
                    next_population.append(parent1)
            else:
                # í† ë„ˆë¨¼íŠ¸ ì„ íƒ â†’ ë³€ì´
                parent = _tournament_select(fitness_scores, tournament_size=5)
                mutated = mutate_alpha(parent)
                if mutated:
                    next_population.append(mutated)
                else:
                    next_population.append(parent)

        population = next_population[:population_size]

        del results, raw_scores, fitness_scores, next_population
        gc.collect()

    print(f"\n    ğŸ“Š LLM ì£¼ì… ì´ê³„: {llm_injection_count}ê°œ (mutation + crossover)")

    # Top-20 ë‹¤ì–‘í•œ ì•ŒíŒŒ ì„ íƒ (mainì—ì„œ val/test ICë¡œ ìµœì¢… 5ê°œ ì„ íƒ)
    top_diverse = _select_diverse_top_n(all_results_history, n=30)

    # Proven seeds í•­ìƒ ìµœì¢… í›„ë³´ì— í¬í•¨ (GPì—ì„œ íƒˆë½í•´ë„ main()ì—ì„œ ì¬í‰ê°€)
    existing_exprs = {a for a, _ in top_diverse}
    for seed in seed_alphas[:14]:  # ì²« 14ê°œ = proven seeds
        if seed not in existing_exprs:
            top_diverse.append((seed, 0.0))

    return best_ever, top_diverse


def _make_cv_folds(close_index, n_folds=4, test_days=60, purge_days=20):
    """Purged Walk-Forward CV í´ë“œ ìƒì„±.

    Expanding window train + ê³ ì • test + purge gap (forward return ëˆ„ì¶œ ë°©ì§€).
    ë’¤ì—ì„œë¶€í„° ì—­ìˆœìœ¼ë¡œ foldë¥¼ ë°°ì¹˜í•˜ì—¬ ìµœì‹  ë°ì´í„°ë¥¼ í•­ìƒ í…ŒìŠ¤íŠ¸.
    """
    n_total = len(close_index)
    min_train_days = 120  # ìµœì†Œ train ê¸°ê°„

    folds = []
    current_end = n_total - 1

    for _ in range(n_folds):
        test_end_idx = current_end
        test_start_idx = max(0, current_end - test_days + 1)
        train_end_idx = test_start_idx - purge_days - 1
        train_start_idx = 0  # expanding window

        if train_end_idx - train_start_idx + 1 < min_train_days:
            break  # train ë°ì´í„° ë¶€ì¡±

        folds.append((
            close_index[train_start_idx],
            close_index[train_end_idx],
            close_index[test_start_idx],
            close_index[test_end_idx],
        ))
        current_end = test_start_idx - 1  # ë‹¤ìŒ foldëŠ” ì´ì „ ê¸°ê°„

    folds.reverse()  # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
    return folds


def main():
    print("=" * 80)
    print("Alpha-GPT: 20-day (1-month) Forward with GPT-4o (v10 â€” CogAlpha: LLM-guided Evolution)")
    print("=" * 80)
    print()

    # 1. ì „ì²´ ë°ì´í„° ë¡œë“œ
    full_data = load_market_data()

    # 2. Purged Walk-Forward CV í´ë“œ ìƒì„±
    close = full_data['close']
    cv_folds = _make_cv_folds(close.index, n_folds=4, test_days=60, purge_days=20)

    print(f"\nğŸ“ Purged Walk-Forward CV ({len(cv_folds)} folds, purge=20d):")
    for i, (tr_s, tr_e, te_s, te_e) in enumerate(cv_folds, 1):
        tr_len = len(close.loc[tr_s:tr_e])
        te_len = len(close.loc[te_s:te_e])
        print(f"   Fold {i}: Train [{tr_s}~{tr_e}] ({tr_len}d) | Test [{te_s}~{te_e}] ({te_len}d)")

    # 3. GPT-4o ì‹œë“œ ìƒì„±
    seed_alphas = generate_seed_alphas_gpt4o()

    # 4. GP ì§„í™” â€” ê°€ì¥ í° foldì˜ train ê¸°ê°„ìœ¼ë¡œ 1íšŒ ì‹¤í–‰
    largest_fold = cv_folds[-1]
    gp_train_end = largest_fold[1]

    (best_alpha, best_ic), top_diverse = genetic_programming(
        seed_alphas,
        full_data,
        train_start_date=None,
        train_end_date=gp_train_end,
        generations=70,
        population_size=300
    )

    # 5. Top í›„ë³´ë¥¼ ëª¨ë“  CV foldì—ì„œ í‰ê°€
    print("\n" + "=" * 80)
    print(f"ğŸ“Š Cross-Validation: Top-{len(top_diverse)} candidates x {len(cv_folds)} folds")
    print("=" * 80)

    all_candidates = []
    for i, (alpha, gp_fitness) in enumerate(top_diverse, 1):
        fold_train_ics = []
        fold_test_ics = []
        fold_test_irs = []

        for fi, (tr_s, tr_e, te_s, te_e) in enumerate(cv_folds):
            train_ic = _compute_raw_ic(alpha, full_data, date_start=tr_s, date_end=tr_e)
            if train_ic <= -999.0:
                train_ic = 0.0
            fold_train_ics.append(train_ic)

            test_ic_list, _ = _compute_ic_series(alpha, full_data, date_start=te_s, date_end=te_e)
            if len(test_ic_list) >= 5:
                test_ic = float(np.mean(test_ic_list))
                test_std = float(np.std(test_ic_list))
                test_ir = test_ic / max(test_std, 0.001)
            else:
                test_ic = -0.05
                test_ir = -1.0
            fold_test_ics.append(test_ic)
            fold_test_irs.append(test_ir)

        # CV ì¼ê´€ì„± ì§€í‘œ
        n_positive_folds = sum(1 for ic in fold_test_ics if ic > 0)
        mean_test_ic = float(np.mean(fold_test_ics))
        mean_test_ir = float(np.mean(fold_test_irs))
        mean_train_ic = float(np.mean(fold_train_ics))

        # íŒ©í„° ë¶„ë¥˜
        factors = []
        if any(kw in alpha for kw in ['close', 'open_price', 'high', 'low', 'vwap']):
            factors.append('price')
        if any(kw in alpha for kw in ['volume', 'amount', 'vol_ratio']):
            factors.append('volume')
        if 'returns' in alpha:
            factors.append('returns')
        if any(kw in alpha for kw in ['high_low_range', 'body', 'upper_shadow', 'lower_shadow', 'atr_ratio']):
            factors.append('volatility')
        if any(kw in alpha for kw in ['amihud', 'gap', 'intraday_ret']):
            factors.append('micro')
        if any(kw in alpha for kw in ['foreign_net_ratio', 'inst_net_ratio', 'retail_net_ratio', 'foreign_ownership_pct']):
            factors.append('flow')
        factor_str = '+'.join(factors) if factors else 'unknown'

        all_candidates.append({
            'expr': alpha,
            'mean_train_ic': mean_train_ic,
            'mean_test_ic': mean_test_ic,
            'mean_test_ir': mean_test_ir,
            'n_positive_folds': n_positive_folds,
            'fold_test_ics': fold_test_ics,
            'factors': factor_str,
        })

        if i <= 10:
            fold_str = ' '.join([f"F{fi+1}:{ic:+.3f}" for fi, ic in enumerate(fold_test_ics)])
            print(f"  #{i:2d} [{factor_str:20s}] Train={mean_train_ic:.4f} | Test={mean_test_ic:.4f} | "
                  f"Pos={n_positive_folds}/{len(cv_folds)} | {fold_str}")

    # 6. CV ì¼ê´€ì„± ê¸°ë°˜ ìµœì¢… 5ê°œ ì„ ë³„ â€” ë³€ìˆ˜ ì¡°í•© ë‹¤ì–‘ì„± ê°•ì œ
    #    ê°™ì€ ë³€ìˆ˜ ì¡°í•©(ì˜ˆ: foreign_ownership+vol_ratio+amihud+close)ì€ ìµœëŒ€ 2ê°œ
    #    â†’ ë‚˜ë¨¸ì§€ 3ê°œëŠ” ë°˜ë“œì‹œ ë‹¤ë¥¸ ë³€ìˆ˜ ì¡°í•©ì´ì–´ì•¼ í•¨
    MAX_SAME_VARS = 2

    tier1 = [a for a in all_candidates if a['n_positive_folds'] >= 3]
    tier2 = [a for a in all_candidates if a['n_positive_folds'] >= 2 and a not in tier1]
    tier3 = [a for a in all_candidates if a not in tier1 and a not in tier2]

    tier1.sort(key=lambda x: (x['n_positive_folds'], x['mean_test_ic']), reverse=True)
    tier2.sort(key=lambda x: x['mean_test_ic'], reverse=True)
    tier3.sort(key=lambda x: x['mean_train_ic'], reverse=True)

    ranked_candidates = tier1 + tier2 + tier3

    validated_alphas = []
    var_sig_counts = {}
    for a in ranked_candidates:
        sig = _get_variable_signature(a['expr'])
        count = var_sig_counts.get(sig, 0)
        if count < MAX_SAME_VARS:
            validated_alphas.append(a)
            var_sig_counts[sig] = count + 1
            if len(validated_alphas) >= 5:
                break

    # ë‹¤ì–‘ì„± í†µê³„ ì¶œë ¥
    unique_var_sigs = len(set(_get_variable_signature(a['expr']) for a in validated_alphas))
    print(f"\n   ğŸ§¬ ë³€ìˆ˜ ë‹¤ì–‘ì„±: {unique_var_sigs}ê°œ ê³ ìœ  ë³€ìˆ˜ ì¡°í•© / {len(validated_alphas)}ê°œ ì•ŒíŒŒ "
          f"(ê°™ì€ ì¡°í•© ìµœëŒ€ {MAX_SAME_VARS}ê°œ)")
    for sig, cnt in sorted(var_sig_counts.items(), key=lambda x: -x[1]):
        if cnt > 0:
            var_list = sorted(sig)
            print(f"      {cnt}ê°œ: {{{', '.join(var_list)}}}")

    print("\n" + "=" * 80)
    print("ğŸ† TOP 5 ALPHAS (CV-validated)")
    print("=" * 80)

    for i, a in enumerate(validated_alphas, 1):
        n_pos = a['n_positive_folds']
        status = "âœ…" if n_pos >= 3 else ("ğŸ”¶" if n_pos >= 2 else "âš ï¸")
        fold_detail = ' '.join([f"{ic:+.3f}" for ic in a['fold_test_ics']])
        print(f"\n  #{i} {status} (positive folds: {n_pos}/{len(cv_folds)})")
        print(f"     Train IC: {a['mean_train_ic']:.4f} | Test IC: {a['mean_test_ic']:.4f} | "
              f"Test IR: {a['mean_test_ir']:.2f} [{a['factors']}]")
        print(f"     Fold ICs: [{fold_detail}]")
        print(f"     {a['expr'][:100]}{'...' if len(a['expr']) > 100 else ''}")

    # 7. ìµœì¢… Best ì„ ì •
    if tier1:
        final_best = tier1[0]
    elif tier2:
        final_best = tier2[0]
    else:
        final_best = validated_alphas[0] if validated_alphas else {
            'expr': best_alpha, 'mean_train_ic': best_ic, 'mean_test_ic': -999,
            'mean_test_ir': -999, 'n_positive_folds': 0, 'fold_test_ics': [], 'factors': '?'
        }

    print("\n" + "=" * 80)
    print("ğŸ¥‡ FINAL BEST (CV-validated)")
    print("=" * 80)
    print(f"Mean Train IC:  {final_best['mean_train_ic']:.4f}")
    print(f"Mean Test IC:   {final_best['mean_test_ic']:.4f}")
    print(f"Mean Test IR:   {final_best['mean_test_ir']:.2f}")
    print(f"Positive Folds: {final_best['n_positive_folds']}/{len(cv_folds)}")
    print(f"Factors:        {final_best['factors']}")
    print(f"Expression:     {final_best['expr']}")

    # 8. DB ì €ì¥ (Top-5 ì „ë¶€)
    print("\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
    try:
        conn = get_db_connection()
        cursor = conn.cursor()

        for a in validated_alphas:
            cursor.execute("""
                INSERT INTO alpha_formulas (formula, ic_score, description, created_at)
                VALUES (%s, %s, %s, NOW())
                ON CONFLICT (formula) DO UPDATE
                SET ic_score = EXCLUDED.ic_score, updated_at = NOW()
            """, (
                a['expr'],
                float(a['mean_test_ic']),
                f"20d fwd, train={a['mean_train_ic']:.4f}, test={a['mean_test_ic']:.4f}, "
                f"IR={a['mean_test_ir']:.2f}, pos_folds={a['n_positive_folds']}/{len(cv_folds)}, "
                f"{a['factors']}, v10-cogalpha-cv"
            ))

        conn.commit()
        cursor.close()
        conn.close()
        print(f"âœ… {len(validated_alphas)}ê°œ ì•ŒíŒŒ ì €ì¥ ì™„ë£Œ!")
    except Exception as e:
        print(f"âš ï¸  DB ì €ì¥ ì‹¤íŒ¨: {e}")

    # 9. Multi-Alpha Ensembleìš© JSON ë‚´ë³´ë‚´ê¸°
    alpha_export = []
    for a in validated_alphas:
        alpha_export.append({
            'expression': a['expr'],
            'mean_test_ic': float(a['mean_test_ic']),
            'mean_test_ir': float(a['mean_test_ir']),
            'n_positive_folds': a['n_positive_folds'],
            'factors': a['factors'],
        })

    export_path = project_root / 'best_alphas.json'
    with open(export_path, 'w') as f:
        json.dump(alpha_export, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ“ Multi-Alpha Ensemble ë‚´ë³´ë‚´ê¸°: {export_path}")
    for i, ae in enumerate(alpha_export, 1):
        print(f"   #{i} IC={ae['mean_test_ic']:.4f} IR={ae['mean_test_ir']:.2f} "
              f"[{ae['factors']}] {ae['expression'][:80]}...")

    print("\nğŸ‰ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
