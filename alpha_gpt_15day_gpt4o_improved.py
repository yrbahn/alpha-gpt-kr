#!/usr/bin/env python3
"""
Alpha-GPT: 15-day Forward with GPT-4o (v2 â€” Improved Prompt)
ê°œì„ ëœ QuantDeveloper í”„ë¡¬í”„íŠ¸ + ops.xxx() ë¬¸ë²• + ë³‘ë ¬ GP
"""

import sys
import os
import re
import json
from pathlib import Path
import pandas as pd
import numpy as np
from dotenv import load_dotenv
import psycopg2
import openai
import random
import gc
from multiprocessing import Pool

project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from alpha_gpt_kr.mining.operators import AlphaOperators as ops
from alpha_gpt_kr.agents.quant_developer import QuantDeveloper

load_dotenv()

def get_db_connection():
    return psycopg2.connect(
        host=os.getenv('DB_HOST', '192.168.0.248'),
        port=int(os.getenv('DB_PORT', 5432)),
        database=os.getenv('DB_NAME', 'marketsense'),
        user=os.getenv('DB_USER', 'yrbahn'),
        password=os.getenv('DB_PASSWORD', '1234')
    )

def load_market_data():
    """500ê°œ ì¢…ëª© ë°ì´í„° ë¡œë“œ (ì‹œê°€ì´ì•¡ ìƒìœ„)"""
    print("ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘... (ì‹œì´ ìƒìœ„ 500ì¢…ëª©, 2ë…„)")
    
    conn = get_db_connection()
    
    # ì‹œê°€ì´ì•¡ ìƒìœ„ 500ê°œ
    query_stocks = """
        SELECT 
            s.id,
            s.ticker,
            s.name,
            s.market_cap
        FROM stocks s
        WHERE s.is_active = true
        AND s.market_cap IS NOT NULL
        AND EXISTS (
            SELECT 1 FROM price_data p 
            WHERE p.stock_id = s.id 
            AND p.date >= CURRENT_DATE - INTERVAL '730 days'
            LIMIT 1
        )
        ORDER BY s.market_cap DESC
        LIMIT 500
    """
    
    stocks_df = pd.read_sql(query_stocks, conn)
    stock_ids = stocks_df['id'].tolist()
    stock_id_list = ', '.join(map(str, stock_ids))
    
    query_prices = f"""
        SELECT
            s.ticker,
            p.date,
            p.open,
            p.high,
            p.low,
            p.close,
            p.volume
        FROM price_data p
        JOIN stocks s ON p.stock_id = s.id
        WHERE p.stock_id IN ({stock_id_list})
        AND p.date >= CURRENT_DATE - INTERVAL '730 days'
        ORDER BY s.ticker, p.date
    """

    price_df = pd.read_sql(query_prices, conn)

    open_price = price_df.pivot(index='date', columns='ticker', values='open')
    high = price_df.pivot(index='date', columns='ticker', values='high')
    low = price_df.pivot(index='date', columns='ticker', values='low')
    close = price_df.pivot(index='date', columns='ticker', values='close')
    volume = price_df.pivot(index='date', columns='ticker', values='volume')
    returns = close.pct_change()

    # íŒŒìƒ ë³€ìˆ˜ (OHLC)
    vwap = (high + low + close) / 3
    high_low_range = (high - low) / close
    body = (close - open_price) / open_price
    upper_shadow = (high - close.clip(lower=open_price)) / close
    lower_shadow = (close.clip(upper=open_price) - low) / close

    # â”€â”€ ì¬ë¬´ 4ë¶„ê¸° ì¶”ì„¸ ë³€ìˆ˜ â”€â”€
    print("   ì¬ë¬´ ì¶”ì„¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
    import json as _json
    id_ticker = dict(zip(stocks_df['id'], stocks_df['ticker']))

    fin_df = pd.read_sql(f"""
        SELECT stock_id, period_end, revenue, operating_income, net_income,
               total_equity, total_assets, raw_data
        FROM financial_statements
        WHERE stock_id IN ({stock_id_list})
        ORDER BY stock_id, period_end
    """, conn)
    conn.close()

    # raw_dataì—ì„œ ROE, ì˜ì—…ì´ìµë¥  ì¶”ì¶œ
    def _parse_raw(row):
        rd = row.get('raw_data')
        if rd is None:
            return {'quarter_type': None, 'roe': None}
        if isinstance(rd, str):
            rd = _json.loads(rd)
        return {
            'quarter_type': rd.get('quarter', ''),
            'roe': rd.get('roe'),
        }

    raw_parsed = fin_df.apply(_parse_raw, axis=1, result_type='expand')
    fin_df = pd.concat([fin_df, raw_parsed], axis=1)
    fin_df['ticker'] = fin_df['stock_id'].map(id_ticker)
    fin_df = fin_df.dropna(subset=['ticker'])

    # ì—°ê°„(12-31) ì œì™¸ â†’ standalone quarterlyë§Œ ì‚¬ìš©
    fin_df = fin_df[fin_df['quarter_type'] != 'ì—°ê°„'].copy()

    # ì˜ì—…ì´ìµë¥  ê³„ì‚°
    fin_df['op_margin'] = np.where(
        (fin_df['revenue'].notna()) & (fin_df['revenue'] != 0),
        fin_df['operating_income'] / fin_df['revenue'],
        np.nan
    )

    # ë¶„ê¸° ì¸ë±ìŠ¤ (ì •ë ¬ìš©)
    fin_df = fin_df.sort_values(['ticker', 'period_end'])

    # ê° ì¢…ëª©ë³„ QoQ/YoY ì¶”ì„¸ ê³„ì‚°
    trend_records = []
    for ticker, grp in fin_df.groupby('ticker'):
        grp = grp.sort_values('period_end').reset_index(drop=True)
        for i in range(len(grp)):
            row = grp.iloc[i]
            rec = {'ticker': ticker, 'period_end': row['period_end']}

            # QoQ (ì „ë¶„ê¸° ëŒ€ë¹„)
            if i >= 1:
                prev = grp.iloc[i - 1]
                if prev['operating_income'] and prev['operating_income'] != 0:
                    rec['oi_qoq'] = (row['operating_income'] - prev['operating_income']) / abs(prev['operating_income'])
                if prev['net_income'] and prev['net_income'] != 0:
                    rec['ni_qoq'] = (row['net_income'] - prev['net_income']) / abs(prev['net_income'])
                if row['op_margin'] is not None and prev['op_margin'] is not None:
                    rec['margin_qoq'] = row['op_margin'] - prev['op_margin']
                if row['roe'] is not None and prev['roe'] is not None:
                    rec['roe_qoq'] = row['roe'] - prev['roe']

            # YoY (4ë¶„ê¸° ì „ = ê°™ì€ ë¶„ê¸° ì „ë…„) - Q1/Q2/Q3 ìˆœì„œë¡œ 3ë¶„ê¸°ì”©ì´ë¯€ë¡œ 3ì¹¸ ë’¤
            if i >= 3:
                yoy_prev = grp.iloc[i - 3]
                # ê°™ì€ ë¶„ê¸°ì¸ì§€ í™•ì¸ (3/31â†”3/31, 6/30â†”6/30, 9/30â†”9/30)
                if row['period_end'].month == yoy_prev['period_end'].month:
                    if yoy_prev['operating_income'] and yoy_prev['operating_income'] != 0:
                        rec['oi_yoy'] = (row['operating_income'] - yoy_prev['operating_income']) / abs(yoy_prev['operating_income'])
                    if yoy_prev['net_income'] and yoy_prev['net_income'] != 0:
                        rec['ni_yoy'] = (row['net_income'] - yoy_prev['net_income']) / abs(yoy_prev['net_income'])
                    if row['op_margin'] is not None and yoy_prev['op_margin'] is not None:
                        rec['margin_yoy'] = row['op_margin'] - yoy_prev['op_margin']
                    if row['roe'] is not None and yoy_prev['roe'] is not None:
                        rec['roe_yoy'] = row['roe'] - yoy_prev['roe']

            # 3ë¶„ê¸° ì¶”ì„¸ ê¸°ìš¸ê¸° (ìµœê·¼ 3ë¶„ê¸° OIì˜ ì„ í˜• ì¶”ì„¸)
            if i >= 2:
                oi_vals = [grp.iloc[j]['operating_income'] for j in range(i - 2, i + 1)
                           if grp.iloc[j]['operating_income'] is not None and not np.isnan(grp.iloc[j]['operating_income'])]
                if len(oi_vals) == 3:
                    # ê°„ë‹¨í•œ ì„ í˜• ê¸°ìš¸ê¸°: (last - first) / 2
                    rec['oi_trend'] = (oi_vals[2] - oi_vals[0]) / (abs(oi_vals[0]) + 1e-10)

            trend_records.append(rec)

    trend_df = pd.DataFrame(trend_records)

    # ì¼ë³„ ë°ì´í„°ë¡œ ë³€í™˜: ë¶„ê¸°ë³„ â†’ ì¼ë³„ forward-fill, cross-sectional rank
    trend_vars = {}
    trend_fields = ['oi_qoq', 'ni_qoq', 'oi_yoy', 'ni_yoy', 'margin_yoy', 'roe_yoy', 'oi_trend']
    trend_field_names = []

    for field in trend_fields:
        if field not in trend_df.columns:
            continue
        pivot = trend_df.pivot_table(index='period_end', columns='ticker', values=field, aggfunc='last')
        if pivot.empty or pivot.notna().sum().sum() < 50:
            continue
        # ì¼ë³„ reindex + forward-fill
        daily = pivot.reindex(close.index).ffill()
        daily = daily.reindex(columns=close.columns)
        # cross-sectional rank [0,1] (ë§¤ì¼)
        ranked = daily.rank(axis=1, pct=True)
        var_name = f'{field}_rank'
        trend_vars[var_name] = ranked
        trend_field_names.append(var_name)

    print(f"âœ… {len(close.columns)}ê°œ ì¢…ëª©, {len(close)}ì¼ ë°ì´í„°")
    print(f"   ê°€ê²© ë³€ìˆ˜: close, open_price, high, low, volume, returns, vwap, high_low_range, body")
    if trend_field_names:
        print(f"   ì¬ë¬´ ì¶”ì„¸ ë³€ìˆ˜ ({len(trend_field_names)}ê°œ): {', '.join(trend_field_names)}")
    else:
        print(f"   âš ï¸  ì¬ë¬´ ì¶”ì„¸ ë³€ìˆ˜ ìƒì„± ì‹¤íŒ¨")

    result = {
        'close': close,
        'open_price': open_price,
        'high': high,
        'low': low,
        'volume': volume,
        'returns': returns,
        'vwap': vwap,
        'high_low_range': high_low_range,
        'body': body,
        'upper_shadow': upper_shadow,
        'lower_shadow': lower_shadow,
    }
    result.update(trend_vars)
    return result

def generate_seed_alphas_gpt4o(num_seeds=20):
    """GPT-4o + ê°œì„ ëœ QuantDeveloper í”„ë¡¬í”„íŠ¸ë¡œ ì‹œë“œ ì•ŒíŒŒ ìƒì„±"""
    print(f"\nğŸ¤– GPT-4oë¡œ ì´ˆê¸° ì•ŒíŒŒ {num_seeds}ê°œ ìƒì„± ì¤‘ (ê°œì„ ëœ í”„ë¡¬í”„íŠ¸)...")

    client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

    # QuantDeveloperì˜ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ì¬ì‚¬ìš©
    system_prompt = QuantDeveloper.SYSTEM_PROMPT

    # 15ì¼ ë³´ìœ ì— íŠ¹í™”ëœ user prompt
    prompt = f"""### Task
Generate {num_seeds} diverse, high-performance alpha expressions optimized for **15-day forward returns** in the Korean stock market.

### Trading Idea
15ì¼ ë³´ìœ  ì „ëµì— ìµœì í™”ëœ ì¤‘ê¸° ì•ŒíŒŒ íŒ©í„°. ë‹¨ê¸° ë…¸ì´ì¦ˆë¥¼ í•„í„°ë§í•˜ê³ ,
15ì¼ í›„ ìˆ˜ìµë¥ ê³¼ ë†’ì€ ìƒê´€ê´€ê³„(IC)ë¥¼ ê°€ì§€ëŠ” ì‹œê·¸ë„ì„ ì°¾ì•„ì•¼ í•¨.
ëª¨ë©˜í…€, ê±°ë˜ëŸ‰, ë³€ë™ì„±, ì¶”ì„¸ ê°•ë„ë¥¼ ì¡°í•©í•˜ì—¬ ë‹¤ì–‘í•œ íŒ©í„°ë¥¼ ìƒì„±.

### Available Data Fields
close, open_price, high, low, volume, returns, vwap, high_low_range, body, upper_shadow, lower_shadow,
oi_yoy_rank, ni_yoy_rank, oi_qoq_rank, oi_trend_rank, margin_yoy_rank, roe_yoy_rank

**Price variables** (daily time-series):
- `close`, `open_price`, `high`, `low`: OHLC prices
- `volume`: trading volume
- `returns`: daily close-to-close returns
- `vwap`: (high + low + close) / 3
- `high_low_range`, `body`, `upper_shadow`, `lower_shadow`: candle pattern ratios

**Fundamental TREND rank variables** (cross-sectional rank [0,1], higher = more improvement):
- `oi_yoy_rank`: ì˜ì—…ì´ìµ YoY ë³€í™”ìœ¨ ìˆœìœ„ (ì „ë…„ ë™ë¶„ê¸° ëŒ€ë¹„ ê°œì„ ë„)
- `ni_yoy_rank`: ìˆœì´ìµ YoY ë³€í™”ìœ¨ ìˆœìœ„
- `oi_qoq_rank`: ì˜ì—…ì´ìµ QoQ ë³€í™”ìœ¨ ìˆœìœ„ (ì „ë¶„ê¸° ëŒ€ë¹„ ê°œì„ ë„)
- `oi_trend_rank`: ì˜ì—…ì´ìµ 3ë¶„ê¸° ì¶”ì„¸ ê¸°ìš¸ê¸° ìˆœìœ„
- `margin_yoy_rank`: ì˜ì—…ì´ìµë¥  YoY ë³€í™” ìˆœìœ„
- `roe_yoy_rank`: ROE YoY ë³€í™” ìˆœìœ„

**IMPORTANT for fundamental trend variables**:
- These capture IMPROVEMENT (not level) â€” "ROEê°€ ê°œì„ ì¤‘ì¸ ì¢…ëª©" not "ROEê°€ ë†’ì€ ì¢…ëª©"
- Already cross-sectionally ranked [0,1], do NOT apply normed_rank() again
- Use as weights with price signals: `ops.cwise_mul(ops.ts_delta_ratio(close, 15), oi_yoy_rank)` â€” momentum Ã— earnings improvement
- DO NOT apply ts_delta/ts_delta_ratio/ts_corr on them (they change quarterly, will create artifacts)

### Requirements

**Diversity** â€” Each alpha MUST belong to a DIFFERENT category:
  1. `ma_golden_cross` â€” Moving average crossover: `ops.div(ops.ts_mean(close, 5), ops.ts_mean(close, 20))` Ã— fundamental rank (PROVEN: IC 0.039)
  2. `ma_distance` â€” Price distance from long-term MA: `ops.div(close, ops.ts_mean(close, 120))` (PROVEN: IC 0.037)
  3. `ma_slope` â€” MA slope/trend strength: `ops.ts_delta_ratio(ops.ts_mean(close, 60), 10)` (PROVEN: IC 0.032)
  4. `ma_multi_volume` â€” Multiple MA crossover + volume confirmation (PROVEN: IC 0.031)
  5. `momentum_volume` â€” Momentum confirmed by volume surge
  6. `volatility_adjusted` â€” Signal adjusted/filtered by volatility (use high_low_range)
  7. `short_term_reversal` â€” Mean-reversion exploiting KRX reversal effect
  8. `multi_timeframe` â€” Combining short + medium + long timeframes (use 5/20/60/120 windows)
  9. `price_volume_diverge` â€” Price-volume divergence / smart money
  10. `trend_strength` â€” Trend strength via regression slope or IR
  11. `earnings_momentum` â€” Price momentum Ã— earnings improvement (oi_yoy_rank, oi_qoq_rank)
  12. `price_position` â€” Price position relative to recent high/low
  13. `volume_anomaly` â€” Abnormal volume detection
  14. `earnings_reversal` â€” Oversold + earnings improving (reversal Ã— oi_yoy_rank)
  15. `quality_momentum` â€” Momentum weighted by margin improvement (margin_yoy_rank)
  16. `roe_improvement` â€” Price Ã— ROE improvement (roe_yoy_rank)
  17. `trend_confirmation` â€” Price trend + earnings trend aligned (oi_trend_rank)
  18. `candle_pattern` â€” Candle body/shadow patterns
  19. `ma_earnings_composite` â€” MA signal Ã— multiple fundamental ranks (3+ factors)
  20. `composite_all` â€” Most complex: MA + price + volume + fundamental trend

**15-Day Holding Optimization**:
- **Moving averages are highly effective** â€” use `ops.ts_mean(close, N)` for MA(N), `ops.div(close, ops.ts_mean(close, N))` for distance from MA
- Use diverse lookback windows: 5, 10, 15, 20, 30, 60, 120 days
- Combine at least 2 timeframes per alpha
- Volume confirmation is critical for 15-day predictions
- **Multiplicative combination with fundamental ranks works better than additive**: use `ops.cwise_mul(price_signal, oi_trend_rank)`

**Quality Checklist** â€” Every alpha must satisfy ALL:
- [ ] Multi-factor: combines 2+ distinct signal types
- [ ] Market-neutral: wrapped with `ops.normed_rank()` or `ops.zscore_scale()`
- [ ] Multi-timeframe: uses 2+ lookback windows
- [ ] No look-ahead bias
- [ ] Complexity 2~4 nesting levels
- [ ] Safe division: use `ops.div()` instead of raw `/`

### Output Format
Return a JSON array:
```json
[
  {{
    "alpha_name": "Alpha_Name",
    "category": "category_name",
    "rationale": "Economic logic explanation",
    "expression": "ops.normed_rank(...)",
    "complexity": 4,
    "operators_used": ["op1", "op2"],
    "timeframes_used": [10, 20]
  }}
]
```

**CRITICAL**:
- You MUST return a JSON object with key "alphas" containing an array of {num_seeds} alpha objects.
- Format: {{"alphas": [{{...}}, {{...}}, ...]}}
- Each object MUST have "expression" field with valid ops.xxx() Python code.
- Generate ALL {num_seeds} alphas. Do NOT return just 1."""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=16000,
        response_format={"type": "json_object"}
    )

    content = response.choices[0].message.content
    print(f"   GPT-4o ì‘ë‹µ ê¸¸ì´: {len(content)}ì")
    print(f"   ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {content[:200]}...")

    # JSON íŒŒì‹±
    alphas = []
    try:
        data = json.loads(content)
        print(f"   íŒŒì‹±ëœ íƒ€ì…: {type(data).__name__}")

        # dict â†’ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        if isinstance(data, dict):
            print(f"   í‚¤ ëª©ë¡: {list(data.keys())}")

            # 1ìˆœìœ„: dict ìì²´ê°€ ë‹¨ì¼ ì•ŒíŒŒì¸ ê²½ìš° (expression í‚¤ ì¡´ì¬)
            if 'expression' in data or 'expr' in data:
                data = [data]
                print(f"   ë‹¨ì¼ ì•ŒíŒŒ dict â†’ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜")

            else:
                # 2ìˆœìœ„: {"alphas": [{...}, ...]} í˜•íƒœ â€” dict ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì§„ í‚¤ ì°¾ê¸°
                found_list = False
                for key in data:
                    if isinstance(data[key], list) and data[key] and isinstance(data[key][0], dict):
                        data = data[key]
                        print(f"   '{key}' í‚¤ì—ì„œ {len(data)}ê°œ í•­ëª© ì¶”ì¶œ")
                        found_list = True
                        break

                if not found_list:
                    # 3ìˆœìœ„: ì¤‘ì²© dict: {"alpha_1": {...}, "alpha_2": {...}}
                    items = []
                    for key, val in data.items():
                        if isinstance(val, dict) and ('expression' in val or 'expr' in val):
                            items.append(val)
                    if items:
                        data = items
                        print(f"   ì¤‘ì²© dictì—ì„œ {len(items)}ê°œ í•­ëª© ì¶”ì¶œ")
                    else:
                        print(f"   âš ï¸  ì•Œ ìˆ˜ ì—†ëŠ” dict êµ¬ì¡°: {list(data.keys())[:5]}")
                        data = []

        for item in data:
            if isinstance(item, str):
                if 'ops.' in item:
                    alphas.append(item)
                continue
            if not isinstance(item, dict):
                continue
            expr = item.get('expression', item.get('expr', ''))
            if expr and 'ops.' in expr:
                alphas.append(expr)
            elif expr:
                print(f"   âš ï¸  ops. ì—†ëŠ” í‘œí˜„ì‹ ìŠ¤í‚µ: {expr[:80]}")

    except (json.JSONDecodeError, Exception) as e:
        print(f"âš ï¸  JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì•ˆì˜ JSON ì¶”ì¶œ ì‹œë„
        json_match = re.search(r'```(?:json)?\s*(\[.*?\])\s*```', content, re.DOTALL)
        if json_match:
            try:
                data = json.loads(json_match.group(1))
                for item in data:
                    if isinstance(item, dict):
                        expr = item.get('expression', item.get('expr', ''))
                        if expr and 'ops.' in expr:
                            alphas.append(expr)
                print(f"   ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì—ì„œ {len(alphas)}ê°œ ë³µêµ¬")
            except Exception:
                pass

    # í´ë°±: ê°œì„ ëœ ë³µí•© íŒ©í„° + MA ê¸°ë°˜ ì•ŒíŒŒ
    if len(alphas) < 10:
        print(f"âš ï¸  {len(alphas)}ê°œë§Œ íŒŒì‹±ë¨, í´ë°± ì¶”ê°€")
        fallback = [
            # â”€â”€ MA ê¸°ë°˜ ì•ŒíŒŒ (ê²€ì¦ ì™„ë£Œ, Test IC 0.03~0.04) â”€â”€
            # ê³¨ë“ í¬ë¡œìŠ¤(5/20) Ã— ì˜ì—…ì´ìµ ì¶”ì„¸ (Test IC 0.0391)
            "ops.normed_rank(ops.cwise_mul(ops.div(ops.ts_mean(close, 5), ops.ts_mean(close, 20)), oi_trend_rank))",
            # 120ì¼ ì´ê²©ë„ (Test IC 0.0369)
            "ops.normed_rank(ops.div(close, ops.ts_mean(close, 120)))",
            # 60ì¼ MA ê¸°ìš¸ê¸° (Test IC 0.0323)
            "ops.normed_rank(ops.ts_delta_ratio(ops.ts_mean(close, 60), 10))",
            # ë‹¤ì¤‘ MA ì¢…í•©: ê³¨ë“ í¬ë¡œìŠ¤ Ã— ê±°ë˜ëŸ‰ + MAê¸°ìš¸ê¸° (Test IC 0.0311)
            "ops.normed_rank(ops.add(ops.cwise_mul(ops.div(ops.ts_mean(close, 5), ops.ts_mean(close, 20)), ops.div(ops.ts_mean(volume, 5), ops.ts_mean(volume, 20))), ops.ts_delta_ratio(ops.ts_mean(close, 20), 10)))",
            # ì´ê²©ë„ Ã— ê±°ë˜ëŸ‰ (Test IC 0.0288)
            "ops.normed_rank(ops.cwise_mul(ops.div(close, ops.ts_mean(close, 20)), ops.div(ops.ts_mean(volume, 5), ops.ts_mean(volume, 20))))",
            # ê³¨ë“ í¬ë¡œìŠ¤ Ã— ê±°ë˜ëŸ‰ (Test IC 0.0280)
            "ops.normed_rank(ops.cwise_mul(ops.div(ops.ts_mean(close, 5), ops.ts_mean(close, 20)), ops.div(ops.ts_mean(volume, 5), ops.ts_mean(volume, 20))))",
            # MAì´ê²© 120d Ã— oi_yoy (Test IC 0.0233)
            "ops.normed_rank(ops.cwise_mul(ops.div(close, ops.ts_mean(close, 120)), oi_yoy_rank))",
            # MAê¸°ìš¸ê¸° 60d Ã— oi_yoy (Test IC 0.0237)
            "ops.normed_rank(ops.cwise_mul(ops.ts_delta_ratio(ops.ts_mean(close, 60), 10), oi_yoy_rank))",
            # â”€â”€ ê¸°ì¡´ ê²€ì¦ëœ íŒ©í„° â”€â”€
            "ops.normed_rank(ops.cwise_mul(ops.cwise_mul(ops.ts_delta_ratio(close, 25), ops.div(ops.ts_median(volume, 10), ops.ts_std(volume, 15))), ops.ts_maxmin_scale(close, 28)))",
            "ops.normed_rank(ops.cwise_mul(ops.ts_delta_ratio(close, 15), ops.div(ops.ts_mean(volume, 5), ops.ts_mean(volume, 20))))",
            "ops.normed_rank(ops.neg(ops.ts_corr(ops.ts_delta(close, 5), ops.ts_delta(volume, 5), 20)))",
            # â”€â”€ ê°€ê²© Ã— ì¬ë¬´ ì¶”ì„¸ ë³µí•© íŒ©í„° â”€â”€
            "ops.normed_rank(ops.cwise_mul(ops.ts_delta_ratio(close, 15), oi_yoy_rank))",
            "ops.normed_rank(ops.cwise_mul(ops.neg(ops.ts_zscore_scale(close, 15)), oi_qoq_rank))",
            "ops.normed_rank(ops.cwise_mul(ops.ts_maxmin_scale(close, 20), oi_trend_rank))",
            "ops.normed_rank(ops.cwise_mul(ops.ts_delta_ratio(close, 20), margin_yoy_rank))",
            "ops.normed_rank(ops.cwise_mul(ops.cwise_mul(ops.ts_delta_ratio(close, 15), oi_yoy_rank), ops.div(ops.ts_mean(volume, 5), ops.ts_mean(volume, 20))))",
            "ops.normed_rank(ops.cwise_mul(ops.ts_linear_reg(close, 20), roe_yoy_rank))",
            "ops.normed_rank(ops.cwise_mul(ops.ts_delta_ratio(close, 20), ops.neg(ops.ts_mean(high_low_range, 15))))",
            "ops.normed_rank(ops.div(ops.neg(ops.ts_zscore_scale(close, 10)), ops.ts_std(returns, 20)))",
            "ops.normed_rank(ops.minus(ops.ts_ir(returns, 5), ops.ts_ir(returns, 20)))",
        ]
        alphas = alphas + [f for f in fallback if f not in alphas]

    print(f"âœ… {len(alphas)}ê°œ ì´ˆê¸° ì•ŒíŒŒ ìƒì„±")
    for i, a in enumerate(alphas[:5], 1):
        print(f"   {i}. {a[:80]}...")

    return alphas[:num_seeds]

# ì „ì—­ ë°ì´í„°
_global_data = None

def set_global_data(data):
    global _global_data
    _global_data = data

def _compute_raw_ic(alpha_expr, data):
    """ì•ŒíŒŒì˜ raw IC ê³„ì‚° (train ë˜ëŠ” test ë°ì´í„°)"""
    close = data['close']
    open_price = data['open_price']
    high = data['high']
    low = data['low']
    volume = data['volume']
    returns = data['returns']
    vwap = data['vwap']
    high_low_range = data['high_low_range']
    body = data['body']
    upper_shadow = data.get('upper_shadow', (high - close.clip(lower=open_price)) / close)
    lower_shadow = data.get('lower_shadow', (close.clip(upper=open_price) - low) / close)
    # ì¬ë¬´ ì¶”ì„¸ rank ë³€ìˆ˜ (0~1, cross-sectional rank, ë†’ì„ìˆ˜ë¡ ê°œì„  ì¤‘)
    _empty = pd.DataFrame(np.nan, index=close.index, columns=close.columns)
    oi_yoy_rank = data.get('oi_yoy_rank', _empty)
    ni_yoy_rank = data.get('ni_yoy_rank', _empty)
    oi_qoq_rank = data.get('oi_qoq_rank', _empty)
    ni_qoq_rank = data.get('ni_qoq_rank', _empty)
    oi_trend_rank = data.get('oi_trend_rank', _empty)
    margin_yoy_rank = data.get('margin_yoy_rank', _empty)
    roe_yoy_rank = data.get('roe_yoy_rank', _empty)

    forward_return_15d = close.shift(-15) / close - 1
    alpha_values = eval(alpha_expr)

    if not isinstance(alpha_values, pd.DataFrame):
        return -999.0

    ic_list = []
    for date in alpha_values.index[:-15]:
        alpha_cs = alpha_values.loc[date]
        returns_cs = forward_return_15d.loc[date]
        valid = alpha_cs.notna() & returns_cs.notna()

        if valid.sum() > 30:
            ic = alpha_cs[valid].corr(returns_cs[valid])
            if not np.isnan(ic):
                ic_list.append(ic)

    if len(ic_list) < 10:
        return -999.0

    return float(np.mean(ic_list))

def _multi_factor_bonus(alpha_expr):
    """ë‹¤ì¤‘ íŒ©í„° êµ¬ì¡° ë³´ë„ˆìŠ¤"""
    bonus = 0.0
    trend_vars = ['oi_yoy_rank', 'ni_yoy_rank', 'oi_qoq_rank', 'ni_qoq_rank', 'oi_trend_rank', 'margin_yoy_rank', 'roe_yoy_rank']

    # rank ë³€ìˆ˜ì— ts_delta/ts_corr â†’ ê°•í•œ í˜ë„í‹° (ë¶„ê¸° ê²½ê³„ artifacts)
    for rv in trend_vars:
        if rv in alpha_expr:
            if re.search(rf'ts_(delta|delta_ratio|corr|cov)\([^)]*{rv}', alpha_expr):
                return -0.05

    # ê±°ë˜ëŸ‰ ì‚¬ìš© ë³´ë„ˆìŠ¤
    if 'volume' in alpha_expr:
        bonus += 0.002
    # MA êµ¬ì¡° ë³´ë„ˆìŠ¤ (ê²€ì¦ëœ íŒ¨í„´)
    has_ma = bool(re.search(r'ts_mean\([^)]*close[^)]*,\s*\d+\)', alpha_expr))
    if has_ma:
        bonus += 0.002
        # MA + ì¬ë¬´ì¶”ì„¸ ê³±ì…ˆ ê²°í•© ë³´ë„ˆìŠ¤ (ê°€ì¥ ê°•ë ¥í•œ íŒ¨í„´)
        has_trend = any(tv in alpha_expr for tv in trend_vars)
        if has_trend and 'cwise_mul' in alpha_expr:
            bonus += 0.003
    # ê°€ê²©+ì¶”ì„¸ ê²°í•© ë³´ë„ˆìŠ¤ (ë‹¤ì¤‘íŒ©í„° ì¥ë ¤)
    has_price_ts = bool(re.search(r'ts_\w+\([^)]*(?:close|open_price|high|low|volume|returns)', alpha_expr))
    has_trend = any(tv in alpha_expr for tv in trend_vars)
    if has_price_ts and has_trend:
        bonus += 0.003
    # ë‹¤ì¤‘ íƒ€ì„í”„ë ˆì„ ë³´ë„ˆìŠ¤ (ìœˆë„ìš° ì°¨ì´ â‰¥ 2ë°°)
    windows = [int(w) for w in re.findall(r',\s*(\d+)\)', alpha_expr)]
    if len(windows) >= 2 and max(windows) >= min(windows) * 2:
        bonus += 0.002
    # ì¥ê¸° ìœˆë„ìš° ë³´ë„ˆìŠ¤ (60ì¼+ ì‚¬ìš© ì‹œ)
    if windows and max(windows) >= 60:
        bonus += 0.001
    # ë³µì¡ë„ í˜ë„í‹°
    depth = alpha_expr.count('(')
    if depth < 3:
        bonus -= 0.002
    if depth > 10:
        bonus -= 0.003
    return bonus

def evaluate_alpha_worker(alpha_expr):
    """ë³‘ë ¬ ì²˜ë¦¬ìš© ì•ŒíŒŒ í‰ê°€ â€” train IC + ë‹¤ì¤‘íŒ©í„° ë³´ë„ˆìŠ¤"""
    global _global_data
    data = _global_data

    try:
        raw_ic = _compute_raw_ic(alpha_expr, data)
        if raw_ic <= -999.0:
            return (alpha_expr, -999.0)
        bonus = _multi_factor_bonus(alpha_expr)
        return (alpha_expr, raw_ic + bonus)
    except Exception:
        return (alpha_expr, -999.0)

def evaluate_alpha_oos(alpha_expr, test_data):
    """Out-of-sample IC ê³„ì‚° (ë³´ë„ˆìŠ¤ ì—†ì´ ìˆœìˆ˜ IC)"""
    try:
        return _compute_raw_ic(alpha_expr, test_data)
    except Exception:
        return -999.0

# ì—°ì‚°ì êµí™˜ ê·¸ë£¹ (ê°™ì€ ì‹œê·¸ë‹ˆì²˜ë¼ë¦¬ë§Œ êµì²´)
OPERATOR_SWAP_GROUPS = [
    ['ts_mean', 'ts_std', 'ts_median', 'ts_ema', 'ts_linear_reg', 'ts_decayed_linear'],
    ['ts_zscore_scale', 'ts_maxmin_scale', 'ts_rank'],
    ['ts_delta', 'ts_delta_ratio'],
    ['ts_skew', 'ts_kurt', 'ts_ir'],
    ['ts_min', 'ts_max'],
    ['ts_argmin', 'ts_argmax'],
    ['ts_max_diff', 'ts_min_diff'],
    ['normed_rank', 'zscore_scale'],
    ['cwise_mul', 'add', 'minus'],
]

OPERAND_POOL = ['close', 'open_price', 'high', 'low', 'volume', 'returns', 'vwap', 'high_low_range', 'body',
                'oi_yoy_rank', 'ni_yoy_rank', 'oi_qoq_rank', 'oi_trend_rank', 'margin_yoy_rank', 'roe_yoy_rank']

def mutate_alpha(alpha_expr):
    """ì•ŒíŒŒ ë³€ì´ â€” 3ê°€ì§€ íƒ€ì…: ìœˆë„ìš°(50%), ì—°ì‚°ì(30%), í”¼ì—°ì‚°ì(20%)"""
    try:
        mutation_type = random.choices(
            ['window', 'operator', 'operand'],
            weights=[0.5, 0.3, 0.2]
        )[0]

        if mutation_type == 'window':
            return _mutate_window(alpha_expr)
        elif mutation_type == 'operator':
            return _mutate_operator(alpha_expr)
        else:
            return _mutate_operand(alpha_expr)
    except Exception:
        return None

def _mutate_window(alpha_expr):
    """ìœˆë„ìš° íŒŒë¼ë¯¸í„° ë³€ê²½ (ë²”ìœ„ 5~120, MA ì¥ê¸° ì‹œê·¸ë„ ì§€ì›)"""
    matches = list(re.finditer(r'(ts_\w+|shift)\([^,]+,\s*(\d+)\)', alpha_expr))
    if not matches:
        return None
    match = random.choice(matches)
    old_window = int(match.group(2))
    # í˜„ì¬ ìœˆë„ìš° í¬ê¸°ì— ë”°ë¼ ë³€ì´ í­ ì¡°ì ˆ (ë¹„ë¡€ì  ë³€ì´)
    if old_window <= 20:
        deltas = [-5, -3, -2, 2, 3, 5, 7, 10, 15]
    elif old_window <= 60:
        deltas = [-15, -10, -7, -5, 5, 7, 10, 15, 20, 30]
    else:
        deltas = [-30, -20, -10, 10, 20, 30]
    new_window = max(5, min(120, old_window + random.choice(deltas)))
    if new_window == old_window:
        new_window = max(5, min(120, old_window + random.choice([-20, 20])))
    start, end = match.span(2)
    return alpha_expr[:start] + str(new_window) + alpha_expr[end:]

def _mutate_operator(alpha_expr):
    """ì—°ì‚°ì êµì²´ â€” ê°™ì€ ì‹œê·¸ë‹ˆì²˜ ê·¸ë£¹ ë‚´ì—ì„œë§Œ"""
    # í˜„ì¬ í‘œí˜„ì‹ì—ì„œ ì—°ì‚°ì ì¶”ì¶œ
    op_matches = list(re.finditer(r'ops\.(\w+)\(', alpha_expr))
    if not op_matches:
        return None

    # êµí™˜ ê°€ëŠ¥í•œ ì—°ì‚°ìë§Œ í•„í„°
    swappable = []
    for m in op_matches:
        op_name = m.group(1)
        for group in OPERATOR_SWAP_GROUPS:
            if op_name in group:
                swappable.append((m, op_name, group))
                break

    if not swappable:
        return _mutate_window(alpha_expr)  # êµí™˜ ë¶ˆê°€ë©´ ìœˆë„ìš° ë³€ì´

    match, old_op, group = random.choice(swappable)
    candidates = [op for op in group if op != old_op]
    if not candidates:
        return _mutate_window(alpha_expr)

    new_op = random.choice(candidates)
    start, end = match.span(1)
    return alpha_expr[:start] + new_op + alpha_expr[end:]

def _mutate_operand(alpha_expr):
    """í”¼ì—°ì‚°ì êµì²´ â€” close/volume/returns ê°„ êµí™˜"""
    present = [op for op in OPERAND_POOL if op in alpha_expr]
    if not present:
        return _mutate_window(alpha_expr)

    old_operand = random.choice(present)
    candidates = [op for op in OPERAND_POOL if op != old_operand]
    new_operand = random.choice(candidates)

    # ì²« ë²ˆì§¸ ë“±ì¥ë§Œ êµì²´ (ì „ì²´ êµì²´ ë°©ì§€)
    return alpha_expr.replace(old_operand, new_operand, 1)


def _subtree_crossover(alpha1, alpha2):
    """ì„œë¸ŒíŠ¸ë¦¬ êµì°¨ â€” í•œ ì•ŒíŒŒì˜ ì„œë¸ŒíŠ¸ë¦¬ë¥¼ ë‹¤ë¥¸ ì•ŒíŒŒì˜ ì„œë¸ŒíŠ¸ë¦¬ë¡œ êµì²´"""
    try:
        # ops.xxx(...) íŒ¨í„´ì˜ ì„œë¸ŒíŠ¸ë¦¬ ì¶”ì¶œ
        def find_subtrees(expr):
            """ê´„í˜¸ ë§¤ì¹­ìœ¼ë¡œ ops.xxx(...) ì„œë¸ŒíŠ¸ë¦¬ ìœ„ì¹˜ ì°¾ê¸°"""
            subtrees = []
            for m in re.finditer(r'ops\.\w+\(', expr):
                start = m.start()
                depth = 0
                for i in range(m.end() - 1, len(expr)):
                    if expr[i] == '(':
                        depth += 1
                    elif expr[i] == ')':
                        depth -= 1
                    if depth == 0:
                        subtrees.append((start, i + 1, expr[start:i+1]))
                        break
            return subtrees

        trees1 = find_subtrees(alpha1)
        trees2 = find_subtrees(alpha2)

        if len(trees1) < 2 or not trees2:
            return None

        # alpha1ì—ì„œ êµì²´í•  ì„œë¸ŒíŠ¸ë¦¬ ì„ íƒ (ìµœìƒìœ„ ì œì™¸)
        replaceable = [t for t in trees1 if t[2] != alpha1]
        if not replaceable:
            return None

        target = random.choice(replaceable)
        donor = random.choice(trees2)

        result = alpha1[:target[0]] + donor[2] + alpha1[target[1]:]
        # ìœ íš¨ì„± ê²€ì‚¬: ops.ê°€ ìˆê³  ê´„í˜¸ê°€ ë§ëŠ”ì§€
        if result.count('(') != result.count(')') or 'ops.' not in result:
            return None
        return result
    except Exception:
        return None


def crossover_alphas(alpha1, alpha2):
    """ì•ŒíŒŒ êµì°¨ â€” ìœˆë„ìš° êµí™˜(60%) + ì„œë¸ŒíŠ¸ë¦¬ êµì°¨(40%)"""
    try:
        # 40% í™•ë¥ ë¡œ ì„œë¸ŒíŠ¸ë¦¬ êµì°¨ ì‹œë„
        if random.random() < 0.4:
            result = _subtree_crossover(alpha1, alpha2)
            if result:
                return result

        matches1 = list(re.finditer(r'(ts_\w+|shift)\(([^,]+),\s*(\d+)\)', alpha1))
        matches2 = list(re.finditer(r'(ts_\w+|shift)\(([^,]+),\s*(\d+)\)', alpha2))

        if not matches1 or not matches2:
            return None

        # ê°™ì€ ì—°ì‚°ìê°€ ìˆìœ¼ë©´ ìš°ì„  êµì°¨
        ops1 = {m.group(1): m for m in matches1}
        ops2 = {m.group(1): m for m in matches2}
        common_ops = set(ops1.keys()) & set(ops2.keys())

        if common_ops:
            op = random.choice(list(common_ops))
            m1, m2 = ops1[op], ops2[op]
        else:
            m1 = random.choice(matches1)
            m2 = random.choice(matches2)

        # alpha1ì˜ ìœˆë„ìš°ë¥¼ alpha2ì˜ ê°’ìœ¼ë¡œ êµì²´
        win2 = m2.group(3)
        start, end = m1.span(3)
        return alpha1[:start] + win2 + alpha1[end:]
    except Exception:
        return None

def _get_alpha_structure(alpha_expr):
    """ì•ŒíŒŒì˜ êµ¬ì¡° ì‹œê·¸ë‹ˆì²˜ (ìœˆë„ìš° ì œê±°) â€” ë‹¤ì–‘ì„± ë¹„êµìš©"""
    return re.sub(r',\s*\d+\)', ', N)', alpha_expr)

def _select_diverse_top_n(results, n=5):
    """IC ìƒìœ„ì—ì„œ êµ¬ì¡°ê°€ ë‹¤ë¥¸ Top-N ì„ íƒ"""
    sorted_results = sorted(results, key=lambda x: x[1], reverse=True)
    selected = []
    seen_structures = set()

    for alpha, ic in sorted_results:
        if ic <= -999.0:
            continue
        structure = _get_alpha_structure(alpha)
        if structure not in seen_structures:
            selected.append((alpha, ic))
            seen_structures.add(structure)
            if len(selected) >= n:
                break

    return selected

def _tournament_select(fitness_scores, tournament_size=5):
    """í† ë„ˆë¨¼íŠ¸ ì„ íƒ â€” ë‹¤ì–‘ì„± ìœ ì§€í•˜ë©´ì„œ ìš°ìˆ˜ ê°œì²´ ì„ í˜¸"""
    candidates = random.sample(fitness_scores, min(tournament_size, len(fitness_scores)))
    return max(candidates, key=lambda x: x[1])[0]


def _fitness_sharing(fitness_scores, sharing_radius=0.8):
    """ì í•©ë„ ê³µìœ  â€” ê°™ì€ êµ¬ì¡°ì˜ ì•ŒíŒŒë¼ë¦¬ fitnessë¥¼ ë‚˜ëˆ  ë‹¤ì–‘ì„± ë³´ì¡´"""
    structures = {}
    for alpha, ic in fitness_scores:
        struct = _get_alpha_structure(alpha)
        if struct not in structures:
            structures[struct] = []
        structures[struct].append((alpha, ic))

    shared = []
    for struct, members in structures.items():
        niche_size = len(members)
        for alpha, ic in members:
            # ê°™ì€ êµ¬ì¡°ê°€ ë§ì„ìˆ˜ë¡ fitness ê°ì†Œ (niche pressure)
            shared_ic = ic / (1.0 + sharing_radius * (niche_size - 1))
            shared.append((alpha, shared_ic))

    return sorted(shared, key=lambda x: x[1], reverse=True)


def genetic_programming(seed_alphas, data, generations=50, population_size=200):
    """ìµœì í™”ëœ ë³‘ë ¬ GP â€” ì´ë¯¼ + í† ë„ˆë¨¼íŠ¸ ì„ íƒ + ì í•©ë„ ê³µìœ  + ì ì‘ì  ë³€ì´"""

    print(f"\nğŸ§¬ ë³‘ë ¬ GP ì‹œì‘ (v2 ìµœì í™”)")
    print(f"   Seed: {len(seed_alphas)}ê°œ, ì„¸ëŒ€: {generations}, ê°œì²´ìˆ˜: {population_size}, ì›Œì»¤: 4")

    population = seed_alphas[:population_size]
    while len(population) < population_size:
        parent = random.choice(seed_alphas)
        mutated = mutate_alpha(parent)
        if mutated:
            population.append(mutated)

    set_global_data(data)
    best_ever = (None, -999.0)
    stagnation_count = 0
    immigration_count = 0
    all_results_history = []

    elite_count = max(5, population_size // 14)  # 7% ì—˜ë¦¬íŠ¸ (ìˆ˜ë ´ ì§€ì—°)
    base_mutation_rate = 0.45  # ê¸°ë³¸ ë³€ì´ìœ¨

    for gen in range(1, generations + 1):
        # ì ì‘ì  ë³€ì´ìœ¨: ì •ì²´ ì‹œ ë³€ì´ ë¹„ì¤‘ ì¦ê°€
        mutation_rate = min(0.7, base_mutation_rate + stagnation_count * 0.05)
        crossover_rate = 1.0 - mutation_rate

        print(f"\n  ì„¸ëŒ€ {gen}/{generations} (ë³€ì´ìœ¨: {mutation_rate:.0%}, ì •ì²´: {stagnation_count})")

        with Pool(4, initializer=set_global_data, initargs=(data,)) as pool:
            results = pool.map(evaluate_alpha_worker, population)

        # ì í•©ë„ ê³µìœ  ì ìš© (ê°™ì€ êµ¬ì¡°ë¼ë¦¬ fitness ë¶„ì‚°)
        raw_scores = sorted(results, key=lambda x: x[1], reverse=True)
        fitness_scores = _fitness_sharing(raw_scores)
        all_results_history.extend([(a, ic) for a, ic in raw_scores if ic > -999.0])

        best_ic = raw_scores[0][1]  # ê³µìœ  ì „ ì‹¤ì œ IC
        median_ic = raw_scores[len(raw_scores)//2][1] if raw_scores else -999.0
        unique_structures = len(set(_get_alpha_structure(a) for a, _ in raw_scores if _ > -999.0))
        print(f"    ìµœê³  IC: {best_ic:.4f}  ì¤‘ì•™ê°’: {median_ic:.4f}  ê³ ìœ êµ¬ì¡°: {unique_structures}ê°œ")

        if best_ic > best_ever[1]:
            best_ever = raw_scores[0]
            stagnation_count = 0
            print(f"    ğŸ† ì‹ ê¸°ë¡!")
        else:
            stagnation_count += 1

        # ì´ë¯¼(immigration): ì •ì²´ ì‹œ ìƒˆë¡œìš´ ê°œì²´ ì£¼ì… (ì¡°ê¸°ì¢…ë£Œ ëŒ€ì‹ )
        if stagnation_count >= 5 and immigration_count < 3:
            immigration_count += 1
            stagnation_count = 0
            n_immigrants = population_size // 4  # 25% êµì²´
            print(f"    ğŸŒ ì´ë¯¼ #{immigration_count}: {n_immigrants}ê°œ ìƒˆ ê°œì²´ ì£¼ì…")
            # ì‹œë“œì—ì„œ ìƒˆ ë³€ì´ ìƒì„±
            immigrants = []
            for _ in range(n_immigrants):
                parent = random.choice(seed_alphas)
                # 2-3íšŒ ì—°ì† ë³€ì´ë¡œ ë‹¤ì–‘ì„± ê·¹ëŒ€í™”
                for _ in range(random.randint(2, 3)):
                    m = mutate_alpha(parent)
                    if m:
                        parent = m
                immigrants.append(parent)
            # í•˜ìœ„ 25% êµì²´
            population = [a for a, _ in fitness_scores[:population_size - n_immigrants]] + immigrants
            continue

        # ìµœì¢… ì¢…ë£Œ: ì´ë¯¼ 3íšŒ í›„ì—ë„ 5ì„¸ëŒ€ ë¬´ê°œì„ 
        if stagnation_count >= 5:
            print(f"    â¹ï¸  ì´ë¯¼ {immigration_count}íšŒ í›„ 5ì„¸ëŒ€ ë¬´ê°œì„  â†’ ì¢…ë£Œ")
            break

        # ë‹¤ìŒ ì„¸ëŒ€ êµ¬ì„±
        next_population = []

        # ì—˜ë¦¬íŠ¸ ë³´ì¡´ (7%)
        for alpha, _ in fitness_scores[:elite_count]:
            next_population.append(alpha)

        # í† ë„ˆë¨¼íŠ¸ ì„ íƒ + êµì°¨/ë³€ì´
        while len(next_population) < population_size:
            if random.random() < crossover_rate:
                # í† ë„ˆë¨¼íŠ¸ ì„ íƒìœ¼ë¡œ ë¶€ëª¨ 2ê°œ ì„ íƒ â†’ êµì°¨
                parent1 = _tournament_select(fitness_scores, tournament_size=5)
                parent2 = _tournament_select(fitness_scores, tournament_size=5)
                child = crossover_alphas(parent1, parent2)
                if child:
                    next_population.append(child)
                else:
                    next_population.append(parent1)
            else:
                # í† ë„ˆë¨¼íŠ¸ ì„ íƒ â†’ ë³€ì´
                parent = _tournament_select(fitness_scores, tournament_size=5)
                mutated = mutate_alpha(parent)
                if mutated:
                    next_population.append(mutated)
                else:
                    next_population.append(parent)

        population = next_population[:population_size]

        del results, raw_scores, fitness_scores, next_population
        gc.collect()

    # Top-5 ë‹¤ì–‘í•œ ì•ŒíŒŒ ì„ íƒ
    top_diverse = _select_diverse_top_n(all_results_history, n=5)

    return best_ever, top_diverse

def main():
    print("=" * 80)
    print("Alpha-GPT: 15-day Forward with GPT-4o (v6 â€” Price + Fundamental Trend)")
    print("=" * 80)
    print()

    # 1. ì „ì²´ ë°ì´í„° ë¡œë“œ
    full_data = load_market_data()

    # 2. Train/Test ë¶„í•  (70/30)
    close = full_data['close']
    split_idx = int(len(close) * 0.7)
    split_date = close.index[split_idx]
    print(f"\nğŸ“ Train/Test ë¶„í• : {split_idx}ì¼ train / {len(close) - split_idx}ì¼ test")
    print(f"   Train: ~{close.index[0]} ~ {close.index[split_idx-1]}")
    print(f"   Test:  ~{split_date} ~ {close.index[-1]}")

    train_data = {k: v.iloc[:split_idx] for k, v in full_data.items()}
    test_data = {k: v.iloc[split_idx:] for k, v in full_data.items()}

    # 3. GPT-4o ì‹œë“œ ìƒì„±
    seed_alphas = generate_seed_alphas_gpt4o()

    # 4. GP ì§„í™” (train ë°ì´í„°ë¡œ)
    (best_alpha, best_ic), top_diverse = genetic_programming(
        seed_alphas,
        train_data,
        generations=50,
        population_size=200
    )

    # 5. Top-5 OOS ê²€ì¦
    print("\n" + "=" * 80)
    print("ğŸ† TOP 5 ALPHAS (Train IC + Test IC)")
    print("=" * 80)

    validated_alphas = []
    for i, (alpha, train_ic_with_bonus) in enumerate(top_diverse, 1):
        # ìˆœìˆ˜ train IC (ë³´ë„ˆìŠ¤ ì œê±°)
        train_ic = _compute_raw_ic(alpha, train_data)
        # OOS test IC
        test_ic = evaluate_alpha_oos(alpha, test_data)
        # íŒ©í„° ë¶„ë¥˜
        factors = []
        if any(kw in alpha for kw in ['close', 'open_price', 'high', 'low', 'vwap']):
            factors.append('price')
        if 'volume' in alpha:
            factors.append('volume')
        if 'returns' in alpha:
            factors.append('returns')
        if any(kw in alpha for kw in ['high_low_range', 'body', 'upper_shadow', 'lower_shadow']):
            factors.append('candle')
        if any(kw in alpha for kw in ['oi_yoy_rank', 'ni_yoy_rank', 'oi_qoq_rank', 'ni_qoq_rank', 'oi_trend_rank', 'margin_yoy_rank', 'roe_yoy_rank']):
            factors.append('fund_trend')
        factor_str = '+'.join(factors) if factors else 'unknown'

        status = "âœ…" if test_ic > 0.015 else "âš ï¸"
        print(f"\n  #{i} {status}")
        print(f"     Train IC: {train_ic:.4f}  |  Test IC: {test_ic:.4f}  [{factor_str}]")
        print(f"     {alpha[:100]}{'...' if len(alpha) > 100 else ''}")

        validated_alphas.append({
            'expr': alpha,
            'train_ic': train_ic,
            'test_ic': test_ic,
            'factors': factor_str,
        })

    # 6. ìµœì¢… Best ì„ ì • (test ICê°€ ì–‘ìˆ˜ì¸ ê²ƒ ì¤‘ train IC ìµœê³ )
    valid_alphas = [a for a in validated_alphas if a['test_ic'] > 0]
    if valid_alphas:
        final_best = max(valid_alphas, key=lambda x: x['train_ic'])
    else:
        final_best = validated_alphas[0] if validated_alphas else {'expr': best_alpha, 'train_ic': best_ic, 'test_ic': -999, 'factors': '?'}

    print("\n" + "=" * 80)
    print("ğŸ¥‡ FINAL BEST (OOS-validated)")
    print("=" * 80)
    print(f"Train IC: {final_best['train_ic']:.4f}")
    print(f"Test IC:  {final_best['test_ic']:.4f}")
    print(f"Factors:  {final_best['factors']}")
    print(f"Expression: {final_best['expr']}")

    # 7. DB ì €ì¥ (Top-5 ì „ë¶€)
    print("\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
    try:
        conn = get_db_connection()
        cursor = conn.cursor()

        for a in validated_alphas:
            cursor.execute("""
                INSERT INTO alpha_formulas (formula, ic_score, description, created_at)
                VALUES (%s, %s, %s, NOW())
                ON CONFLICT (formula) DO UPDATE
                SET ic_score = EXCLUDED.ic_score, updated_at = NOW()
            """, (
                a['expr'],
                float(a['train_ic']),
                f"15d fwd, train IC={a['train_ic']:.4f}, test IC={a['test_ic']:.4f}, factors={a['factors']}, v3-enhanced"
            ))

        conn.commit()
        cursor.close()
        conn.close()
        print(f"âœ… {len(validated_alphas)}ê°œ ì•ŒíŒŒ ì €ì¥ ì™„ë£Œ!")
    except Exception as e:
        print(f"âš ï¸  DB ì €ì¥ ì‹¤íŒ¨: {e}")

    print("\nğŸ‰ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
