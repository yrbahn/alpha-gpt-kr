#!/usr/bin/env python3
"""
í•œêµ­ ì‹œì¥ íŠ¹ì„± ê¸°ë°˜ ì•ŒíŒŒ ê°€ì„¤ í…ŒìŠ¤íŠ¸
Korean Market-Specific Alpha Hypotheses
"""

import sys
import os
from pathlib import Path
import numpy as np
import pandas as pd
import psycopg2
from dotenv import load_dotenv

sys.path.insert(0, str(Path(__file__).parent.parent))
load_dotenv()

from alpha_gpt_kr.mining.operators import AlphaOperators as ops

def get_db_connection():
    return psycopg2.connect(
        host=os.getenv('DB_HOST', '192.168.0.248'),
        port=int(os.getenv('DB_PORT', 5432)),
        database=os.getenv('DB_NAME', 'marketsense'),
        user=os.getenv('DB_USER', 'yrbahn'),
        password=os.getenv('DB_PASSWORD', '1234')
    )

def load_kosdaq200_data():
    """KOSDAQ 200 ë°ì´í„° + ìˆ˜ê¸‰ ë¡œë“œ"""
    print("ğŸ“Š KOSDAQ 200 ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    conn = get_db_connection()
    
    query_stocks = """
        SELECT s.id, s.ticker, s.name, s.market_cap
        FROM stocks s
        WHERE s.is_active = true
        AND s.market_cap IS NOT NULL
        AND s.ticker >= '400000'
        ORDER BY s.market_cap DESC
        LIMIT 200
    """
    
    stocks_df = pd.read_sql(query_stocks, conn)
    stock_ids = stocks_df['id'].tolist()
    stock_id_list = ', '.join(map(str, stock_ids))
    
    # ê°€ê²© ë°ì´í„°
    query_prices = f"""
        SELECT s.ticker, p.date, p.open, p.high, p.low, p.close, p.volume
        FROM price_data p
        JOIN stocks s ON p.stock_id = s.id
        WHERE p.stock_id IN ({stock_id_list})
        AND p.date >= '2019-01-01'
        ORDER BY s.ticker, p.date
    """
    price_df = pd.read_sql(query_prices, conn)
    
    # ìˆ˜ê¸‰ ë°ì´í„°
    query_supply = f"""
        SELECT s.ticker, sd.date,
               sd.foreign_net_buy, sd.institution_net_buy,
               sd.individual_net_buy, sd.foreign_ownership
        FROM supply_demand_data sd
        JOIN stocks s ON sd.stock_id = s.id
        WHERE sd.stock_id IN ({stock_id_list})
        AND sd.date >= '2019-01-01'
        ORDER BY s.ticker, sd.date
    """
    supply_df = pd.read_sql(query_supply, conn)
    conn.close()
    
    # Pivot
    close = price_df.pivot(index='date', columns='ticker', values='close')
    high = price_df.pivot(index='date', columns='ticker', values='high')
    low = price_df.pivot(index='date', columns='ticker', values='low')
    open_price = price_df.pivot(index='date', columns='ticker', values='open')
    volume = price_df.pivot(index='date', columns='ticker', values='volume')
    
    foreign_net = supply_df.pivot(index='date', columns='ticker', values='foreign_net_buy')
    inst_net = supply_df.pivot(index='date', columns='ticker', values='institution_net_buy')
    indiv_net = supply_df.pivot(index='date', columns='ticker', values='individual_net_buy')
    foreign_own = supply_df.pivot(index='date', columns='ticker', values='foreign_ownership')
    
    # ê³µí†µ ì¸ë±ìŠ¤
    common_idx = close.index.intersection(foreign_net.index)
    
    print(f"  âœ… {len(close.columns)}ì¢…ëª©, {len(common_idx)}ì¼")
    
    return {
        'close': close.loc[common_idx],
        'high': high.loc[common_idx],
        'low': low.loc[common_idx],
        'open': open_price.loc[common_idx],
        'volume': volume.loc[common_idx],
        'foreign_net': foreign_net.loc[common_idx],
        'inst_net': inst_net.loc[common_idx],
        'indiv_net': indiv_net.loc[common_idx],
        'foreign_own': foreign_own.loc[common_idx],
    }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í•œêµ­ ì‹œì¥ ì•ŒíŒŒ ê°€ì„¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

KOREAN_ALPHA_HYPOTHESES = [
    # â”€â”€ 1. ê°œë¯¸ ì—­í–‰ (Retail Contrarian) â”€â”€
    # ê°€ì„¤: ê°œì¸ íˆ¬ììê°€ ë§ì´ íŒŒëŠ” ì¢…ëª©ì´ ë°˜ë“±í•œë‹¤
    {
        "name": "ê°œë¯¸ì—­í–‰_ë‹¨ê¸°",
        "hypothesis": "ê°œì¸ ìˆœë§¤ë„ ê¸‰ì¦ â†’ ë‹¨ê¸° ë°˜ë“± (ê°œë¯¸í„¸ê¸° í›„ ìƒìŠ¹)",
        "formula": lambda d: ops.zscore_scale(ops.neg(ops.ts_mean(d['indiv_net'], 5))),
    },
    {
        "name": "ê°œë¯¸ì—­í–‰_ì¤‘ê¸°",
        "hypothesis": "ê°œì¸ 20ì¼ ìˆœë§¤ë„ ëˆ„ì  â†’ ì¤‘ê¸° ë°˜ë“±",
        "formula": lambda d: ops.zscore_scale(ops.neg(ops.ts_sum(d['indiv_net'], 20))),
    },
    
    # â”€â”€ 2. ì™¸êµ­ì¸ ì¶”ì¢… (Foreign Flow Following) â”€â”€
    # ê°€ì„¤: ì™¸êµ­ì¸ì´ ê¾¸ì¤€íˆ ì‚¬ëŠ” ì¢…ëª©ì´ ìƒìŠ¹
    {
        "name": "ì™¸êµ­ì¸ì¶”ì¢…_ëª¨ë©˜í…€",
        "hypothesis": "ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜ ê°€ì†ë„ (ì¦ê°€ ì¶”ì„¸)",
        "formula": lambda d: ops.zscore_scale(ops.ts_delta(ops.ts_mean(d['foreign_net'], 20), 10)),
    },
    {
        "name": "ì™¸êµ­ì¸ì§€ë¶„_ê¸‰ì¦",
        "hypothesis": "ì™¸êµ­ì¸ ì§€ë¶„ìœ¨ ê¸‰ìƒìŠ¹ ì¢…ëª©",
        "formula": lambda d: ops.zscore_scale(ops.ts_delta(d['foreign_own'], 20)),
    },
    
    # â”€â”€ 3. ê¸°ê´€-ì™¸êµ­ì¸ ë™ì¡° (Smart Money Consensus) â”€â”€
    # ê°€ì„¤: ê¸°ê´€+ì™¸êµ­ì¸ ë™ì‹œ ë§¤ìˆ˜ = ê°•í•œ ì‹ í˜¸
    {
        "name": "ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ_ë™ì¡°",
        "hypothesis": "ê¸°ê´€+ì™¸êµ­ì¸ ë™ì‹œ ìˆœë§¤ìˆ˜ (ê°œì¸ ì—­í–‰)",
        "formula": lambda d: ops.add(
            ops.zscore_scale(ops.ts_mean(d['foreign_net'], 10)),
            ops.zscore_scale(ops.ts_mean(d['inst_net'], 10))
        ),
    },
    
    # â”€â”€ 4. ìˆ˜ê¸‰ ë°˜ì „ (Flow Reversal) â”€â”€
    # ê°€ì„¤: ì™¸êµ­ì¸ ë§¤ë„ì„¸ê°€ êº¾ì´ë©´ ë°˜ë“±
    {
        "name": "ì™¸êµ­ì¸_ë°˜ì „",
        "hypothesis": "ì™¸êµ­ì¸ ìˆœë§¤ë„ â†’ ìˆœë§¤ìˆ˜ ì „í™˜ì ",
        "formula": lambda d: ops.zscore_scale(
            ops.sub(ops.ts_mean(d['foreign_net'], 5), ops.ts_mean(d['foreign_net'], 20))
        ),
    },
    
    # â”€â”€ 5. ê±°ë˜ëŸ‰ ê³ ê°ˆ (Volume Dry-up) â”€â”€
    # ê°€ì„¤: ê±°ë˜ëŸ‰ ê¸‰ê° í›„ í„°ì§€ëŠ” ì¢…ëª©
    {
        "name": "ê±°ë˜ëŸ‰ê³ ê°ˆ_ë°˜ë“±",
        "hypothesis": "ê±°ë˜ëŸ‰ ê¸‰ê° â†’ ì—ë„ˆì§€ ì¶•ì  â†’ ê¸‰ë“±",
        "formula": lambda d: ops.zscore_scale(ops.neg(ops.div(
            ops.ts_mean(d['volume'], 5),
            ops.ts_mean(d['volume'], 60)
        ))),
    },
    
    # â”€â”€ 6. ê°­ ë³µêµ¬ (Gap Recovery) â”€â”€
    # ê°€ì„¤: ê°­í•˜ë½ í›„ ë³µêµ¬í•˜ëŠ” ì¢…ëª©
    {
        "name": "ê°­í•˜ë½_ë³µêµ¬",
        "hypothesis": "ë‹¹ì¼ ê°­í•˜ë½ but ì–‘ë´‰ ë§ˆê° = ë§¤ìˆ˜ì„¸ ìœ ì…",
        "formula": lambda d: ops.zscore_scale(ops.ts_mean(
            ops.sub(d['close'] - d['open'], d['open'] - d['close'].shift(1)),
            10
        )),
    },
    
    # â”€â”€ 7. ë³€ë™ì„± ìˆ˜ì¶• (Volatility Squeeze) â”€â”€
    # ê°€ì„¤: ë³¼ë¦°ì € ë°´ë“œ ìˆ˜ì¶• â†’ í­ë°œ ëŒ€ê¸°
    {
        "name": "ë³¼ë¦°ì €_ìˆ˜ì¶•",
        "hypothesis": "ê°€ê²© ë³€ë™ì„± ìˆ˜ì¶• â†’ ë‹¤ìŒ ì›€ì§ì„ ì¤€ë¹„",
        "formula": lambda d: ops.zscore_scale(ops.neg(
            ops.div(ops.ts_std(d['close'], 20), ops.ts_mean(d['close'], 20))
        )),
    },
    
    # â”€â”€ 8. 52ì£¼ ì‹ ì €ê°€ ë°˜ë“± (52-Week Low Bounce) â”€â”€
    # ê°€ì„¤: ì‹ ì €ê°€ ê·¼ì²˜ì—ì„œ ë°˜ë“±
    {
        "name": "ì‹ ì €ê°€_ë°˜ë“±",
        "hypothesis": "52ì£¼ ìµœì €ê°€ ëŒ€ë¹„ ìœ„ì¹˜ (ë‚®ì„ìˆ˜ë¡ ë°˜ë“± ê¸°ëŒ€)",
        "formula": lambda d: ops.zscore_scale(ops.neg(ops.div(
            d['close'],
            ops.ts_max(d['close'], 240)
        ))),
    },
    
    # â”€â”€ 9. ì›”ìš”ì¼ íš¨ê³¼ ëŒ€ì‘ (Monday Effect) â”€â”€
    # ê°€ì„¤: ê¸ˆìš”ì¼ ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜ â†’ ì›”ìš”ì¼ ê°­ì—…
    {
        "name": "ê¸ˆìš”ì¼_ì™¸êµ­ì¸",
        "hypothesis": "ê¸ˆìš”ì¼ ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜ ê°•ë„",
        "formula": lambda d: ops.zscore_scale(ops.ts_mean(d['foreign_net'], 5)),  # simplified
    },
    
    # â”€â”€ 10. ì•„ë«ê¼¬ë¦¬ ë§¤ì§‘ (Lower Shadow Accumulation) â”€â”€
    # ê°€ì„¤: ì•„ë«ê¼¬ë¦¬ê°€ ê¸¸ë©´ ì €ì  ë§¤ìˆ˜ì„¸ ìœ ì…
    {
        "name": "ì•„ë«ê¼¬ë¦¬_ë§¤ì§‘",
        "hypothesis": "ì•„ë«ê¼¬ë¦¬ ê¸¸ì´ = ì €ì  ë§¤ìˆ˜ ê°•ë„",
        "formula": lambda d: ops.zscore_scale(ops.ts_mean(
            ops.div(
                np.minimum(d['open'], d['close']) - d['low'],
                d['high'] - d['low'] + 0.0001
            ),
            20
        )),
    },
    
    # â”€â”€ 11. ê¸°ê´€ ì„ í–‰ (Institutional Lead) â”€â”€
    # ê°€ì„¤: ê¸°ê´€ì´ ë¨¼ì € ì‚¬ê³  ì™¸êµ­ì¸ì´ ë”°ë¼ì˜´
    {
        "name": "ê¸°ê´€ì„ í–‰_ì™¸êµ­ì¸í›„í–‰",
        "hypothesis": "ê¸°ê´€ ìˆœë§¤ìˆ˜ but ì™¸êµ­ì¸ ì•„ì§ ì•ˆ ì‚° ì¢…ëª©",
        "formula": lambda d: ops.sub(
            ops.zscore_scale(ops.ts_sum(d['inst_net'], 20)),
            ops.zscore_scale(ops.ts_sum(d['foreign_net'], 20))
        ),
    },
    
    # â”€â”€ 12. ìˆ˜ê¸‰ ì§‘ì¤‘ë„ (Flow Concentration) â”€â”€
    # ê°€ì„¤: íŠ¹ì • ì„¸ë ¥ì˜ ì§‘ì¤‘ ë§¤ìˆ˜
    {
        "name": "ì™¸êµ­ì¸_ì§‘ì¤‘ë§¤ìˆ˜",
        "hypothesis": "ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜ / ì´ê±°ë˜ëŸ‰ ë¹„ìœ¨",
        "formula": lambda d: ops.zscore_scale(ops.ts_mean(
            ops.div(d['foreign_net'], d['volume'] * d['close'] + 1),
            20
        )),
    },
    
    # â”€â”€ 13. ì €PBR + ì™¸êµ­ì¸ â”€â”€
    # ê°€ì„¤: ì €í‰ê°€ + ì™¸êµ­ì¸ ê´€ì‹¬ = ê°€ì¹˜ ì¬ë°œê²¬
    {
        "name": "ê°€ì¹˜ë°œêµ´_ì™¸êµ­ì¸",
        "hypothesis": "52ì£¼ ì €ì  ê·¼ì²˜ + ì™¸êµ­ì¸ ìœ ì…",
        "formula": lambda d: ops.add(
            ops.zscore_scale(ops.neg(ops.div(d['close'], ops.ts_max(d['close'], 240)))),
            ops.zscore_scale(ops.ts_delta(d['foreign_own'], 30))
        ),
    },
    
    # â”€â”€ 14. ì´ê²©ë„ íšŒê·€ (Moving Average Reversion) â”€â”€
    # ê°€ì„¤: ì´ê²©ë„ê°€ ë‚®ìœ¼ë©´ í‰ê·  íšŒê·€
    {
        "name": "ì´ê²©ë„_íšŒê·€",
        "hypothesis": "20ì¼ì„  ëŒ€ë¹„ ì´ê²©ë„ ë‚®ì€ ì¢…ëª© ë°˜ë“±",
        "formula": lambda d: ops.zscore_scale(ops.neg(ops.div(
            d['close'],
            ops.ts_mean(d['close'], 20)
        ))),
    },
    
    # â”€â”€ 15. ë³µí•©: ì €ë³€ë™ì„± + ì™¸êµ­ì¸ ìœ ì… â”€â”€
    {
        "name": "ì €ë³€ë™ì„±_ì™¸êµ­ì¸",
        "hypothesis": "ë³€ë™ì„± ë‚®ê³  ì™¸êµ­ì¸ ë“¤ì–´ì˜¤ëŠ” ì¢…ëª©",
        "formula": lambda d: ops.add(
            ops.zscore_scale(ops.neg(ops.ts_std(d['close'].pct_change(), 60))),
            ops.zscore_scale(ops.ts_delta(d['foreign_own'], 30))
        ),
    },
]

def calc_ic(alpha_vals, forward_ret, start, end):
    """Calculate IC for a date range"""
    idx_str = pd.to_datetime(alpha_vals.index).strftime('%Y-%m-%d')
    mask = (idx_str >= start) & (idx_str <= end)
    a = alpha_vals.loc[mask]
    r = forward_ret.loc[mask]
    
    ics = []
    for dt in a.index:
        if dt not in r.index:
            continue
        av = a.loc[dt].dropna()
        rv = r.loc[dt].dropna()
        common = av.index.intersection(rv.index)
        if len(common) < 20:
            continue
        ic = av[common].corr(rv[common])
        if not np.isnan(ic):
            ics.append(ic)
    return np.mean(ics) if ics else 0, np.std(ics) if ics else 1

def main():
    data = load_kosdaq200_data()
    
    # 20ì¼ ì„ í–‰ ìˆ˜ìµë¥ 
    forward_ret = data['close'].pct_change(20).shift(-20)
    
    # 4-fold CV
    folds = [
        ('2020-01-01', '2021-06-30', '2021-07-21', '2022-12-31'),
        ('2020-01-01', '2022-06-30', '2022-07-21', '2023-12-31'),
        ('2020-01-01', '2023-06-30', '2023-07-21', '2024-12-31'),
        ('2020-01-01', '2024-06-30', '2024-07-21', '2025-12-31'),
    ]
    
    results = []
    
    print("\n" + "=" * 70)
    print("ğŸ‡°ğŸ‡· í•œêµ­ ì‹œì¥ ì•ŒíŒŒ ê°€ì„¤ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    for hypo in KOREAN_ALPHA_HYPOTHESES:
        name = hypo['name']
        print(f"\ní…ŒìŠ¤íŠ¸: {name}")
        print(f"  ê°€ì„¤: {hypo['hypothesis']}")
        
        try:
            alpha_vals = hypo['formula'](data)
            
            test_ics = []
            for train_start, train_end, test_start, test_end in folds:
                test_ic, _ = calc_ic(alpha_vals, forward_ret, test_start, test_end)
                test_ics.append(test_ic)
            
            avg_ic = np.mean(test_ics)
            std_ic = np.std(test_ics)
            ir = avg_ic / std_ic if std_ic > 0 else 0
            
            results.append({
                'name': name,
                'hypothesis': hypo['hypothesis'],
                'test_ic': avg_ic,
                'ir': ir,
            })
            print(f"  âœ… Test IC: {avg_ic:+.4f}, IR: {ir:.2f}")
            
        except Exception as e:
            print(f"  âŒ Error: {e}")
            results.append({
                'name': name,
                'hypothesis': hypo['hypothesis'],
                'test_ic': 0,
                'ir': 0,
            })
    
    # ê²°ê³¼ ì •ë ¬
    results_df = pd.DataFrame(results)
    results_df = results_df.sort_values('test_ic', ascending=False)
    
    print("\n" + "=" * 70)
    print("ğŸ† í•œêµ­ ì‹œì¥ ì•ŒíŒŒ ìˆœìœ„ (Test IC)")
    print("=" * 70)
    
    for i, row in results_df.iterrows():
        emoji = "ğŸ¥‡" if row['test_ic'] == results_df['test_ic'].max() else "  "
        print(f"{emoji} {row['name']:20s} | IC: {row['test_ic']:+.4f} | IR: {row['ir']:.2f}")
        print(f"     â””â”€ {row['hypothesis']}")
    
    print("\n" + "=" * 70)
    print("ğŸ“Š ê¸°ì¡´ ìµœê³  ëŒ€ë¹„")
    print("=" * 70)
    print(f"ê¸°ì¡´ Combined Alpha: IC = 0.1376")
    print(f"ì‹ ê·œ ìµœê³ : {results_df.iloc[0]['name']} IC = {results_df.iloc[0]['test_ic']:.4f}")
    
    # ì €ì¥
    results_df.to_csv('/Users/yrbahn/.openclaw/workspace/alpha-gpt-kr/experiments/korean_alpha_results.csv', index=False)
    print("\nê²°ê³¼ ì €ì¥: experiments/korean_alpha_results.csv")

if __name__ == "__main__":
    main()
